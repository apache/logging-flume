<html><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>Flume User Guide</title><link rel="stylesheet" href="docbook.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"></head><body><div style="clear:both; margin-bottom: 4px"></div><div align="center"><a href="index.html"><img src="images/home.png" alt="Documentation Home"></a></div><span class="breadcrumbs"><div class="breadcrumbs"><span class="breadcrumb-node">Flume User Guide</span></div></span><div lang="en" class="article" title="Flume User Guide"><div class="titlepage"><div><div><h2 class="title"><a name="id402441"></a>Flume User Guide</h2></div><div><div class="author"><h3 class="author"><span class="firstname">flume-dev@cloudera.org</span></h3></div></div><div><div class="revhistory"><table border="1" width="100%" summary="Revision history"><tr><th align="left" valign="top" colspan="3"><b>Revision History</b></th></tr><tr><td align="left">Revision 0.9.3-SNAPSHOT</td><td align="left">November 16 2010</td><td align="left">F</td></tr></table></div></div></div><hr></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#Introduction">1. Introduction</a></span></dt><dd><dl><dt><span class="section"><a href="#_architecture">1.1. Architecture</a></span></dt><dt><span class="section"><a href="#_reliability">1.2. Reliability</a></span></dt><dt><span class="section"><a href="#_scalability">1.3. Scalability</a></span></dt><dt><span class="section"><a href="#_manageability">1.4. Manageability</a></span></dt><dt><span class="section"><a href="#_extensibility">1.5. Extensibility</a></span></dt><dt><span class="section"><a href="#_section_summary">1.6. Section summary</a></span></dt></dl></dd><dt><span class="section"><a href="#Quickstart">2. Flume Single Node Quick Start</a></span></dt><dd><dl><dt><span class="section"><a href="#_sources_and_the_literal_dump_literal_command">2.1. Sources and the <code class="literal">dump</code> command</a></span></dt><dd><dl><dt><span class="section"><a href="#_reading_from_a_text_file_literal_text_literal">2.1.1. Reading from a text file, <code class="literal">text</code></a></span></dt><dt><span class="section"><a href="#_tailing_a_file_name_literal_tail_literal_and_literal_multitail_literal">2.1.2. Tailing a file name, <code class="literal">tail</code> and <code class="literal">multitail</code></a></span></dt><dt><span class="section"><a href="#_synthetic_sources_literal_synth_literal">2.1.3. Synthetic sources, <code class="literal">synth</code></a></span></dt><dt><span class="section"><a href="#_syslog_as_a_source_literal_syslogudp_literal_and_literal_syslogtcp_literal">2.1.4. Syslog as a source, <code class="literal">syslogUdp</code> and <code class="literal">syslogTcp</code></a></span></dt></dl></dd><dt><span class="section"><a href="#_anatomy_of_an_event">2.2. Anatomy of an Event</a></span></dt><dt><span class="section"><a href="#_section_summary_2">2.3. Section Summary</a></span></dt></dl></dd><dt><span class="section"><a href="#_pseudo_distributed_mode">3. Pseudo-distributed Mode</a></span></dt><dd><dl><dt><span class="section"><a href="#_starting_pseudo_distributed_flume_daemons">3.1. Starting Pseudo-distributed Flume Daemons</a></span></dt><dd><dl><dt><span class="section"><a href="#_the_master">3.1.1. The Master</a></span></dt><dt><span class="section"><a href="#_the_flume_node">3.1.2. The Flume Node</a></span></dt></dl></dd><dt><span class="section"><a href="#_configuring_a_node_via_the_master">3.2. Configuring a Node via the Master</a></span></dt><dt><span class="section"><a href="#_introducing_sinks">3.3. Introducing Sinks</a></span></dt><dt><span class="section"><a href="#_aggregated_configurations">3.4. Aggregated Configurations</a></span></dt><dt><span class="section"><a href="#_tiering_flume_nodes_agents_and_collectors">3.5. Tiering Flume Nodes: Agents and Collectors</a></span></dt><dt><span class="section"><a href="#_section_summary_3">3.6. Section Summary</a></span></dt></dl></dd><dt><span class="section"><a href="#_fully_distributed_mode">4. Fully-distributed Mode</a></span></dt><dd><dl><dt><span class="section"><a href="#_static_configuration_files">4.1. Static Configuration Files</a></span></dt><dd><dl><dt><span class="section"><a href="#_using_default_values">4.1.1. Using Default Values</a></span></dt></dl></dd><dt><span class="section"><a href="#_multiple_collectors">4.2. Multiple Collectors</a></span></dt><dd><dl><dt><span class="section"><a href="#_partitioning_agents_across_multiple_collectors">4.2.1. Partitioning Agents across Multiple Collectors</a></span></dt><dt><span class="section"><a href="#_manually_specifying_failover_chains">4.2.2. Manually Specifying Failover Chains</a></span></dt><dt><span class="section"><a href="#_automatic_failover_chains">4.2.3. Automatic Failover Chains</a></span></dt></dl></dd><dt><span class="section"><a href="#_logical_configurations">4.3. Logical Configurations</a></span></dt><dd><dl><dt><span class="section"><a href="#_logical_nodes">4.3.1. Logical Nodes</a></span></dt><dt><span class="section"><a href="#_logical_sources_and_logical_sinks">4.3.2. Logical Sources and Logical Sinks</a></span></dt><dt><span class="section"><a href="#_flow_isolation">4.3.3. Flow Isolation</a></span></dt><dt><span class="section"><a href="#_section_summary_4">4.3.4. Section Summary</a></span></dt></dl></dd><dt><span class="section"><a href="#_multiple_masters">4.4. Multiple Masters</a></span></dt><dd><dl><dt><span class="section"><a href="#_standalone_mode_compared_to_distributed_mode">4.4.1. Standalone Mode Compared to Distributed Mode</a></span></dt><dt><span class="section"><a href="#_running_in_standalone_mode">4.4.2. Running in Standalone Mode</a></span></dt><dt><span class="section"><a href="#_running_in_distributed_mode">4.4.3. Running in Distributed Mode</a></span></dt><dt><span class="section"><a href="#_configuration_stores">4.4.4. Configuration Stores</a></span></dt><dt><span class="section"><a href="#_which_configuration_store_should_i_use">4.4.5. Which Configuration Store Should I Use?</a></span></dt><dt><span class="section"><a href="#_configuring_the_zbcs">4.4.6. Configuring the ZBCS</a></span></dt><dt><span class="section"><a href="#_gossip_in_distributed_mode">4.4.7. Gossip in Distributed Mode</a></span></dt><dt><span class="section"><a href="#_diagrams_how_the_masters_and_nodes_talk_to_each_other">4.4.8. Diagrams: How the Masters and Nodes talk to each other</a></span></dt><dt><span class="section"><a href="#_configuring_flume_nodes_to_connect_to_multiple_master_servers">4.4.9. Configuring Flume Nodes to Connect to Multiple Master Servers</a></span></dt></dl></dd><dt><span class="section"><a href="#_external_zookeeper_cluster">4.5. External ZooKeeper Cluster</a></span></dt><dt><span class="section"><a href="#_section_summary_5">4.6. Section Summary</a></span></dt></dl></dd><dt><span class="section"><a href="#_integrating_flume_with_your_data_sources">5. Integrating Flume with your Data Sources</a></span></dt><dd><dl><dt><span class="section"><a href="#_push_sources">5.1. Push Sources</a></span></dt><dt><span class="section"><a href="#_polling_sources">5.2. Polling Sources</a></span></dt><dt><span class="section"><a href="#_embedding_sources">5.3. Embedding Sources</a></span></dt><dt><span class="section"><a href="#_logging_via_log4j_directly">5.4. Logging via log4j Directly</a></span></dt><dd><dl><dt><span class="section"><a href="#_example_of_logging_hadoop_jobs">5.4.1. Example of Logging Hadoop Jobs</a></span></dt><dt><span class="section"><a href="#_logging_hadoop_daemons">5.4.2. Logging Hadoop Daemons</a></span></dt></dl></dd></dl></dd><dt><span class="section"><a href="#_using_data_collected_by_flume">6. Using Data Collected by Flume</a></span></dt><dd><dl><dt><span class="section"><a href="#_the_data_model_of_a_flume_event">6.1. The Data Model of a Flume Event</a></span></dt><dt><span class="section"><a href="#_output_bucketing">6.2. Output Bucketing</a></span></dt><dt><span class="section"><a href="#_output_format">6.3. Output Format</a></span></dt><dt><span class="section"><a href="#_small_files_compared_to_high_latency">6.4. Small Files Compared to High Latency</a></span></dt></dl></dd><dt><span class="section"><a href="#_compression_for_files_written_to_hdfs">7. Compression for files written to HDFS.</a></span></dt><dt><span class="section"><a href="#_advanced_flume_usage">8. Advanced Flume Usage</a></span></dt><dd><dl><dt><span class="section"><a href="#_the_flume_command_shell">8.1. The Flume Command Shell</a></span></dt><dd><dl><dt><span class="section"><a href="#_using_the_flume_command_shell">8.1.1. Using the Flume Command Shell</a></span></dt></dl></dd><dt><span class="section"><a href="#_flume_8217_s_dataflow_specification_language">8.2. Flume&#8217;s Dataflow Specification Language</a></span></dt><dd><dl><dt><span class="section"><a href="#_special_sinks_fan_out_fail_over_and_roll">8.2.1. Special Sinks: Fan out, Fail over, and Roll</a></span></dt><dt><span class="section"><a href="#_introducing_sink_decorators">8.2.2. Introducing Sink Decorators</a></span></dt><dt><span class="section"><a href="#_translations_of_high_level_sources_and_sinks">8.2.3. Translations of High-level Sources and Sinks</a></span></dt></dl></dd><dt><span class="section"><a href="#_custom_metadata_extraction">8.3. Custom Metadata Extraction</a></span></dt><dd><dl><dt><span class="section"><a href="#_extractors">8.3.1. Extractors</a></span></dt><dt><span class="section"><a href="#_meta_data_filtering_and_transformations">8.3.2. Meta Data Filtering and Transformations</a></span></dt><dt><span class="section"><a href="#_role_defaults">8.3.3. Role Defaults</a></span></dt><dt><span class="section"><a href="#_arbitrary_data_flows_and_custom_architectures">8.3.4. Arbitrary Data Flows and Custom Architectures</a></span></dt></dl></dd><dt><span class="section"><a href="#_extending_via_sink_source_decorator_plugins">8.4. Extending via Sink/Source/Decorator Plugins</a></span></dt><dd><dl><dt><span class="section"><a href="#_semantics_of_flume_extensions">8.4.1. Semantics of Flume Extensions</a></span></dt><dd><dl><dt><span class="section"><a href="#_simple_source_semantics">8.4.1.1. Simple source semantics.</a></span></dt><dt><span class="section"><a href="#_buffered_source_semantics">8.4.1.2. Buffered source semantics</a></span></dt><dt><span class="section"><a href="#_simple_sinks">8.4.1.3. Simple Sinks.</a></span></dt><dt><span class="section"><a href="#_buffered_sink_and_decorator_semantics">8.4.1.4. Buffered sink and decorator semantics</a></span></dt><dt><span class="section"><a href="#_retries_sleeps_and_unclean_exits">8.4.1.5. Retries, sleeps, and unclean exits.</a></span></dt></dl></dd></dl></dd><dt><span class="section"><a href="#_limiting_data_transfer_rate_between_source_sink_pairs">8.5. Limiting Data Transfer Rate between Source-Sink pairs</a></span></dt></dl></dd><dt><span class="section"><a href="#_flume_and_hdfs_security_integration">9. Flume and HDFS Security Integration</a></span></dt><dd><dl><dt><span class="section"><a href="#_basics">9.1. Basics</a></span></dt><dt><span class="section"><a href="#_setting_up_flume_users_on_kerberos">9.2. Setting up Flume users on Kerberos</a></span></dt><dd><dl><dt><span class="section"><a href="#_administering_kerberos_principals">9.2.1. Administering Kerberos principals</a></span></dt></dl></dd></dl></dd><dt><span class="section"><a href="#_appendix">10. Appendix</a></span></dt><dd><dl><dt><span class="section"><a href="#_flume_source_catalog">10.1. Flume Source Catalog</a></span></dt><dt><span class="section"><a href="#_flume_sinks_catalog">10.2. Flume Sinks Catalog</a></span></dt><dt><span class="section"><a href="#_flume_sink_decorator_catalog">10.3. Flume Sink Decorator Catalog</a></span></dt><dt><span class="section"><a href="#_flume_environment_variables">10.4. Flume Environment Variables</a></span></dt><dt><span class="section"><a href="#_flume_site_xml_configuration_settings">10.5. flume-site.xml configuration settings</a></span></dt><dt><span class="section"><a href="#_troubleshooting">10.6. Troubleshooting</a></span></dt><dd><dl><dt><span class="section"><a href="#_what_are_the_default_ports">10.6.1. What are the default ports?</a></span></dt><dt><span class="section"><a href="#_what_versions_of_hadoop_hdfs_can_i_use_how_do_i_change_this">10.6.2. What versions of Hadoop HDFS can I use?  How do I change this?</a></span></dt><dt><span class="section"><a href="#_why_doesn_8217_t_a_flume_node_appear_on_flume_master">10.6.3. Why doesn&#8217;t a Flume node appear on Flume Master?</a></span></dt><dt><span class="section"><a href="#_why_is_the_state_of_a_flume_node_changing_rapidly">10.6.4. Why is the state of a Flume node changing rapidly?</a></span></dt><dt><span class="section"><a href="#_where_are_the_logs_for_troubleshooting_flume_itself">10.6.5. Where are the logs for troubleshooting Flume itself?</a></span></dt><dt><span class="section"><a href="#_what_can_i_do_if_i_get_node_failure_due_to_out_of_file_handles">10.6.6. What can I do if I get node failure due to out of file handles?</a></span></dt><dt><span class="section"><a href="#_failures_due_when_using_disk_failover_or_write_ahead_log">10.6.7. Failures due when using Disk Failover or Write Ahead Log</a></span></dt><dt><span class="section"><a href="#_can_i_write_data_to_amazon_s3">10.6.8. Can I write data to Amazon S3?</a></span></dt></dl></dd></dl></dd><dt><span class="glossary"><a href="#_glossary">Glossary</a></span></dt><dt><span class="section"><a href="#_versions">11. Versions</a></span></dt><dd><dl><dt><span class="section"><a href="#_history">11.1. history</a></span></dt></dl></dd></dl></div><div class="list-of-figures"><p><b>List of Figures</b></p><dl><dt>1. <a href="#id415609">Flume Flows: Single Flow</a></dt><dt>2. <a href="#id415698">Flume Flows: Multiple Flows</a></dt><dt>3. <a href="#id416532">Flume Master: Standalone Mode</a></dt><dt>4. <a href="#id416555">Flume Master: Distributed Mode</a></dt></dl></div><div class="abstract" title="Abstract"><a name="_abstract"></a><p class="title"><b>Abstract</b></p><div class="abstract" title="Abstract"><p class="title"><b>Abstract</b></p><p>Flume is a distributed, reliable, and available service for
efficiently collecting, aggregating, and moving large amounts of log
data.  It has a simple and flexible architecture based on streaming
data flows.  It is robust and fault tolerant with tunable reliability
mechanisms and many failover and recovery mechanisms.  The system is
centrally managed and allows for intelligent dynamic management. It
uses a simple extensible data model that allows for online analytic
applications.</p></div></div><div class="section" title="1. Introduction"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Introduction"></a>1. Introduction</h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_architecture">1.1. Architecture</a></span></dt><dt><span class="section"><a href="#_reliability">1.2. Reliability</a></span></dt><dt><span class="section"><a href="#_scalability">1.3. Scalability</a></span></dt><dt><span class="section"><a href="#_manageability">1.4. Manageability</a></span></dt><dt><span class="section"><a href="#_extensibility">1.5. Extensibility</a></span></dt><dt><span class="section"><a href="#_section_summary">1.6. Section summary</a></span></dt></dl></div><p>Flume is a distributed, reliable, and available service for
efficiently moving large amounts of data soon after the data is
produced.  This release provides a scalable conduit to move data
around a cluster as well as reliable logging.</p><p>The primary use case for Flume is as a logging system that gathers a
set of log files on every machine in a cluster and aggregates them to
a centralized persistent store such as the Hadoop Distributed File
System (HDFS).</p><p>The system was designed with these four key goals in mind:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Reliability
</li><li class="listitem">
Scalability
</li><li class="listitem">
Manageability
</li><li class="listitem">
Extensibility
</li></ul></div><p>This section provides a high-level overview of Flume&#8217;s architecture
and describes how the four design goals are achieved.</p><div class="section" title="1.1. Architecture"><div class="titlepage"><div><div><h3 class="title"><a name="_architecture"></a>1.1. Architecture</h3></div></div></div><p>Flume&#8217;s architecture is simple, robust, and flexible.  The main
abstraction in Flume is a stream-oriented <span class="strong"><strong>data flow</strong></span>.  A data flow
describes the way a single stream of data is transferred and processed
from its point of generation to its eventual destination. Data flows
are composed of <span class="strong"><strong>logical nodes</strong></span> that can transform or aggregate the
events they receive.  Logical nodes are wired together in chains to
form a data flow. The way in which they are wired is called the
logical node&#8217;s <span class="strong"><strong>configuration</strong></span>.</p><p>Controlling all this is the <span class="strong"><strong>Flume Master</strong></span>, which is a separate
service with knowledge of all the physical and logical nodes in a
Flume installation. The Master assigns configurations to logical
nodes, and is responsible for communicating configuration updates by
the user to all logical nodes.  In turn, the logical nodes
periodically contact the master so they can share monitoring
information and check for updates to their configuration.</p><div class="informalfigure"><div class="mediaobject"><img src="architecture.png" alt="architecture.png"></div></div><p>The graph above shows a typical deployment of Flume that collects log
data from a set of application servers.  The deployment consists of a
number of <span class="emphasis"><em>logical nodes</em></span>, arranged into three tiers. The first tier
is the <span class="emphasis"><em>agent</em></span> tier.  Agent nodes are typically installed on the
machines that generate the logs and are your data&#8217;s initial point of
contact with Flume.  They forward data to the next tier of <span class="emphasis"><em>collector
nodes</em></span>, which aggregate the separate data flows and forward them to
the final <span class="emphasis"><em>storage tier</em></span>.</p><p>For example, the agents could be machines listening for <code class="literal">syslog</code> data
or monitoring the logs of a service such as a web server or the Hadoop
JobTracker. The agents produce streams of data that are sent to the
collectors; the collectors then aggregate the streams into larger
streams which can be written efficiently to a storage tier such as
HDFS.</p><p>Logical nodes are a very flexible abstraction. Every logical node has
just two components - a <span class="emphasis"><em>source</em></span> and a <span class="emphasis"><em>sink</em></span>. The source tells a
logical node where to collect data, and the sink tells it where to
send the data. The only difference between two logical nodes is how
the source and sink are configured. Both source and sink can
additionally be configured with <span class="emphasis"><em>decorators</em></span> which perform some simple
processing on data as it passes through. In the previous example, the
collector and the agents are <span class="emphasis"><em>running the same node software</em></span>.  The
Master assigns a configuration to each logical node at run-time - all
components of a node&#8217;s configuration are instantiated dynamically at
run-time, and therefore configurations can be changed many times
throughout the lifetime of a Flume service without having to restart
any Java processes or log into the machines themselves. In fact,
logical nodes themselves can be created and deleted dynamically.</p><p>The source, sink, and optional decorators are a powerful set of
primitives. Flume uses this architecture to provide per-flow data
properties (for example durability guarantees, compression, or
batching), or to compute event metadata, or even generate new events
that are inserted into data flow.  A logical node can also send data
downstream to several logical nodes.  This allows for multiple flows
and each subflow can potentially be configured differently.  For
example, it&#8217;s possible to have one flow be a collection path,
delivering data reliably to a persistent store, while another branch
computes lightweight analytics to be delivered to an alerting system.</p><div class="sidebar" title="Logical and Physical Nodes"><p class="title"><b>Logical and Physical Nodes</b></p><p>It&#8217;s important to make the distinction between <span class="emphasis"><em>logical nodes</em></span> and
<span class="emphasis"><em>physical nodes</em></span>. A physical node corresponds to a single Java process
running on one machine in a single JVM instance. Usually there is just
one physical node per machine. Physical nodes act as containers for
<span class="emphasis"><em>logical nodes</em></span>, which are wired together to form data flows. Each
physical node can play host to many logical nodes, and takes care of
arbitrating the assignment of machine resources between them. So,
although the agents and the collectors in the preceding example are
<span class="emphasis"><em>logically</em></span> separate processes, they could be running on the same
<span class="emphasis"><em>physical</em></span> node. Flume gives users the flexibility to specify where
the computation and data transfer are done. The remainder of this
guide describes logical nodes unless stated otherwise.</p></div></div><div class="section" title="1.2. Reliability"><div class="titlepage"><div><div><h3 class="title"><a name="_reliability"></a>1.2. Reliability</h3></div></div></div><p>Reliability, the ability to continue delivering events in the face of
failures without losing data, is a vital feature of Flume. Large
distributed systems can and do suffer partial failures in many ways -
physical hardware can fail, resources such as network bandwidth or
memory can become scarce, or software can crash or run slowly. Flume
emphasizes fault-tolerance as a core design principle and keeps
running and collecting data even when many components have failed.</p><p>Flume can guarantee that all data received by an agent node will
eventually make it to the collector at the end of its flow as long as
the agent node keeps running.  That is, data can be <span class="strong"><strong>reliably</strong></span>
delivered to its eventual destination.</p><p>However, reliable delivery can be very resource intensive and is often
a stronger guarantee than some data sources require. Therefore, Flume
allows the user to specify, on a per-flow basis, the level of
reliability required. There are three supported reliability levels:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
End-to-end
</li><li class="listitem">
Store on failure
</li><li class="listitem">
Best effort
</li></ul></div><div class="sidebar" title="A Note About Reliability"><p class="title"><b>A Note About Reliability</b></p><p>Although Flume is extremely tolerant to machine, network, and software
failures, there is never any such thing as <span class="emphasis"><em>100% reliability</em></span>. If all
the machines in a Flume installation were irrevocably destroyed in
some terrible data center incident, all copies of Flume&#8217;s data would
be lost and there would be no way to recover them. Therefore all of
Flume&#8217;s reliability levels make guarantees about data delivery <span class="emphasis"><em>until
some maximum number of failures have occurred</em></span>. Flume&#8217;s failure modes
- in terms of what can fail and what will keep running if they do -
are described in detail later in this guide.</p></div><p>The <span class="strong"><strong>end-to-end</strong></span> reliability level guarantees that once Flume accepts
an event, that event will make it to the endpoint - as long as the
agent that accepted the event remains live long enough. The first
thing the agent does in this setting is write the event to disk in a
<span class="emphasis"><em>'write-ahead log</em></span>' (WAL) so that, if the agent crashes and restarts,
knowledge of the event is not lost. After the event has successfully
made its way to the end of its flow, an acknowledgment is sent back to
the originating agent so that it knows it no longer needs to store the
event on disk. This reliability level can withstand any number of
failures downstream of the initial agent.</p><p>The <span class="strong"><strong>store on failure</strong></span> reliability level causes nodes to only require
an acknowledgement from the node one hop downstream.  If the sending
node detects a failure, it will store data on its local disk until the
downstream node is repaired, or an alternate downstream destination
can be selected.  While this is effective, data can be lost if a
compound or silent failure occurs.</p><p>The <span class="strong"><strong>best-effort</strong></span> reliability level sends data to the next hop with no
attempts to confirm or retry delivery.  If nodes fail, any data that
they were in the process of transmitting or receiving can be
lost. This is the weakest reliability level, but also the most
lightweight.</p><div class="sidebar" title="Eliminating Single Points of Failure"><p class="title"><b>Eliminating Single Points of Failure</b></p><p>Even when using the end-to-end reliability level, a Flume flow can
fail to make progress if there aren&#8217;t any nodes available to process
events. The data will continue to be collected by the agents, but the
agents may not be able to deliver the data downstream until a suitable
node comes online. In order to maintain high availability, Flume
allows flows to fail over from one downstream node to another without
any user intervention. The Flume Master can also be replicated for
high availability which means that both the data plane and the control
plane can tolerate a number of faults.</p></div></div><div class="section" title="1.3. Scalability"><div class="titlepage"><div><div><h3 class="title"><a name="_scalability"></a>1.3. Scalability</h3></div></div></div><p>Scalability is the ability to increase system performance linearly -
or better - by adding more resources to the system.  Flume&#8217;s goal is
horizontal scalability&#8201;&#8212;&#8201;the ability to incrementally add more
machines to the system to increase throughput. A key performance
measure in Flume is the number or size of events entering the system
and being delivered.  When load increases, it is simple to add more
resources to the system in the form of more machines to handle the
increased load.</p><p>As seen in the preceding example installation, there are three
separate components of Flume that require different approaches to
scalability: the collector tier, the master, and the storage tier.</p><p>The collector tier needs to be able to scale in order to handle large
volumes of data coming from large numbers of agent nodes.  This
workload is write heavy, partitionable, and thus parallelizable.  By
adding more machines to the collector tier, you can increase the
number of agents and the maximum available throughput of the system.</p><p>An individual collector can usually handle many agents (up to
hundreds) because each individual agent often produces only small
amounts of log data compared to the full bandwidth available to the
collector. Therefore, Flume balances flows from agents across
different collectors. (One flow from an agent will talk to the same
collector.) Flume uses a randomized algorithm to evenly assign lists
of collectors to flows.  This automatically spreads the load, and also
keeps the load spread in the case where a collector fails.</p><p>As the number of nodes in the system increases, the volume of traffic
on the control path to and from the Flume Master may become a
bottleneck. The Flume Master also supports horizontal scaling by
adding more machines - although just a small number of commodity
servers can serve a large installation of nodes. The state of the
Flume Master is kept synchronized and fully replicated, which ensures
that it is both fault tolerant and highly scalable.</p><p>Finally, Flume can only write data through a flow at the rate that the
final destinations can accept. Although Flume is able to buffer data
inside a flow to smooth out high-volume bursts, the output rate needs
to be equal on average to the input rate to avoid log jams. Thus,
writing to a scalable storage tier is advisable.  For example, HDFS
has been shown to scale to thousands of machines and can handle many
petabytes of data.</p></div><div class="section" title="1.4. Manageability"><div class="titlepage"><div><div><h3 class="title"><a name="_manageability"></a>1.4. Manageability</h3></div></div></div><p>Manageability is the ability to control data flows, monitor nodes,
modify settings, and control outputs of a large system.  Manually
managing the data flow from the sources to the end point is tedious,
error prone, and a major pain point.  With the potential to have
thousands of log-generating applications and services, it&#8217;s important
to have a centralized management point to monitor and change data
flows, and the ability to dynamically handle different conditions or
problems.</p><p>The Flume Master is the point where global state such as the data
flows can be managed. Via the Flume Master, users can monitor flows
and reconfigure them on the fly.  The Flume Master has the information
required to automatically respond to system changes such as load
imbalances, partial failures, or newly provisioned hardware.</p><p>You can dynamically reconfigure nodes by using the Flume Master.
Although this guide describes examples of traditional three-tier
deployments, the flexibility of the nodes allow for arbitrary node
topologies.  You can reconfigure nodes by using small scripts written
in a flexible dataflow specification language, which can be submitted
via the Flume Master interface.</p><p>You can administer the Flume Master by using either of two interfaces:
a web interface or the scriptable Flume command shell.  The web
interface provides interactive updates of the system&#8217;s state.  The
shell enables administration via manually crafted scripts or
machine-generated scripts.</p></div><div class="section" title="1.5. Extensibility"><div class="titlepage"><div><div><h3 class="title"><a name="_extensibility"></a>1.5. Extensibility</h3></div></div></div><p>Extensibility is the ability to add new functionality to a system. For
example, you can extend Flume by adding connectors to existing storage
layers or data platforms.  This is made possible by simple interfaces,
separation of functional concerns into simple composable pieces, a
flow specification language, and a simple but flexible data model.</p><p>Flume provides many common input and output connectors.  When new
input connectors (sources) are added, extra metadata fields specific
to that source can be attached to each event it produces.  Flume
reuses the common components that provide particular reliability and
resource usage properties.  Some general sources include files from
the file system, syslog and syslog-ng emulation, or the standard
output of a process.  More specific sources such as IRC channels and
Twitter streams can also be added.  Similarly, there are many output
destinations for events.  Although HDFS is the primary output
destination, events can be sent to local files, or to monitoring and
alerting applications such as Ganglia or communication channels such
as IRC.</p><p>To enable easy integration with HDFS, MapReduce, and Hive, Flume
provides simple mechanisms for output file management and output
format management.  Data gathered by Flume can be processed easily
with Hadoop and Hive.</p></div><div class="section" title="1.6. Section summary"><div class="titlepage"><div><div><h3 class="title"><a name="_section_summary"></a>1.6. Section summary</h3></div></div></div><p>The preceding Introduction section describes the high level goals and
features of Flume. The following sections of this guide describe how
to set up and use Flume:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Step-by-step tutorial that introduces a single Flume node
</li><li class="listitem">
Introduction to the Flume Master and a pseudo-distributed mode that
  includes multiple nodes coordinated by the Flume Master
</li><li class="listitem">
Description of a fully-distributed setup that also removes single
  points of failure
</li><li class="listitem">
Flume use cases and a description of how to integrate Flume with
  existing sources of data
</li><li class="listitem">
How to set up Flume&#8217;s output so that integration with heavyweight
  analysis systems such as Hadoop and Hive
</li><li class="listitem">
How to deploy Flume, set up arbitrary flows, and a specification of
  Flume&#8217;s data flow specification language
</li><li class="listitem">
Catalog of components available via the language
</li><li class="listitem">
A description of experimental features
</li><li class="listitem">
Troubleshooting information
</li></ul></div></div></div><div class="section" title="2. Flume Single Node Quick Start"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Quickstart"></a>2. Flume Single Node Quick Start</h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_sources_and_the_literal_dump_literal_command">2.1. Sources and the <code class="literal">dump</code> command</a></span></dt><dd><dl><dt><span class="section"><a href="#_reading_from_a_text_file_literal_text_literal">2.1.1. Reading from a text file, <code class="literal">text</code></a></span></dt><dt><span class="section"><a href="#_tailing_a_file_name_literal_tail_literal_and_literal_multitail_literal">2.1.2. Tailing a file name, <code class="literal">tail</code> and <code class="literal">multitail</code></a></span></dt><dt><span class="section"><a href="#_synthetic_sources_literal_synth_literal">2.1.3. Synthetic sources, <code class="literal">synth</code></a></span></dt><dt><span class="section"><a href="#_syslog_as_a_source_literal_syslogudp_literal_and_literal_syslogtcp_literal">2.1.4. Syslog as a source, <code class="literal">syslogUdp</code> and <code class="literal">syslogTcp</code></a></span></dt></dl></dd><dt><span class="section"><a href="#_anatomy_of_an_event">2.2. Anatomy of an Event</a></span></dt><dt><span class="section"><a href="#_section_summary_2">2.3. Section Summary</a></span></dt></dl></div><p>In this section, you will learn how to get a single Flume node running and
transmitting data. You will also learn about some data <span class="strong"><strong>sources</strong></span>, and how to
configure Flume flows on a per-node basis.</p><p>Each logical node consists of a event-producing <span class="strong"><strong>source</strong></span> and an event-
consuming <span class="strong"><strong>sink</strong></span>.  Nodes pull data from their sources, and push data out
through their sink.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>This section assumes that the Flume node and Flume Master are running in
the foreground and not as daemons. You can stop the daemons by using <span class="emphasis"><em>/etc/
init.d/flume-master stop</em></span> and <span class="emphasis"><em>/etc/init.d/flume-node stop</em></span>.</p></td></tr></table></div><div class="section" title="2.1. Sources and the dump command"><div class="titlepage"><div><div><h3 class="title"><a name="_sources_and_the_literal_dump_literal_command"></a>2.1. Sources and the <code class="literal">dump</code> command</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_reading_from_a_text_file_literal_text_literal">2.1.1. Reading from a text file, <code class="literal">text</code></a></span></dt><dt><span class="section"><a href="#_tailing_a_file_name_literal_tail_literal_and_literal_multitail_literal">2.1.2. Tailing a file name, <code class="literal">tail</code> and <code class="literal">multitail</code></a></span></dt><dt><span class="section"><a href="#_synthetic_sources_literal_synth_literal">2.1.3. Synthetic sources, <code class="literal">synth</code></a></span></dt><dt><span class="section"><a href="#_syslog_as_a_source_literal_syslogudp_literal_and_literal_syslogtcp_literal">2.1.4. Syslog as a source, <code class="literal">syslogUdp</code> and <code class="literal">syslogTcp</code></a></span></dt></dl></div><p>Start by getting a Flume node running that echoes data written to standard
input from the console back out to the console on stdout. You do this by using
the <code class="literal">dump</code> command.</p><pre class="screen">$ flume dump console</pre><div class="tip" title="Tip" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Tip"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Tip]" src="images/tip.png"></td><th align="left">Tip</th></tr><tr><td align="left" valign="top"><p>The Flume program has the general form <code class="literal">flume &lt;command&gt; [args ...]</code>.  If
you installed from the tarball package, the command can be found in
<code class="literal">$FLUME_HOME/bin/</code>. If you installed from either RPM or DEB, then <code class="literal">flume</code>
should already be in your path.</p></td></tr></table></div><div class="tip" title="Tip" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Tip"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Tip]" src="images/tip.png"></td><th align="left">Tip</th></tr><tr><td align="left" valign="top"><p>The example above uses the <code class="literal">dump</code> command and <code class="literal">console</code> is the argument.
The command&#8217;s syntax is <code class="literal">flume dump &lt;source&gt;</code>. It prints data from <code class="literal">&lt;source&gt;</code>
to the console.</p></td></tr></table></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Some flume configurations by default write to local disk.
Initially the default is <span class="emphasis"><em>/tmp/flume</em></span>.  This is good for initial
testing but for production environments the <code class="literal">flume.agent.logdir</code>
property should be set to a more durable location.</p></td></tr></table></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>If the node refuses to run and exits with this message,
<code class="literal">agent.FlumeNode: Aborting: Unexpected problem with
environment.Failure to write in log directory: <span class="emphasis"><em>/tmp/flume</em></span>.  Check
permissions?</code>, then check the <code class="literal">/ tmp/flume</code> directory to make sure you
have write permissions to it (change the owner or have the user join
the group).  This is, by default, where various logging information is
kept.</p></td></tr></table></div><p>You have started a Flume node where <code class="literal">console</code> is the source of incoming data.
When you run it, you should see some logging messages displayed to the
console.  For now, you can ignore messages about Masters, back-off and failed
connections (these are explained in later sections).  When you type at the
console and press a new line, you should see a new log entry line appear
showing the data that you typed. If you entered <code class="literal">This is a test</code>, it should
look similar to this:</p><pre class="screen">hostname [INFO Thu Nov 19 08:37:13 PST 2009] This is a test</pre><p>To exit the program, press ^C.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Some sources do not automatically exit and require a manual ^C to
exit.</p></td></tr></table></div><div class="section" title="2.1.1. Reading from a text file, text"><div class="titlepage"><div><div><h4 class="title"><a name="_reading_from_a_text_file_literal_text_literal"></a>2.1.1. Reading from a text file, <code class="literal">text</code></h4></div></div></div><p>You can also specify other sources of events.  For example, if you want a text
file where each line represents a new event, run the following command.</p><pre class="screen">$ flume dump 'text("/etc/services")'</pre><p>This command reads the file, and then outputs each line as a new event.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>The default console output escapes special characters with Java-style
escape sequences. Characters such as <span class="emphasis"><em>"</em></span> and <span class="emphasis"><em>\</em></span> are prefaced with an extra
<span class="emphasis"><em>\</em></span>.</p></td></tr></table></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>You can try this command with other files such as <code class="literal">/var/log/messages</code>,
<code class="literal">/var/log/syslog</code>, or <code class="literal">/var/log/hadoop/hadoop.log</code> also.  However, Flume must
run with appropriate permissions to read the files.</p></td></tr></table></div></div><div class="section" title="2.1.2. Tailing a file name, tail and multitail"><div class="titlepage"><div><div><h4 class="title"><a name="_tailing_a_file_name_literal_tail_literal_and_literal_multitail_literal"></a>2.1.2. Tailing a file name, <code class="literal">tail</code> and <code class="literal">multitail</code></h4></div></div></div><p>If you want to tail a file instead of just reading it, specify another source
by using <code class="literal">tail</code> instead of <code class="literal">text</code>.</p><pre class="screen">$ flume dump 'tail("testfile")'</pre><p>This command pipes data from the file into Flume and then out to the console.</p><p>This message appears: "File <span class="emphasis"><em>testfile</em></span> does not currently exist, waiting for
file to appear".</p><p>In another terminal, you can create and write data to the file:</p><pre class="screen">$ echo Hello world! &gt;&gt; testfile</pre><p>New data should appear.</p><p>When you delete the file:</p><pre class="screen">$ rm testfile</pre><p>The <code class="literal">tail</code> sink detects this.  If you then recreate the file, the <code class="literal">tail</code>
source detects the new file and follows it:</p><pre class="screen">$ echo Hello world again! &gt;&gt; testfile</pre><p>You should see your new message appear in the Flume node console.</p><p>You can also use the <code class="literal">multitail</code> source to follow multiple files by file name:</p><pre class="screen">$ flume dump 'multitail("test1", "test2")'</pre><p>And send it data coming from the two different files:</p><pre class="screen">$ echo Hello world test1! &gt;&gt; test1
$ echo Hello world test2! &gt;&gt; test2</pre></div><div class="section" title="2.1.3. Synthetic sources, synth"><div class="titlepage"><div><div><h4 class="title"><a name="_synthetic_sources_literal_synth_literal"></a>2.1.3. Synthetic sources, <code class="literal">synth</code></h4></div></div></div><p>Here&#8217;s one more example where you use the <code class="literal">synth</code> sources to generate events:</p><pre class="screen">$ flume dump 'asciisynth(20,30)'</pre><p>You should get 20 events, each with 30 random ASCII bytes.</p></div><div class="section" title="2.1.4. Syslog as a source, syslogUdp and syslogTcp"><div class="titlepage"><div><div><h4 class="title"><a name="_syslog_as_a_source_literal_syslogudp_literal_and_literal_syslogtcp_literal"></a>2.1.4. Syslog as a source, <code class="literal">syslogUdp</code> and <code class="literal">syslogTcp</code></h4></div></div></div><p>As with files, you can also accept data from well known wire formats such as
syslog. For example, you can start a traditional syslog-like UDP server
listening on port 5140 (the normal syslog UDP port is the privileged port 514)
by running this command:</p><pre class="screen">$ flume dump 'syslogUdp(5140)'</pre><p>You can feed the source data by using netcat to send syslog formatted data as
shown in the example below:</p><pre class="screen">$ echo "<a name="CO1-1"></a><img src="images/callouts/1.png" alt="1" border="0">hello via syslog"  | nc -u localhost 5140</pre><div class="tip" title="Tip" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Tip"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Tip]" src="images/tip.png"></td><th align="left">Tip</th></tr><tr><td align="left" valign="top"><p>You may need to press ^C to exit this command.</p></td></tr></table></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>The extra <code class="literal">&lt;37&gt;</code> is a syslog wireformat encoding of a message category
and priority level.</p></td></tr></table></div><p>Similarly, you can set up a syslog-ng compatible source that listens on TCP
port 5140 (the normal syslog-ng TCP port is the privileged port 514):</p><pre class="screen">$ flume dump 'syslogTcp(5140)'</pre><p>And send it data:</p><pre class="screen">$ echo "<a name="CO1-2"></a><img src="images/callouts/1.png" alt="1" border="0">hello via syslog" | nc -t localhost 5140</pre><div class="tip" title="Tip" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Tip"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Tip]" src="images/tip.png"></td><th align="left">Tip</th></tr><tr><td align="left" valign="top"><p>You may need to press ^C to exit this command.</p></td></tr></table></div><p>Syslog backwards-compatibility allows data normally created from syslog,
rsyslog, or syslog-ng to be sent to and processed by Flume.</p></div></div><div class="section" title="2.2. Anatomy of an Event"><div class="titlepage"><div><div><h3 class="title"><a name="_anatomy_of_an_event"></a>2.2. Anatomy of an Event</h3></div></div></div><p>This section describes a number of sources of data that Flume can interoperate
with. Before going any further, it will be helpful for you to understand what
Flume is actually sending and processing internally.</p><p>Flume internally converts every external source of data into a stream of
<span class="strong"><strong>events</strong></span>. Events are Flume&#8217;s unit of data and are a simple and flexible
representation. An event is composed of a <span class="strong"><strong>body</strong></span> and <span class="strong"><strong>metadata</strong></span>. The event
body is a string of bytes representing the content of an event. For example, a
line in a log file is represented as an event whose body was the actual byte
representation of that line. The event metadata is a table of key / value
pairs that capture some detail about the event, such as the time it was
created or the name of the machine on which it originated. This table can be
appended as an event travels along a Flume flow, and the table can be read to
control the operation of individual components of that flow. For example, the
machine name attached to an event can be used to control the output path where
the event is written at the end of the flow.</p><p>An event&#8217;s body can be up to 32KB long - although this limit can be controlled
via a system property, it is recommended that it is not changed in order to
preserve performance.</p></div><div class="section" title="2.3. Section Summary"><div class="titlepage"><div><div><h3 class="title"><a name="_section_summary_2"></a>2.3. Section Summary</h3></div></div></div><p>In this section, you learned how to use Flume&#8217;s <code class="literal">dump</code> command to print data
from a variety of different input sources to the console. You also learned
about the <span class="strong"><strong>event</strong></span>, the fundamental unit of data transfer in Flume.</p><p>The following table summarizes the sources described in this section.</p><div class="variablelist" title="Flume Event"><p class="title"><b>Flume Event</b></p><dl><dt><span class="term">
Sources <code class="literal">console</code> 
</span></dt><dd>
Stdin console
</dd><dt><span class="term">
<code class="literal">text("filename")</code> 
</span></dt><dd>
One shot text file source.  One line is one event
</dd><dt><span class="term">
<code class="literal">tail("filename")</code> 
</span></dt><dd>
Similar to Unix&#8217;s <code class="literal">tail -F</code>. One line is one event.
Stays open for more data and follows filename if file rotated.
</dd><dt><span class="term">
<code class="literal">multitail("file1"[, "file2"[, &#8230;]])</code> 
</span></dt><dd>
Similar to <code class="literal">tail</code> source but follows
multiple files.
</dd><dt><span class="term">
<code class="literal">asciisynth(msg_count,msg_size)</code> 
</span></dt><dd>
A source that synthetically generates
msg_count random messages of size msg_size.  This converts all characters into
printable ASCII characters.
</dd><dt><span class="term">
<code class="literal">syslogUdp(port)</code> 
</span></dt><dd>
Syslog over UDP port, port.  This is syslog compatible.
</dd><dt><span class="term">
<code class="literal">syslogTcp(port)</code> 
</span></dt><dd>
Syslog over TCP port, port. This is syslog-ng compatible.
</dd></dl></div></div></div><div class="section" title="3. Pseudo-distributed Mode"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_pseudo_distributed_mode"></a>3. Pseudo-distributed Mode</h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_starting_pseudo_distributed_flume_daemons">3.1. Starting Pseudo-distributed Flume Daemons</a></span></dt><dd><dl><dt><span class="section"><a href="#_the_master">3.1.1. The Master</a></span></dt><dt><span class="section"><a href="#_the_flume_node">3.1.2. The Flume Node</a></span></dt></dl></dd><dt><span class="section"><a href="#_configuring_a_node_via_the_master">3.2. Configuring a Node via the Master</a></span></dt><dt><span class="section"><a href="#_introducing_sinks">3.3. Introducing Sinks</a></span></dt><dt><span class="section"><a href="#_aggregated_configurations">3.4. Aggregated Configurations</a></span></dt><dt><span class="section"><a href="#_tiering_flume_nodes_agents_and_collectors">3.5. Tiering Flume Nodes: Agents and Collectors</a></span></dt><dt><span class="section"><a href="#_section_summary_3">3.6. Section Summary</a></span></dt></dl></div><p>Flume is intended to be run as a distributed system with processes spread out
across <span class="emphasis"><em>many</em></span> machines.  It can also be run as several processes on a <span class="emphasis"><em>single</em></span>
machine, which is called &#8220;pseudo-distributed&#8221; mode.  This mode is useful for
debugging Flume data flows and getting a better idea of how Flume components
interact.</p><p>The previous section described a Flume node and introduced the concept of
Flume sources.  This section introduces some new concepts required for a
distributed setup: the Flume <span class="strong"><strong>master</strong></span> server, the specification of sources and
<span class="strong"><strong>sinks</strong></span>, and connecting multiple Flume <span class="strong"><strong>nodes</strong></span>.</p><div class="section" title="3.1. Starting Pseudo-distributed Flume Daemons"><div class="titlepage"><div><div><h3 class="title"><a name="_starting_pseudo_distributed_flume_daemons"></a>3.1. Starting Pseudo-distributed Flume Daemons</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_the_master">3.1.1. The Master</a></span></dt><dt><span class="section"><a href="#_the_flume_node">3.1.2. The Flume Node</a></span></dt></dl></div><p>There are two kinds of processes in the system: the Flume <span class="strong"><strong>master</strong></span> and the
Flume <span class="strong"><strong>node</strong></span>.  The Flume Master is the central management point and controls
the data flows of the nodes.  It is the single logical entity that holds
global state data and controls the Flume node data flows and monitors Flume
nodes.  Flume nodes serve as the data path for streams of events.  They can be
the sources, conduits, and consumers of event data.  The nodes periodically
contact the Master to transmit a heartbeat and to get their data flow
configuration.</p><p>In order to get a distributed Flume system working, you must start a single
Flume Master and some Flume nodes that interact with the Master. You&#8217;ll start
with a Master and one Flume node and then expand.</p><div class="section" title="3.1.1. The Master"><div class="titlepage"><div><div><h4 class="title"><a name="_the_master"></a>3.1.1. The Master</h4></div></div></div><p>The Master can be manually started by executing the following command:</p><pre class="screen">$ flume master</pre><p>After the Master is started, you can access it by pointing a web browser to
<a class="ulink" href="http://localhost:35871/" target="_top">http://localhost:35871/</a>.  This web page displays the status of all Flume nodes
that have contacted the Master, and shows each node&#8217;s currently assigned
configuration.  When you start this up without Flume nodes running, the status
and configuration tables will be empty.</p><div class="informalfigure"><div class="mediaobject"><img src="master-empty.png" alt="master-empty.png"></div></div><p>The web page contains four tables&#8201;&#8212;&#8201;the <span class="emphasis"><em>Node status</em></span> table, the <span class="emphasis"><em>Node
configuration</em></span> table, the <span class="emphasis"><em>Physical/Logical Node mapping</em></span> table, and a
<span class="emphasis"><em>Command history</em></span> table.  The information in these tables represent the
current global state of the Flume system.</p><p>The Master&#8217;s <span class="emphasis"><em>Node status</em></span> table contains the names of all of the Flume Nodes
talking to the Master, their current configuration version (initially "none"),
their status (such as IDLE), and when it last reported to the Master.  The
name of each Flume node should be the same as running <code class="literal">hostname</code> from Unix
prompt.</p><p>The Master&#8217;s <span class="emphasis"><em>Node configuration</em></span> table contains the logical name of a node,
the configuration version assigned to it, and a specification of its sources
and its sinks.  Initially, this table is empty, but after you change values
you can view this web page to see the updates. There are two sets of columns -
- the user entered version/source/sink, and translated version/source/sink. A
later section of this guide describes translated configs.</p><p>The Master&#8217;s <span class="emphasis"><em>Physical/Logical Node mapping</em></span> table contains the mapping of
logical nodes to their physical nodes.</p><p>The Master&#8217;s <span class="emphasis"><em>Command history</em></span> table contains the state of commands. In
general, <span class="strong"><strong>commands</strong></span> modify the Master&#8217;s global state.  Commands are processed
serially on a Master and are assigned a unique command ID number. Each command
has a state (for example, SUCCEEDED, FAILED, or PENDING), a command line, and
a message which often contains information about its execution attempt.</p></div><div class="section" title="3.1.2. The Flume Node"><div class="titlepage"><div><div><h4 class="title"><a name="_the_flume_node"></a>3.1.2. The Flume Node</h4></div></div></div><p>To start a Flume node, invoke the following command in another terminal.</p><pre class="screen">$ flume node_nowatch</pre><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Normally, you start a node using <code class="literal">flume node</code> or run the node as a
daemon.  For these examples, you disable the watchdog feature by starting with
the <code class="literal">node_nowatch</code> option.  This option enables you to interact with a node
via the console. The other options disable <code class="literal">stdin</code>.</p></td></tr></table></div><p>To check whether a Flume node is up, point your browser to the <span class="emphasis"><em>Flume Node
status page</em></span> at <a class="ulink" href="http://localhost:35862/" target="_top">http://localhost:35862/</a>.  Each node displays its own data on a
single table that includes diagnostics and metrics data about the node, its
data flows, and the system metrics about the machine it is running on.</p><p>If the node is up, you should also refresh the Master&#8217;s status page (http://
localhost:35871) to make sure that the node has contacted the Master. You
brought up one node (assume the node is named <code class="literal"><span class="emphasis"><em>host</em></span></code>), so you should have
one node listed in the Master&#8217;s node status table, and an entry in the logical
node mapping table that links the <code class="literal"><span class="emphasis"><em>host</em></span></code> logical node to the <code class="literal"><span class="emphasis"><em>host</em></span></code>
physical nodes.</p></div></div><div class="section" title="3.2. Configuring a Node via the Master"><div class="titlepage"><div><div><h3 class="title"><a name="_configuring_a_node_via_the_master"></a>3.2. Configuring a Node via the Master</h3></div></div></div><p>Requiring nodes to contact the Master to get their configuration enables you
to dynamically change the configuration of nodes without having to log in to
the remote machine to restart the daemon.  You can quickly change the node&#8217;s
previous data flow configuration to a new one.</p><p>The following describes how to "wire" nodes using the Master&#8217;s web interface.</p><p>On the Master&#8217;s web page, click on the config link. You are presented with two
forms.  These are web interfaces for setting the node&#8217;s data flows.  When
Flume nodes contact the Master, they will notice that the data flow version
has changed, instantiate, and activate the configuration.</p><p>For this example, you will do the steps you did in the Quick Start section.
Enter the following values into the "Configure a node" form, and then click
Submit.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Node name:
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal"><span class="emphasis"><em>host</em></span></code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Source:
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">console</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    Sink:
    </td><td style="" align="left">
    <code class="literal">console</code>
    </td></tr></tbody></table></div><p>Refresh the Master page and notice that the version stamp changed to a current
time, and that the src and sink fields of the configs updated.  After the
status changes to "ACTIVE", it is ready to receive console traffic.</p><p>On the master, a node can be in one of several states:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
HELLO : A new node instance initially contacted the master.
</li><li class="listitem">
IDLE : A node has completed its configuration or has no configuration.
</li><li class="listitem">
CONFIGURING: A node has received a configuration and is activating the configuration.
</li><li class="listitem">
ACTIVE: A node is actively pulling data from the source and pushing data into the sink.
</li><li class="listitem">
LOST: A node has not contacted the master for an extended period of time (default is after 10x the expected heartbeat period&#8201;&#8212;&#8201;50s by default)
</li><li class="listitem">
DECOMMISSIONED: A node has been purposely decommissioned from a master.
</li><li class="listitem">
ERROR: A node has stopped in an error state.
</li></ul></div><p>On the terminal where your Flume node is running, you should be able to type a
few lines and then get output back showing your new log message.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Node name:
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal"><span class="emphasis"><em>host</em></span></code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Source:
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">text("/etc/services")</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    Sink:
    </td><td style="" align="left">
    <code class="literal">console</code>
    </td></tr></tbody></table></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>You may need to press Enter in the Flume node console.</p></td></tr></table></div><p>Or use these new values to tail a file:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Node name:
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal"><span class="emphasis"><em>host</em></span></code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Source:
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">tail("/etc/services")</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    Sink:
    </td><td style="" align="left">
    <code class="literal">console</code>
    </td></tr></tbody></table></div><p>You can now change the configuration of different nodes in the system to
gather data from a variety of sources by going through the Master.</p></div><div class="section" title="3.3. Introducing Sinks"><div class="titlepage"><div><div><h3 class="title"><a name="_introducing_sinks"></a>3.3. Introducing Sinks</h3></div></div></div><p>Thus far, you have seen that Flume has a variety of sources that generate or
accept new events that are fed into the system.  You have limited the output
of these messages to the <code class="literal">console</code> sink.  As you would expect, Flume also
provides a wide variety of event <span class="strong"><strong>sinks</strong></span>&#8201;&#8212;&#8201;destinations for all of the
events.</p><p>There are many possible destinations for events&#8201;&#8212;&#8201;to local disk, to HDFS, to
the console, or forwarding across the network. You use the sink abstractions
as an interface for forwarding events to any of these destinations.</p><p>You can connect different sources to different sinks by specifying the new
configuration and submitting it to the Master.  For example, with the data
flow below, you can make a copy of <code class="literal">/etc/services</code>.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Node name:
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal"><span class="emphasis"><em>host</em></span></code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Source:
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">text("/etc/services")</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    Sink:
    </td><td style="" align="left">
    <code class="literal">text("services.copy")</code>
    </td></tr></tbody></table></div><div class="warning" title="Warning" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Warning"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Warning]" src="images/warning.png"></td><th align="left">Warning</th></tr><tr><td align="left" valign="top"><p>The <code class="literal">text</code> sinks overwrite if a file previously exists.</p></td></tr></table></div><p>Notice that the file is copied as is. Sinks have optional arguments for output
format, which you can use to write data in other serialization formats.  For
example, instead of copying a file using the default "raw" formatter, you can
format the output using other formatters such as "avrojson" (the Avro
serialization json format), "avrodata" (the Avro serialization binary data
format), or a "debug" mode (this is the formatter used by the console sink).</p><p>If you enter:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Node name:
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal"><span class="emphasis"><em>host</em></span></code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Source:
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">text("/etc/services")</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    Sink:
    </td><td style="" align="left">
    <code class="literal">console("avrojson")</code>
    </td></tr></tbody></table></div><p>You get the file with each record in JSON format displayed to the console.</p><p>If you enter:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Node name:
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal"><span class="emphasis"><em>host</em></span></code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Source:
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">text("/etc/services")</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    Sink:
    </td><td style="" align="left">
    <code class="literal">text("services.json", "avrojson")</code>
    </td></tr></tbody></table></div><p>The newly written local <code class="literal">services.json</code> file is output in avro&#8217;s json format.</p><div class="sidebar" title="Default Arguments"><p class="title"><b>Default Arguments</b></p><p>Many sinks, sources, and decorators have arguments that modify their behavior.
Some of these arguments are required; others are optional and use a default.</p><p>The following notation describes these parameters:</p><p><code class="literal"><span class="emphasis"><em>name</em></span> (<span class="emphasis"><em>req1</em></span>, <span class="emphasis"><em>req2</em></span>[, <span class="emphasis"><em>opt1</em></span>[, <span class="emphasis"><em>opt2</em></span>=<span class="emphasis"><em>foo</em></span>]] )</code></p><p>where a component with name <span class="emphasis"><em>name</em></span>, has two required arguments, <span class="emphasis"><em>req1</em></span> and
<span class="emphasis"><em>req2</em></span>. There are two optional arguments, <span class="emphasis"><em>opt1</em></span> and <span class="emphasis"><em>opt2</em></span>.  However, <span class="emphasis"><em>opt1</em></span>
<span class="strong"><strong>must</strong></span> be specified if <span class="emphasis"><em>opt2</em></span> is specified.  <span class="emphasis"><em>foo</em></span> in this case is the default
argument for <span class="emphasis"><em>opt2</em></span>+.</p><p>For the <code class="literal">console</code> examples above, console could be specified as:</p><p><code class="literal">console[(<span class="emphasis"><em>format</em></span>="debug")]</code></p><p>and text could be specified as:</p><p><code class="literal">text("<span class="emphasis"><em>file</em></span>"[, <span class="emphasis"><em>format</em></span>="raw"])</code></p></div><p>There are several sinks you can use. The following list is a subset; see the
Appendix for more sinks.</p><div class="horizontal"><a name="id413903"></a><p class="title"><b>Table 1. Flume Event Sinks</b></p><div class="horizontal-contents"><table summary="Flume Event Sinks" style="border: none;"><colgroup><col><col></colgroup><tbody valign="top"><tr><td style="" valign="top">
<p>
<code class="literal">null</code>  
</p>
</td><td style="" valign="top">
<p>
Null sink. Events are dropped.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">console[("format")]</code> 
</p>
</td><td style="" valign="top">
<p>
Console sink.  Display to console&#8217;s stdout. The
"format" argument is optional and defaults to the "debug" output format.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">text("<span class="emphasis"><em>txtfile</em></span>"[,"format"])</code> 
</p>
</td><td style="" valign="top">
<p>
Textfile sink.  Write the events to text
file <code class="literal"><span class="emphasis"><em>txtfile</em></span></code> using output format "format".  The default format is "raw"
event bodies with no metadata.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">dfs("<span class="emphasis"><em>dfsfile</em></span>")</code> 
</p>
</td><td style="" valign="top">
<p>
DFS seqfile sink.  Write serialized Flume events
to a dfs path such as <code class="literal">hdfs://namenode/file</code> or <code class="literal">file:///file</code> in
Hadoop&#8217;s seqfile format.  Note that because of the HDFS write
semantics, no data for this sink write until the sink is closed.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">syslogTcp("<span class="emphasis"><em>host</em></span>",<span class="emphasis"><em>port</em></span>)</code> 
</p>
</td><td style="" valign="top">
<p>
Syslog TCP sink.  Forward to events to <code class="literal">host</code>
on TCP port <code class="literal">port</code> in syslog wire format (syslog-ng compatible), or to other
Flume nodes setup to listen for syslogTcp.
</p>
</td></tr></tbody></table></div></div><br class="horizontal-break"><div class="warning" title="Warning" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Warning"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Warning]" src="images/warning.png"></td><th align="left">Warning</th></tr><tr><td align="left" valign="top"><p>Using <code class="literal">dfs</code> has some restrictions and requires some extra
setup.  Files contents will not become available until after the sink
has been closed.  See the Troubleshooting section for details.</p></td></tr></table></div></div><div class="section" title="3.4. Aggregated Configurations"><div class="titlepage"><div><div><h3 class="title"><a name="_aggregated_configurations"></a>3.4. Aggregated Configurations</h3></div></div></div><p>Using the form for single node configuration of a small number of
machines is manageable, but for larger numbers it is more efficient to
maintain or auto-generate the configuration for all of the machines in
a single file. Flume allows you to set the configurations of many
machines in a single aggregated configuration submission.</p><p>Instead of the method you used in the "Configuring a Node via the Master"
section, put the following configuration line into the "Configure Nodes" form
and then submit:</p><pre class="screen">host : text("/etc/services") | console ;</pre><p>Or:</p><pre class="screen">host: text("/etc/services") | text("services.copy");</pre><p>The general format is:</p><pre class="screen">&lt;node1&gt; : &lt;source&gt; | &lt;sink&gt; ;
&lt;node2&gt; : &lt;source&gt; | &lt;sink&gt; ;
&lt;node3&gt; : &lt;source&gt; | &lt;sink&gt; ;
...</pre><p>The remainder of this guide uses this format to configure nodes.</p></div><div class="section" title="3.5. Tiering Flume Nodes: Agents and Collectors"><div class="titlepage"><div><div><h3 class="title"><a name="_tiering_flume_nodes_agents_and_collectors"></a>3.5. Tiering Flume Nodes: Agents and Collectors</h3></div></div></div><p>A simple network connection is abstractly just another sink.  It would be
great if sending events over the network was easy, efficient, and reliable.
In reality, collecting data from a distributed set of machines and relying on
networking connectivity greatly increases the likelihood and kinds of failures
that can occur.  The bottom line is that providing reliability guarantees
introduces complexity and many tradeoffs.</p><p>Flume simplifies these problems by providing a predefined topology and tunable
reliability that only requires you to give each Flume node a role.  One simple
Flume node topology classifies Flume nodes into two roles&#8201;&#8212;&#8201;a Flume <span class="strong"><strong>agent</strong></span>
tier and the Flume <span class="strong"><strong>collector</strong></span> tier. The agent tier Flume nodes are co-located
on machines with the service that is producing logs.  For example, you could
specify a Flume agent configured to have <code class="literal">syslogTcp</code> as a source, and
configure the syslog generating server to send its logs to the specified local
port.  This Flume agent would have an <code class="literal">agentSink</code> as its sink which is
configured to forward data to a node in the collector tier.</p><p>Nodes in the collector tier listen for data from multiple agents, aggregates
logs, and then eventually write the data to HDFS.</p><div class="warning" title="Warning" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Warning"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Warning]" src="images/warning.png"></td><th align="left">Warning</th></tr><tr><td align="left" valign="top"><p>In the next few sections, all <code class="literal"><span class="emphasis"><em>host</em></span></code> arguments currently use the
physical IP or DNS name of the node as opposed to the Flume node name (set by
<code class="literal">-n name</code> option). The default Flume node name is the host name unless you
override it on the command line.</p></td></tr></table></div><p>To demonstrate the new sinks in pseudo-distributed mode, you will instantiate
another Flume node (a physical node) on the local box.  To do this, you need
to start a Flume node with some extra options.  The command line below starts
a physical node named <code class="literal">collector</code> (<code class="literal">-n collector</code>):</p><pre class="screen">$ flume node_nowatch -n collector</pre><p>On the Master&#8217;s web page, you should eventually see two nodes: <code class="literal"><span class="emphasis"><em>host</em></span></code> and
<code class="literal">collector</code>.  The Flume Node status web pages should be available at http://
localhost:35862 and <a class="ulink" href="http://localhost:35863" target="_top">http://localhost:35863</a>.  Port bindings are dependent on
instantiation order&#8201;&#8212;&#8201;the first physical node instantiated binds on 35862 and
the second binds to 35863.</p><p>Next, configure <code class="literal">collector</code> to take on the role of a collector, set up
<code class="literal"><span class="emphasis"><em>host</em></span></code> to send data from the console to the collector by using the
aggregated multiple configuration form.  The agent uses the <code class="literal">agentSink</code>, a
high reliability network sink.  The collector node&#8217;s source is configured to
be a <code class="literal">collectorSource</code>, and its sink is configured to be the <code class="literal">console</code>.</p><pre class="screen">host : console | agentSink("localhost",35853) ;
collector : collectorSource(35853) | console ;</pre><div class="informalfigure"><div class="mediaobject"><img src="tiers-console.png" alt="tiers-console.png"></div></div><p>When you type lines in <code class="literal"><span class="emphasis"><em>host</em></span></code>'s console, events are forwarded to the
collector.  Currently, there is a bit of latency (15s or so) before the
forwarded message shows up on <code class="literal">collector</code>.  This is actually a configurable
setting whose default is set to a value that is amenable for high event
throughputs.  Later sections describe how to tune Flume.</p><p>You have successfully made a event flow from an agent to the collector.</p><div class="tip" title="Tip" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Tip"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Tip]" src="images/tip.png"></td><th align="left">Tip</th></tr><tr><td align="left" valign="top"><p>You can check to see if <code class="literal">agentSink</code> is working by looking in the
write- ahead logging directory for the sink.  The default location is
<code class="literal">/tmp/flume/ &lt;nodename&gt;</code>.  Because the OS periodically cleans this
directory, this configuration property (<code class="literal">flume.agent.logdir</code>) should
be changed a production environment.</p></td></tr></table></div><p>A more interesting setup is to have the agent tailing a local file (using the
<code class="literal">tail</code> source) or listening for local syslog data (using the <code class="literal">syslogTcp</code> or
<code class="literal">syslogUdp</code> sources and modifying the syslog daemon&#8217;s configuration).  Instead
of writing to a console, the collector would write to a <code class="literal">collectorSink</code>, a
smarter sink that writes to disk or HDFS, periodically rotates files, and
manages acknowledgements.</p><div class="informalfigure"><div class="mediaobject"><img src="tiers-hdfs.png" alt="tiers-hdfs.png"></div></div><p>The following configuration is for an agent that listens for syslog messages
and forwards to a collector which writes files to local directory <code class="literal">/tmp/flume/
collected/</code>.</p><pre class="screen">host : syslogTcp(5140) | agentSink("localhost",35853) ;
collector : collectorSource(35853) | collectorSink("file:///tmp/flume/collected", "syslog");</pre><p>In the following slightly modified configuration, the collector writes to an
HDFS cluster (assuming the HDFS nameNode is called <code class="literal">namenode</code>):</p><pre class="screen">host : syslogTcp(5140) | agentSink("localhost",35853) ;
collector : collectorSource(35853) | collectorSink("hdfs://namenode/user/flume/
","syslog");</pre><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>There are no guarantees that data written to an HDFS file is
durable until the HDFS file is properly closed.  Because of this, the
collector sink periodically closes a file and creates a new one in
HDFS.  The default time between file rolls (close then open new) is
30s.  If you are writing data at low throughput (&lt;2MB/s) you may want
to increase the default time by modifying the
<code class="literal">flume.collector.roll.millis</code> and <code class="literal">flume.agent.logdir.retransmit</code> time
properties in your flume-site.xml file.</p></td></tr></table></div></div><div class="section" title="3.6. Section Summary"><div class="titlepage"><div><div><h3 class="title"><a name="_section_summary_3"></a>3.6. Section Summary</h3></div></div></div><p>This section describes how to start a Master and a node, and configure a node
via the Master. The next section describes a more concise way of specifying
many configurations, explains agents and collectors, and how to build an
agent-collector pipeline on a single machine in a three-tier topology.</p></div></div><div class="section" title="4. Fully-distributed Mode"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_fully_distributed_mode"></a>4. Fully-distributed Mode</h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_static_configuration_files">4.1. Static Configuration Files</a></span></dt><dd><dl><dt><span class="section"><a href="#_using_default_values">4.1.1. Using Default Values</a></span></dt></dl></dd><dt><span class="section"><a href="#_multiple_collectors">4.2. Multiple Collectors</a></span></dt><dd><dl><dt><span class="section"><a href="#_partitioning_agents_across_multiple_collectors">4.2.1. Partitioning Agents across Multiple Collectors</a></span></dt><dt><span class="section"><a href="#_manually_specifying_failover_chains">4.2.2. Manually Specifying Failover Chains</a></span></dt><dt><span class="section"><a href="#_automatic_failover_chains">4.2.3. Automatic Failover Chains</a></span></dt></dl></dd><dt><span class="section"><a href="#_logical_configurations">4.3. Logical Configurations</a></span></dt><dd><dl><dt><span class="section"><a href="#_logical_nodes">4.3.1. Logical Nodes</a></span></dt><dt><span class="section"><a href="#_logical_sources_and_logical_sinks">4.3.2. Logical Sources and Logical Sinks</a></span></dt><dt><span class="section"><a href="#_flow_isolation">4.3.3. Flow Isolation</a></span></dt><dt><span class="section"><a href="#_section_summary_4">4.3.4. Section Summary</a></span></dt></dl></dd><dt><span class="section"><a href="#_multiple_masters">4.4. Multiple Masters</a></span></dt><dd><dl><dt><span class="section"><a href="#_standalone_mode_compared_to_distributed_mode">4.4.1. Standalone Mode Compared to Distributed Mode</a></span></dt><dt><span class="section"><a href="#_running_in_standalone_mode">4.4.2. Running in Standalone Mode</a></span></dt><dt><span class="section"><a href="#_running_in_distributed_mode">4.4.3. Running in Distributed Mode</a></span></dt><dt><span class="section"><a href="#_configuration_stores">4.4.4. Configuration Stores</a></span></dt><dt><span class="section"><a href="#_which_configuration_store_should_i_use">4.4.5. Which Configuration Store Should I Use?</a></span></dt><dt><span class="section"><a href="#_configuring_the_zbcs">4.4.6. Configuring the ZBCS</a></span></dt><dt><span class="section"><a href="#_gossip_in_distributed_mode">4.4.7. Gossip in Distributed Mode</a></span></dt><dt><span class="section"><a href="#_diagrams_how_the_masters_and_nodes_talk_to_each_other">4.4.8. Diagrams: How the Masters and Nodes talk to each other</a></span></dt><dt><span class="section"><a href="#_configuring_flume_nodes_to_connect_to_multiple_master_servers">4.4.9. Configuring Flume Nodes to Connect to Multiple Master Servers</a></span></dt></dl></dd><dt><span class="section"><a href="#_external_zookeeper_cluster">4.5. External ZooKeeper Cluster</a></span></dt><dt><span class="section"><a href="#_section_summary_5">4.6. Section Summary</a></span></dt></dl></div><p>The main goal for Flume is to collect logs and data from many
different hosts and to scale and intelligently handle different
cluster and network topologies.</p><p>To deploy Flume on your cluster, do the following steps.</p><div class="itemizedlist" title="Steps to Deploy Flume On a Cluster"><p class="title"><b>Steps to Deploy Flume On a Cluster</b></p><ul class="itemizedlist" type="disc"><li class="listitem">
Install Flume on each machine.
</li><li class="listitem">
Select one or more nodes to be the Master.
</li><li class="listitem">
Modify a static configuration file to use site specific properties.
</li><li class="listitem">
Start the Flume Master node on at least <span class="strong"><strong>one</strong></span> machine.
</li><li class="listitem">
Start a Flume node on <span class="strong"><strong>each</strong></span> machine.
</li></ul></div><p>The following section describes how to manually configure the
properties file to specify the Master for each node, and how to set
default values for parameters.  Sections afterwards describe a data
flow configuration for a larger system, how to add more capacity by
adding collectors, and how to improve the reliability of the Master by
adding multiple Masters.</p><div class="section" title="4.1. Static Configuration Files"><div class="titlepage"><div><div><h3 class="title"><a name="_static_configuration_files"></a>4.1. Static Configuration Files</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_using_default_values">4.1.1. Using Default Values</a></span></dt></dl></div><p>In the previous sections, you used Flume on a single machine with the
default configuration settings. With the default settings, nodes
automatically search for a Master on <code class="literal">localhost</code> on a standard
port. In order for the Flume nodes to find the Master in a fully
distributed setup, you must specify site-specific static configuration
settings.</p><p>Site-specific settings for Flume nodes and Masters are configured by
properties in the <code class="literal">conf/flume-site.xml</code> file found on each machine.
If this file is not present, the commands default to the settings
found in <code class="literal">conf/flume-conf.xml</code>.  In the following example, you set up
the property that points a Flume node to search for its Master at a
machine called <code class="literal">master</code>.</p><p title="conf/flume-site.xml"><b><code class="literal">conf/flume-site.xml</code>. </b>
</p><pre class="programlisting">&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl"  href="configuration.xsl"?&gt;

&lt;configuration&gt;
&lt;property&gt;
&lt;name&gt;flume.master.servers&lt;/name&gt;
&lt;value&gt;master&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;</pre><p title="conf/flume-site.xml">
</p><div class="section" title="4.1.1. Using Default Values"><div class="titlepage"><div><div><h4 class="title"><a name="_using_default_values"></a>4.1.1. Using Default Values</h4></div></div></div><p>When you are using agent/collector roles, you can add the following
configuration properties to your <code class="literal">flume-site.xml</code> file to set up the
default hosts used as collector.</p><pre class="programlisting">...
&lt;property&gt;
&lt;name&gt;flume.collector.event.host&lt;/name&gt;
&lt;value&gt;collector&lt;/value&gt;
&lt;description&gt;This is the host name of the default "remote"     collector.
&lt;/description&gt;
&lt;/property&gt;
  &lt;property&gt;
&lt;name&gt;flume.collector.port&lt;/name&gt;
&lt;value&gt;35853&lt;/value&gt;
&lt;description&gt;This default tcp port that the collector listens to     in order to receive events it is collecting.
&lt;/description&gt;
&lt;/property&gt;

...</pre><p>This will make the <code class="literal">agentSink</code> with no arguments default to using
<code class="literal">flume.collector.event.host</code> and <code class="literal">flume.collector.port</code> for their
default target and port.</p><p>In the following example, a larger setup with several agents push data
to a collector. There are seven Flume nodes&#8201;&#8212;&#8201;six in the agent tier,
and one in the collector tier.</p><div class="informalfigure"><div class="mediaobject"><img src="singleCollector.png" alt="singleCollector.png"></div></div><p>An explicit configuration fills in all of the parameters:</p><pre class="screen">agentA : src | agentSink("collector",35853);
agentB : src | agentSink("collector",35853);
agentC : src | agentSink("collector",35853);
agentD : src | agentSink("collector",35853);
agentE : src | agentSink("collector",35853);
agentF : src | agentSink("collector",35853);
collector : collectorSource(35853) | collectorSink("hdfs://namenode/flume/","srcdata");</pre><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>When specifying destinations for agentSinks, use the <span class="emphasis"><em>hostname
and port of the target machine</em></span>.  The default name for a node is its
hostname.  However, if there are multiple logical nodes, you must use
the machine&#8217;s host name, <span class="emphasis"><em>not the name of the logical node</em></span>. In the
preceding examples, agent[A-F] and collector are the <span class="emphasis"><em>physical host
names</em></span> of the machines where these configurations are running.</p></td></tr></table></div><p>You can rely on the default ports set in the configuration files:</p><pre class="screen">agentA : src | agentSink("collector");
agentB : src | agentSink("collector");
agentC : src | agentSink("collector");
agentD : src | agentSink("collector");
agentE : src | agentSink("collector");
agentF : src | agentSink("collector");
collector : collectorSource | collectorSink("hdfs://namenode/flume/","srcdata");</pre><p>You can rely on the default ports and default collector host:</p><pre class="screen">agentA : src | agentSink
agentB : src | agentSink
agentC : src | agentSink
agentD : src | agentSink
agentE : src | agentSink
agentF : src | agentSink
collector : collectorSource | collectorSink("hdfs://namenode/flume/","srcdata");</pre><div class="warning" title="Warning" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Warning"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Warning]" src="images/warning.png"></td><th align="left">Warning</th></tr><tr><td align="left" valign="top"><p>Using defaults can make writing data flow configurations more concise, but may obscure the details about how different nodes are connected to each other.</p></td></tr></table></div><div class="sidebar" title="Reliability Modes"><p class="title"><b>Reliability Modes</b></p><p>You can tune the reliability level of the agents, simply by specifying a different kind of
agent sink.  There are three levels available, and three corresponding agents:</p><div class="variablelist"><dl><dt><span class="term">
<code class="literal">agentE2ESink[("<span class="emphasis"><em>machine</em></span>"[,<span class="emphasis"><em>port</em></span>])]</code> 
</span></dt><dd>
End to end.  This version uses the WAL, relies on an acknowledgement, and will retry if no acknowledgement is received.
</dd><dt><span class="term">
<code class="literal">agentDFOSink[("<span class="emphasis"><em>machine</em></span>"[,<span class="emphasis"><em>port</em></span>])]</code> 
</span></dt><dd>
Disk Failover (store on failure).  This agent writes to disk only if it can detect a failure on the collector.  If any data is stored to disk, it periodically retires the network connection and attempts to resend.
</dd><dt><span class="term">
<code class="literal">agentBESink[("<span class="emphasis"><em>machine</em></span>"[,<span class="emphasis"><em>port</em></span>])]</code> 
</span></dt><dd>
Best Effort.  This agent does not write to disk at all, and drops messages in the event of collector failures.
</dd></dl></div><p>The previous examples use the <code class="literal">agentSink</code>.  This is an alias for the <code class="literal">agentE2ESink</code>.</p></div></div></div><div class="section" title="4.2. Multiple Collectors"><div class="titlepage"><div><div><h3 class="title"><a name="_multiple_collectors"></a>4.2. Multiple Collectors</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_partitioning_agents_across_multiple_collectors">4.2.1. Partitioning Agents across Multiple Collectors</a></span></dt><dt><span class="section"><a href="#_manually_specifying_failover_chains">4.2.2. Manually Specifying Failover Chains</a></span></dt><dt><span class="section"><a href="#_automatic_failover_chains">4.2.3. Automatic Failover Chains</a></span></dt></dl></div><p>Having multiple collectors can increase the log collection throughput
and can improve the timeliness of event delivery by increasing
collector availability.  Data collection is parallelizable; thus, load
from many agents can be shared across many several collectors.</p><div class="section" title="4.2.1. Partitioning Agents across Multiple Collectors"><div class="titlepage"><div><div><h4 class="title"><a name="_partitioning_agents_across_multiple_collectors"></a>4.2.1. Partitioning Agents across Multiple Collectors</h4></div></div></div><p>The preceding graph and dataflow spec shows a typical topology for
Flume nodes.  For reliable delivery, in the event that the collector
stops operating or disconnects from the agents, the agents would need
to store their events to their respective disks locally.  The agents
would then periodically attempt to recontact a collector.  Because the
collector is down, any analysis or processing downstream is blocked.</p><div class="informalfigure"><div class="mediaobject"><img src="multiCollector.png" alt="multiCollector.png"></div></div><p>When you have multiple collectors as in the preceding graph and
dataflow spec, downstream progress is still made even in the face of a
collector&#8217;s failure.  If collector B goes down, agent A, agent B,
agent E, and agent F continue to deliver events via collector A and
collector C respectively.  Agent C and agent D may have to queue their
logs until collector B (or its replacement) comes back online.</p><p>The following configuration partitions the work from the set of agents
across many collectors.  In this example, each of the collectors
specify the same DFS output directory and file prefixes, aggregating
all of the logs into the same directory.</p><pre class="screen">agentA : src | agentE2ESink("collectorA",35853);
agentB : src | agentE2ESink("collectorA",35853);
agentC : src | agentE2ESink("collectorB",35853);
agentD : src | agentE2ESink("collectorB",35853);
agentE : src | agentE2ESink("collectorC",35853);
agentF : src | agentE2ESink("collectorC",35853);
collectorA : collectorSource(35853) | collectorSink("hdfs://...","src");
collectorB : collectorSource(35853) | collectorSink("hdfs://...","src");
collectorC : collectorSource(35853) | collectorSink("hdfs://...","src");</pre></div><div class="section" title="4.2.2. Manually Specifying Failover Chains"><div class="titlepage"><div><div><h4 class="title"><a name="_manually_specifying_failover_chains"></a>4.2.2. Manually Specifying Failover Chains</h4></div></div></div><div class="informalfigure"><div class="mediaobject"><img src="failoverCollector.png" alt="failoverCollector.png"></div></div><p>When you have multiple collectors writing to the same storage
location, instead of having agent C and agent D queue indefinitely,
you can instead have them fail over to other collectors.  In this
scenario, you could have agent C and agent D fail over to collector A
and collector C respectively, while periodically checking to see if
collector B has returned.</p><p>To specify these setups, use agents with <span class="strong"><strong>failover chains</strong></span>. Similarly
to single collector agents, there are three levels of reliability for
the failover chain agents: <code class="literal">agentE2EChain</code>, <code class="literal">agentDFOChain</code>, and
<code class="literal">agentBEChain</code>.</p><p>In the following example, you manually specify the failover chain
using <code class="literal">agentE2EChain</code>, an agent with end-to-end reliability with
multiple failover collectors.  <code class="literal">agentA</code> in this situation will
initially attempt to send to <code class="literal">collectorA</code> on port <code class="literal">35853</code>.  The second
argument in <code class="literal">agentA</code> 's sink specifies the collector to fall back onto
if the first fails.  You can specify an arbitrary number of
collectors, but you must specify at least one.</p><pre class="screen">agentA : src | agentE2EChain("collectorA:35853","collectorB:35853");
agentB : src | agentE2EChain("collectorA:35853","collectorC:35853");
agentC : src | agentE2EChain("collectorB:35853","collectorA:35853");
agentD : src | agentE2EChain("collectorB:35853","collectorC:35853");
agentE : src | agentE2EChain("collectorC:35853","collectorA:35853");
agentF : src | agentE2EChain("collectorC:35853","collectorB:35853");
collectorA : collectorSource(35853) | collectorSink("hdfs://...","src");
collectorB : collectorSource(35853) | collectorSink("hdfs://...","src");
collectorC : collectorSource(35853) | collectorSink("hdfs://...","src");</pre><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>In this section, <code class="literal">agent[A-F]</code> and <code class="literal">collector[A-B]</code> are physical host names.</p></td></tr></table></div><p>As in the single collector case, if no port number is specified, the agent defaults to using the <code class="literal">flume.collector.port</code>.</p><pre class="screen">agentA : src | agentE2EChain("collectorA","collectorB");
agentB : src | agentE2EChain("collectorA","collectorC");
agentC : src | agentE2EChain("collectorB","collectorA");
agentD : src | agentE2EChain("collectorB","collectorC");
agentE : src | agentE2EChain("collectorC","collectorA");
agentF : src | agentE2EChain("collectorC","collectorB"); collectorA : collectorSource | collectorSink("hdfs://...","src");
collectorB : collectorSource | collectorSink("hdfs://...","src");
collectorC : collectorSource | collectorSink("hdfs://...","src");</pre><div class="sidebar" title="Reliability and failover chain semantics"><p class="title"><b>Reliability and failover chain semantics</b></p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
The <code class="literal">agentE2EChain</code> always <span class="emphasis"><em>writes to WAL first</em></span> and then attempts
  to send to the different collectors.  In the event of a collector
  failure, it fails over to another collector.
</li><li class="listitem">
The <code class="literal">agentDFOChain</code> always attempts to send data to the different
  collectors, and <span class="emphasis"><em>only writes to disk if all collectors fail</em></span>.
</li><li class="listitem">
The <code class="literal">agentBEChain</code> attempts to send message to each collector in
  succession in the event of failures.  If all collectors fail, it
  drops messages.
</li></ul></div></div></div><div class="section" title="4.2.3. Automatic Failover Chains"><div class="titlepage"><div><div><h4 class="title"><a name="_automatic_failover_chains"></a>4.2.3. Automatic Failover Chains</h4></div></div></div><p>Flume also provides a mechanism that automatically assigns failover chains based on how nodes are configured.  As collector nodes are assigned in the Flume Master, the Master attempts to distribute the agents evenly amongst the collectors.  In the face of failure, each agent is assigned a different failover chain.  This mitigates the chances of another collector becoming overloaded in the event of failure of a collector.</p><p>To specify a node to use the failover chains, use either the <code class="literal">autoE2EChain</code>, <code class="literal">autoDFOChain</code>, or <code class="literal">autoBEChain</code> agent sink.  Because the Master calculates the failover chains, these sinks take no explicit arguments.</p><pre class="screen">agentA : src | autoE2EChain ;
agentB : src | autoE2EChain ;
agentC : src | autoE2EChain ;
agentD : src | autoE2EChain ;
agentE : src | autoE2EChain ;
agentF : src | autoE2EChain ;
collectorA : autoCollectorSource | collectorSink("hdfs://...", "src");
collectorB : autoCollectorSource | collectorSink("hdfs://...", "src");
collectorC : autoCollectorSource | collectorSink("hdfs://...", "src");</pre><p>The Master updates the configuration of the agents based on the current collectors in the system.  When new collectors are added to the system, the Master updates the failover chains of agents to rebalance.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>If the Master has no nodes with <code class="literal">autoCollectorSource</code> as its source, the agent&#8217;s automatic chains will report a <code class="literal">fail("&#8230;")</code> chain which will wait for <code class="literal">autoCollectorSource</code> s to be specified.  If the nodes are not mapped, they will report a different <code class="literal">fail</code> sink notifying you that the node is unmapped (isn&#8217;t associated with a host/port).</p></td></tr></table></div><div class="tip" title="Tip" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Tip"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Tip]" src="images/tip.png"></td><th align="left">Tip</th></tr><tr><td align="left" valign="top"><p>You can see the translation of the auto*Chain configuration in the node configuration table under the translated configuration column.  This is a little declarative specification of the failure recovery behavior of the sink.  More details on this are in the Advanced section of this guide, and in future revisions the translations for the agents and other chains will also be presented.</p></td></tr></table></div></div></div><div class="section" title="4.3. Logical Configurations"><div class="titlepage"><div><div><h3 class="title"><a name="_logical_configurations"></a>4.3. Logical Configurations</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_logical_nodes">4.3.1. Logical Nodes</a></span></dt><dt><span class="section"><a href="#_logical_sources_and_logical_sinks">4.3.2. Logical Sources and Logical Sinks</a></span></dt><dt><span class="section"><a href="#_flow_isolation">4.3.3. Flow Isolation</a></span></dt><dt><span class="section"><a href="#_section_summary_4">4.3.4. Section Summary</a></span></dt></dl></div><p>Manually configuring nodes in Flume is manageable for a small number of nodes,
but can become burdensome for an operator as demands inevitably grow.
Ideally, the operator only has to assign a role to a particular machine.
Because configuration management is centralized via the Master, the Master
potentially has all the information necessary to intelligently create a node
topology and isolate flows of data from each other.</p><p>To explain how this can be done, the concept of a <span class="strong"><strong>logical node</strong></span> is
introduced. To manage communications between logical nodes, the concepts of
<span class="strong"><strong>logical sources</strong></span> and <span class="strong"><strong>logical sinks</strong></span> are introduced.  To isolate different
groups of nodes, the concept of a <span class="strong"><strong>flow</strong></span> is introduced that allows you to
group agents and collectors into separate and isolated groups.</p><div class="section" title="4.3.1. Logical Nodes"><div class="titlepage"><div><div><h4 class="title"><a name="_logical_nodes"></a>4.3.1. Logical Nodes</h4></div></div></div><p>The logical node abstraction allows for each JVM instance (a physical node) to
contain multiple logical nodes.  This allows for the processing of many source
sink combinations on many threads of execution to occur on a single JVM
instance.</p><p>Each logical node has a name that may be completely different from its
physical name or hostname.  You now need new operations that enable you to
spawn a new node, map logical nodes to physical nodes, and decommission
existing logical nodes.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>The following commands are entered via the web interface using the "raw
command" web page on the Master.  You might prefer using the Flume command
shell (described in a later section) for these operations.  The same commands
described in this section can be entered in web interface or entered at the
command shell by prefixing the command with <span class="emphasis"><em>exec</em></span>.</p></td></tr></table></div><p>Suppose that initially you know you want an agent-collector topology, but you
don&#8217;t know the particular names of the exact machines.  For now, you can
specify the configuration of the logical nodes without specifying any physical
machine names.</p><pre class="screen">agent1 : _source_ | autoBEChain ;
collector1 : collectorSource | collectorSink("hdfs://....") ;</pre><p>Later you learn that host1 is the name of the agent1 machine and host2 is the
name of the collector machine.  You can <span class="emphasis"><em>map</em></span> logical nodes onto the
physical Flume instances on host1 and host2 by issuing the following map
commands:</p><pre class="screen">map host1 agent1
map host2 collector1</pre><p>Afterwards, the node status table should display a new row of information for
each logical node.  Each logical node reports its own execution state,
configuration, and heartbeat.  There is also a new entry in the logical node
mapping table showing that the logical node has been placed on the specified
physical node. To configure the node&#8217;s sources and sinks, use exactly the same
mechanisms described in the previous sections.</p><p>You can also remove a logical node by using the <span class="emphasis"><em>decommission</em></span> command.
Suppose you no longer needed agent1 and wanted to "turn it off".  You can do
so by entering the following command:</p><pre class="screen">decommission agent1</pre><p>This terminates the thread and removes the configuration associated with a
logical node, and the mapping between the logical node and physical node.</p><p>You can also move a logical node from one physical node to another by first
unmapping a logical node and then mapping it on another physical node.  In
this scenario, you change the collector1 from being on host2 to host3.</p><pre class="screen">unmap host2 collector1</pre><p>At this point, the logical node mapping is removed, and collector1 is not
active anywhere. You can then map collector1 onto host3 by using the map
command:</p><pre class="screen">map host3 collector1</pre><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>There are some limitations that need to be further described in this
section.</p></td></tr></table></div></div><div class="section" title="4.3.2. Logical Sources and Logical Sinks"><div class="titlepage"><div><div><h4 class="title"><a name="_logical_sources_and_logical_sinks"></a>4.3.2. Logical Sources and Logical Sinks</h4></div></div></div><p>In the previous example, we used two abstractions under-the-covers that allow
the specifications of a graph topology for communications <span class="emphasis"><em>without having to
use physical hostnames and ports</em></span>. These abstractions&#8201;&#8212;&#8201;the <span class="strong"><strong>logical source</strong></span>
and <span class="strong"><strong>logical sink</strong></span>&#8201;&#8212;&#8201;allow you to create a different graph topology without
having to know physical machines until they are mapped.</p><p>Suppose you have two nodes producing data and sending it to the consumer:</p><pre class="screen">dataProducer1 : console | logicalSink("dataConsumer") ;
dataProducer2 : console | logicalSink("dataConsumer") ;
dataConsumer : logicalSource | console ;</pre><p>Note that in this example, the destination argument is the <span class="strong"><strong>logical name</strong></span> of
the node and not a specific host/port combination.</p><p>To implement these features, there is a generalized mechanism where users
enter logical configurations that are <span class="emphasis"><em>translated</em></span> by the Master to a physical
configuration.</p><p>When the logical nodes get mapped to physical nodes:</p><pre class="screen">map host1 dataProducer1
map host2 dataProducer2
map host3 dataConsumer</pre><p>and after the Master learns the host names (the host1, host2, and host3
machine&#8217;s heartbeat against the Master), the Master has enough information to
translate configurations with physical hostnames and ports.  A possible
translation would replace the logicalSource with a rpcSources and the
logicalSink with an rpcSinks:</p><pre class="screen">dataProducer1 : console | rpcSink("host3",56789) ;
dataProducer2 : console | rpcSink("host3",56789) ;
dataConsumer : rpcSource(56789) | console ;</pre><p>In fact, auto agents and collectors, are another example of <span class="strong"><strong>translated
sources and sinks</strong></span>.  These translate auto*Chain sinks and collectorSource into
a configuration that uses logicalSinks and logicalSources which in turn are
translated into physical rpcSource and rpcSinks instances.</p><div class="tip" title="Tip" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Tip"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Tip]" src="images/tip.png"></td><th align="left">Tip</th></tr><tr><td align="left" valign="top"><p>Translations are powerful and can be fairly smart; if new collectors are
added, they will become new failover options.  If collectors are removed, then
the removed collectors will be automatically replaced by other failover nodes.</p></td></tr></table></div></div><div class="section" title="4.3.3. Flow Isolation"><div class="titlepage"><div><div><h4 class="title"><a name="_flow_isolation"></a>4.3.3. Flow Isolation</h4></div></div></div><p>What happens if you want collect different kinds of data from the same
physical node?  For example, suppose you wanted to collect httpd logs as well
as syslog logs from the same physical machine.  Suppose also you want to write
all of the syslog data from the cluster in one directory tree, and all of the
httpd logs from the cluster in another.</p><p>One approach is to tag all the data with source information and then push all
the data down the same pipe.  This could then be followed by some post-
processing that demultiplexes (demuxes) the data into different buckets.
Another approach is to keep the two sets of data logically isolated from each
other the entire time and avoid post processing.</p><p>Flume can do both approaches but enables the latter lower-latency
approach, by introducing the concept of grouping nodes into
<span class="strong"><strong>flows</strong></span>. Since there are logical nodes that allow for multiple nodes
on a single JVM, you can have a node for each different kinds of
produced data.</p><p>The following example shows how flows can be used. Start by having six logical
nodes in the system.</p><pre class="screen">fooNode1 : fooSrc | autoBEChain ;
barNode1 : barSrc | autoBEChain ;
forNode2 : fooSrc | autoBEChain ;
barNode2 : barSrc | autoBEChain ;
fooConsumer : autoCollectorSource | collectorSink("hdfs://nn/foodir") ;
barConsumer : autoCollectorSource | collectorSink("hdfs://nn/bardir") ;</pre><p>In this scenario, there are two physical machines that produce both
kinds of data&#8201;&#8212;&#8201;foo data and bar data. You want to send data to
single collector that collects both foo data and bar data and writes
it to different HDFS directories.  You could then map the nodes onto
physical nodes:</p><pre class="screen">map host1 fooNode1
map host1 barNode1
map host2 fooNode2
map host2 barNode2
map host3 fooConsumer
map host3 barConsumer</pre><div class="figure"><a name="id415609"></a><p class="title"><b>Figure 1. Flume Flows: Single Flow</b></p><div class="figure-contents"><div class="mediaobject"><img src="single-flow.png" alt="single-flow.png"></div></div></div><br class="figure-break"><p>This setup essentially instantiates the first approach.  It mixes foo and bar
data together since the translation of autoBEChain would see two
collectorSources that the Master considers to be equivalent.  Foo data will
likely be sent to the barConsumer and bar data will likely be sent to
fooConsumer.</p><p>You really wanted to separate sources of information into logically isolated
streams of data.  Flume provides a grouping abstraction called a <span class="strong"><strong>flow</strong></span>.  A
flow groups particular logical nodes together so that the different logical
data types remain isolated.</p><p>More concretely, it allows for a different failover chain for each kind of
data in the Flume cluster.  The auto*Chain based agents would only send data
to collectors in the same flow group.  This isolates data so that it only
flows to nodes within the group.</p><p>Currently, the compact form of the configuration language does not
allow you to specify flows.  Instead you must add an extra argument to
the config command to specify a flow.</p><p>This example shows commands that would be entered in the Flume shell
without flow group information.  In this case all of the nodes are in
the same flow.</p><pre class="screen">exec config fooNode1 fooSrc autoBEChain
exec config barNode1 barSrc autoBEChain
exec config fooNode2 fooSrc autoBEChain
exec config barNode2 barSrc autoBEChain
exec config fooConsumer autoCollectorSource 'collectorSink("hdfs://nn/foodir")'
exec config barConsumer autoCollectorSource 'collectorSink("hdfs://nn/bardir")'</pre><p>Now using the following commands you can specify flows by adding an
extra parameter after the node name.  In this example we have two
flows: flowfoo and flowbar.  flowfoo contains fooNode1, fooNode2 and
fooConsumer.  flowbar contains barNode1, barNode2 and barConsumer.</p><pre class="screen">exec config fooNode1 flowfoo fooSrc autoBEChain
exec config barNode1 flowbar barSrc autoBEChain
exec config fooNode2 flowfoo fooSrc autoBEChain
exec config barNode2 flowbar barSrc autoBEChain
exec config fooConsumer flowfoo autoCollectorSource 'collectorSink("hdfs://nn/foodir")'
exec config barConsumer flowbar autoCollectorSource 'collectorSink("hdfs://nn/bardir")'</pre><div class="figure"><a name="id415698"></a><p class="title"><b>Figure 2. Flume Flows: Multiple Flows</b></p><div class="figure-contents"><div class="mediaobject"><img src="multi-flow.png" alt="multi-flow.png"></div></div></div><br class="figure-break"><p>By using these commands, the data from fooNode1 and fooNode2 will only
be sent to fooConsumer, and barNode1 and barNode2&#8217;s data will only be
sent to barConsumer.  Data from one node is not mixed with other data
from other nodes unless explicitly connected.</p><div class="tip" title="Tip" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Tip"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Tip]" src="images/tip.png"></td><th align="left">Tip</th></tr><tr><td align="left" valign="top"><p>In practice it is a good idea to use different node names and
different flow ids for different kinds of data.  When node names are
reused, the default behavior is to attempt to recover from failures
assuming that leftover data from a crashed execution or previous
source/sink configuration version are still producing the same kind of
data.</p></td></tr></table></div></div><div class="section" title="4.3.4. Section Summary"><div class="titlepage"><div><div><h4 class="title"><a name="_section_summary_4"></a>4.3.4. Section Summary</h4></div></div></div><p>This section introduced logical nodes, logical sources, logical sinks, and
flows and showed how these abstractions enable you to automatically deal with
manageability problems.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Only one input source per physical node.
</li><li class="listitem">
Multiple sets of isolated flows.
</li><li class="listitem">
Being machine specific, having to know all physical host names and ports.
</li></ul></div><p>The translation mechanism can be quite powerful.  When coupled with
metrics information, this could be used to perform automated dynamic
configuration changes.  A possible example would be to automatically
commission or decommission new collectors to match diurnal traffic and
load patterns.</p></div></div><div class="section" title="4.4. Multiple Masters"><div class="titlepage"><div><div><h3 class="title"><a name="_multiple_masters"></a>4.4. Multiple Masters</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_standalone_mode_compared_to_distributed_mode">4.4.1. Standalone Mode Compared to Distributed Mode</a></span></dt><dt><span class="section"><a href="#_running_in_standalone_mode">4.4.2. Running in Standalone Mode</a></span></dt><dt><span class="section"><a href="#_running_in_distributed_mode">4.4.3. Running in Distributed Mode</a></span></dt><dt><span class="section"><a href="#_configuration_stores">4.4.4. Configuration Stores</a></span></dt><dt><span class="section"><a href="#_which_configuration_store_should_i_use">4.4.5. Which Configuration Store Should I Use?</a></span></dt><dt><span class="section"><a href="#_configuring_the_zbcs">4.4.6. Configuring the ZBCS</a></span></dt><dt><span class="section"><a href="#_gossip_in_distributed_mode">4.4.7. Gossip in Distributed Mode</a></span></dt><dt><span class="section"><a href="#_diagrams_how_the_masters_and_nodes_talk_to_each_other">4.4.8. Diagrams: How the Masters and Nodes talk to each other</a></span></dt><dt><span class="section"><a href="#_configuring_flume_nodes_to_connect_to_multiple_master_servers">4.4.9. Configuring Flume Nodes to Connect to Multiple Master Servers</a></span></dt></dl></div><p>The Master has two main jobs to perform. The first is to keep track of all the nodes in a Flume deployment and to keep them informed of any changes to their configuration. The second is to track acknowledgements from the end of a Flume flow that is operating in <span class="strong"><strong>reliable mode</strong></span> so that the source at the top of that flow knows when to stop transmitting an event.</p><p>Both these jobs are critical to the operation of a Flume deployment. Therefore, it is ill-advised to have the Master live on a single machine, as this represents a single point of failure for the whole Flume service (see <span class="emphasis"><em>failure modes</em></span> for more detail).</p><p>Flume therefore supports the notion of multiple Masters which run on physically separate nodes and co-ordinate amongst themselves to stay synchronized. If a single Master should fail, the other Masters can take over its duties and keep all live flows functioning. This all happens transparently with a little effort at configuration time. Nodes will automatically fail over to a working Master when they lose contact with their current Master.</p><div class="section" title="4.4.1. Standalone Mode Compared to Distributed Mode"><div class="titlepage"><div><div><h4 class="title"><a name="_standalone_mode_compared_to_distributed_mode"></a>4.4.1. Standalone Mode Compared to Distributed Mode</h4></div></div></div><p>The Flume Master can be run in one of two ways.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<span class="strong"><strong>Standalone</strong></span> mode - this is where the Master runs on a single machine. This is easy to administer, and simple to set-up, but has disadvantages when it comes to scalability and fault-tolerance.
</li><li class="listitem">
<span class="strong"><strong>Distributed</strong></span> mode - this is where the Master is configured to run on several machines - usually three or five. This option scales to serve many Flows, and also has good fault-tolerance properties.
</li></ul></div><p>Large production deployments of Flume should run a distributed Master so that inevitable machine failures do not impact the availability of Flume itself. For small deployments the issue is less clear-cut - a distributed Master means reserving more computing resources that could be used instead for nodes or other services, and it is possible to recover from many failure modes in a timely manner with human intervention. The choice between distributed and standalone Masters is ultimately dependent both on your use case and your operating requirements.</p></div><div class="section" title="4.4.2. Running in Standalone Mode"><div class="titlepage"><div><div><h4 class="title"><a name="_running_in_standalone_mode"></a>4.4.2. Running in Standalone Mode</h4></div></div></div><div class="sidebar"><p class="title"><b></b></p><p>Standalone mode is where Flume has only one Master node. The configuration described below needs to be done on that machine only.</p></div><p>Whether the Flume Master starts in distributed or standalone mode is indirectly controlled by how many machines are configured to run as Master servers. To run in standalone mode, a single configuration property <code class="literal">flume.master.servers</code> must be set:</p><pre class="screen">&lt;property&gt;
&lt;name&gt;flume.master.servers&lt;/name&gt;
&lt;value&gt;hostA&lt;/value&gt;
&lt;/property&gt;</pre><p>The value of <code class="literal">flume.master.servers</code> is a comma-separated list of all the machine names (or IP addresses) that will be Master servers. If this list contains only machine name, the Flume Master will start in standalone mode. If there is more than one machine name in the list, the Flume Master will start in distributed mode.</p><p>There&#8217;s no other configuration required for standalone mode. Flume will use reasonable default values for any other master-related variables. To start the Master, from the command prompt type:</p><pre class="screen">$ flume master</pre><p>from <code class="literal">$FLUME_HOME</code>. A number of log messages should print to the screen. After the server is running, you can check that everything is working properly by visiting the web interface at <code class="literal">http://master-node-ip:35871/</code>,
where <code class="literal">master-node-ip</code> is the IP address (or hostname) of the Master node. If you see a web page, the Master is running.</p></div><div class="section" title="4.4.3. Running in Distributed Mode"><div class="titlepage"><div><div><h4 class="title"><a name="_running_in_distributed_mode"></a>4.4.3. Running in Distributed Mode</h4></div></div></div><div class="sidebar"><p class="title"><b></b></p><p>Distributed mode runs the Flume Master on several machines. Therefore the configuration described below should be done on <span class="strong"><strong>every</strong></span> Master machine, except where noted. <span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span>**</p><p>Running the Flume Master in distributed mode provides better fault tolerance than in standalone mode, and scalability for hundreds of nodes.</p><p>Configuring machines to run as part of a distributed Flume Master is nearly as simple as standalone mode. As before, <code class="literal">flume.master.servers</code> needs to be set, this time to a list of machines:</p><pre class="screen">&lt;property&gt;
&lt;name&gt;flume.master.servers&lt;/name&gt;
&lt;value&gt;masterA,masterB,masterC&lt;/value&gt;
&lt;/property&gt;</pre></div><p title="How many machines do I need?"><b>How many machines do I need? </b>The distributed Flume Master will continue to work correctly as long as more than half the physical machines running it are still working and haven&#8217;t crashed. Therefore if you want to survive one fault, you need three machines (because 3-1 = 2 &gt; 3/2). For every extra fault you want to tolerate, add another two machines, so for two faults you need five machines. Note that having an even number of machines doesn&#8217;t make the Flume Master any more fault-tolerant - four machines only tolerate one failure, because if two were to fail only two would be left functioning, which is not more than half of four. Common deployments should be well served by three or five machines.</p><div class="sidebar"><p class="title"><b></b></p><p>The final property to set is <span class="strong"><strong>not</strong></span> the same on every machine - every node in the Flume Master must have a unique value for <code class="literal">flume.master.serverid</code>.</p></div><p title="Note"><b>Note. </b><code class="literal">flume.master.serverid</code> is the only Flume Master property that <span class="emphasis"><em>must</em></span> be different on every machine in the ensemble. <span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong>*</strong></span></p><p title="masterA"><b>masterA. </b>
</p><pre class="programlisting">&lt;property&gt;
&lt;name&gt;flume.master.serverid&lt;/name&gt;
&lt;value&gt;0&lt;/value&gt;
&lt;/property&gt;</pre><p title="masterA">
</p><p title="masterB"><b>masterB. </b>
</p><pre class="programlisting">&lt;property&gt;
&lt;name&gt;flume.master.serverid&lt;/name&gt;
&lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;</pre><p title="masterB">
</p><p title="masterC"><b>masterC. </b>
</p><pre class="programlisting">&lt;property&gt;
&lt;name&gt;flume.master.serverid&lt;/name&gt;
&lt;value&gt;2&lt;/value&gt;
&lt;/property&gt;</pre><p title="masterC">
</p><p>The value for <code class="literal">flume.master.serverid</code> for each node is the index of
that node&#8217;s hostname in the list in <code class="literal">flume.master.ensemble</code>, starting
at 0. For example <code class="literal">masterB</code> has index 1 in that list. The purpose of
this property is to allow each node to uniquely identify itself to the
other nodes in the Flume Master.</p><p>This is all the configuration required to start a three-node
distributed Flume Master. To test this out, we can start the Master
process on all three machines:</p><pre class="screen">[flume@masterA] flume master

[flume@masterB] flume master

[flume@masterC] flume master</pre><p>Each Master process will initially try and contact all other nodes in
the ensemble. Until more than half (in this case, two) nodes are alive
and contactable, the configuration store will be unable to start, and
the Flume Master will not be able to read or write configuration data.</p><p>You can check the current state of the ensemble by inspecting the web page for any of the Flume Master machines which by default will be found at, for example, <code class="literal">http://masterA:35871</code>.</p></div><div class="section" title="4.4.4. Configuration Stores"><div class="titlepage"><div><div><h4 class="title"><a name="_configuration_stores"></a>4.4.4. Configuration Stores</h4></div></div></div><p>The Flume Master stores all its data in a <span class="strong"><strong>configuration store</strong></span>. Flume has a pluggable configuration store architecture, and supports two implementations.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
The Memory-Backed Config Store (MBCS) stores configurations temporarily in memory. If the master node fails and reboots, all the configuration data will be lost. The MBCS is incompatible with distributed masters.  However, it is very easy to administer, computationally lightweight, and good for testing and experimentation.
</li><li class="listitem">
The ZooKeeper-Backed Config Store (ZBCS) stores configurations   persistently and takes care of synchronizing them between multiple masters.
</li></ul></div><p title="Flume and Apache ZooKeeper"><b>Flume and Apache ZooKeeper <span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span>. </b>Flume relies on the Apache ZooKeeper coordination platform to provide reliable, consistent, and persistent storage for node configuration data. A ZooKeeper ensemble is made up of two or more nodes which communicate regularly with each other to make sure each is up to date. Flume embeds a ZooKeeper server inside the Master process, so starting and maintaining the service is taken care of. However, if you have an existing ZooKeeper service running, Flume supports using that external cluster as well. <span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span><span class="strong"><strong><span class="strong"><strong></strong></span></strong></span></p></div><div class="section" title="4.4.5. Which Configuration Store Should I Use?"><div class="titlepage"><div><div><h4 class="title"><a name="_which_configuration_store_should_i_use"></a>4.4.5. Which Configuration Store Should I Use?</h4></div></div></div><p>In almost all cases, you should use the ZBCS. It is more reliable and fault-tolerant, and will recover configurations after a restart. It is compatible with both standalone and distributed deployments of the Flume Master.</p><p>The MBCS is appropriate if you are experimenting with Flume and can stand to lose configuration if the machine fails.</p><p>ZBCS is the default configuration store. The choice of which configuration store to use is controlled by the <code class="literal">flume.master.store</code> system property.</p><pre class="programlisting">&lt;property&gt;
&lt;name&gt;flume.master.store&lt;/name&gt;
&lt;value&gt;zookeeper&lt;/value&gt;
&lt;/property&gt;</pre><p>If set to <code class="literal">memory</code>, the Flume Master will use MBCS instead. This is only supported in standalone mode.</p></div><div class="section" title="4.4.6. Configuring the ZBCS"><div class="titlepage"><div><div><h4 class="title"><a name="_configuring_the_zbcs"></a>4.4.6. Configuring the ZBCS</h4></div></div></div><p>Most deployments using the ZBCS can use Flume&#8217;s default configuration. However, where more control over the precise configuration of the Flume Master is needed, there are several properties that you can set.</p><div class="variablelist"><dl><dt><span class="term">
Log Directory - <code class="literal">flume.master.zk.logdir</code> 
</span></dt><dd>
To ensure reliability and the ability to restore its state in the event of a failure, ZBCS continually logs all updates it sees to the directory in <code class="literal">flume.master.zk.logdir</code>. This directory must be writable by the user as which Flume is running, and will be created if it doesn&#8217;t exist at start-up time. WARNING: Do not delete this directory, or any files inside it. If deleted, all your configuration information will be lost.
</dd><dt><span class="term">
ZBCS Server Ports 
</span></dt><dd>
Each machine in the distributed Flume Master communicates with every other on the TCP ports set by <code class="literal">flume.master.zk.server.quorum.port</code> and <code class="literal">flume.master.zk.server.election.port</code>. The defaults are 3182 and 3183 respectively. Note that these settings control both the port on which the ZBCS listens, and on which it looks for other machines in the ensemble.
</dd><dt><span class="term">
ZBCS Client Port - <code class="literal">flume.master.zk.client.port</code> 
</span></dt><dd>
The Flume Master process communicates with ZooKeeper (either on the same machine, or remotely on another Master server) via a client TCP port, which is set by <code class="literal">flume.master.zk.client.port</code>. The default is 3181.
</dd></dl></div></div><div class="section" title="4.4.7. Gossip in Distributed Mode"><div class="titlepage"><div><div><h4 class="title"><a name="_gossip_in_distributed_mode"></a>4.4.7. Gossip in Distributed Mode</h4></div></div></div><p>Flume Master servers also use a <span class="emphasis"><em>gossip</em></span> protocol to exchange information between themselves. Each server periodically wakes and picks another machine to send new data to. This protocol by default uses TCP port 57890, but this is controlled via the <code class="literal">flume.master.gossipport</code> property:</p><pre class="screen">&lt;property&gt;
&lt;name&gt;flume.master.gossip.port&lt;/name&gt;
&lt;value&gt;57890&lt;/value&gt;
&lt;/property&gt;</pre><p>In standalone mode, there is no need to use gossip, so this port is unused.</p></div><div class="section" title="4.4.8. Diagrams: How the Masters and Nodes talk to each other"><div class="titlepage"><div><div><h4 class="title"><a name="_diagrams_how_the_masters_and_nodes_talk_to_each_other"></a>4.4.8. Diagrams: How the Masters and Nodes talk to each other</h4></div></div></div><div class="figure"><a name="id416532"></a><p class="title"><b>Figure 3. Flume Master: Standalone Mode</b></p><div class="figure-contents"><div class="mediaobject"><img src="master-zk-standalone.png" alt="master-zk-standalone.png"></div></div></div><br class="figure-break"><div class="figure"><a name="id416555"></a><p class="title"><b>Figure 4. Flume Master: Distributed Mode</b></p><div class="figure-contents"><div class="mediaobject"><img src="master-zk-internal.png" alt="master-zk-internal.png"></div></div></div><br class="figure-break"></div><div class="section" title="4.4.9. Configuring Flume Nodes to Connect to Multiple Master Servers"><div class="titlepage"><div><div><h4 class="title"><a name="_configuring_flume_nodes_to_connect_to_multiple_master_servers"></a>4.4.9. Configuring Flume Nodes to Connect to Multiple Master Servers</h4></div></div></div><p>One property needs to be set to configure a Flume Node to connect to multiple Masters: <code class="literal">flume.master.servers</code>.</p><pre class="programlisting">&lt;property&gt;
&lt;name&gt;flume.master.servers&lt;/name&gt;
&lt;value&gt;masterA,masterB,masterC&lt;/value&gt;
&lt;/property&gt;</pre><p>The nodes connect over the port <code class="literal">flume.master.heartbeat.port</code> on each machine in the Flume Master - this is the port that the Master servers listen on for node heartbeats.</p><p>If a Master server fails, nodes will automatically fail over to the next randomly selected Master server that they can establish a connection to.</p></div></div><div class="section" title="4.5. External ZooKeeper Cluster"><div class="titlepage"><div><div><h3 class="title"><a name="_external_zookeeper_cluster"></a>4.5. External ZooKeeper Cluster</h3></div></div></div><p>In some cases you may want a ZBCS that relies on an externally managed ZooKeeper service. The most common example of this is where multiple services which rely on ZooKeeper are being used (Flume and Hbase for example). In the following example zkServer{A,B,C}:2181 should be replaced with the hostname/port of the ZooKeeper servers which make up your ensemble.</p><p title="conf/flume-site.xml"><b><code class="literal">conf/flume-site.xml</code>. </b>
</p><pre class="programlisting">&lt;property&gt;
  &lt;name&gt;flume.master.zk.use.external&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;flume.master.zk.servers&lt;/name&gt;
  &lt;value&gt;zkServerA:2181,zkServerB:2181,zkServerC:2181&lt;/value&gt;
&lt;/property&gt;</pre><p title="conf/flume-site.xml">
</p></div><div class="section" title="4.6. Section Summary"><div class="titlepage"><div><div><h3 class="title"><a name="_section_summary_5"></a>4.6. Section Summary</h3></div></div></div><p>This section described installing, deploying, and configuring a set of
Flume nodes in a fully distributed setting. You should now be able to
collect streams of logs with Flume.</p><p>You also used some roles for sources and sinks to connect nodes
together.  You now have an understanding of the basics of setting up a
set of Flume nodes.  Here&#8217;s the new sources and sinks introduced in
this subsection.</p><div class="variablelist" title="Flume&#8217;s Tiered Event Sources"><p class="title"><b>Flume&#8217;s Tiered Event Sources</b></p><dl><dt><span class="term">
<code class="literal">collectorSource[(<span class="emphasis"><em>port</em></span>)]</code> 
</span></dt><dd>
Collector source. Listens for data from
agentSinks forwarding to port <code class="literal"><span class="emphasis"><em>port</em></span></code>.  If port is not specified, the
node default collector TCP port, 35853.
</dd></dl></div></div></div><div class="section" title="5. Integrating Flume with your Data Sources"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_integrating_flume_with_your_data_sources"></a>5. Integrating Flume with your Data Sources</h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_push_sources">5.1. Push Sources</a></span></dt><dt><span class="section"><a href="#_polling_sources">5.2. Polling Sources</a></span></dt><dt><span class="section"><a href="#_embedding_sources">5.3. Embedding Sources</a></span></dt><dt><span class="section"><a href="#_logging_via_log4j_directly">5.4. Logging via log4j Directly</a></span></dt><dd><dl><dt><span class="section"><a href="#_example_of_logging_hadoop_jobs">5.4.1. Example of Logging Hadoop Jobs</a></span></dt><dt><span class="section"><a href="#_logging_hadoop_daemons">5.4.2. Logging Hadoop Daemons</a></span></dt></dl></dd></dl></div><div class="sidebar" title="WARNING"><p class="title"><b>WARNING</b></p><p>This section is incomplete.</p></div><p>Flume&#8217;s source interface is designed to be simple yet powerful and enable logging
of all kinds of data&#8201;&#8212;&#8201;from unstructured blobs of byte, semi-structured blobs
with structured metadata, to completely structured data.</p><p>In this section we describe some of the basic mechanisms that can be used to
pull in data.  Generally, this approach has three flavors. <span class="strong"><strong>Pushing</strong></span> data to
Flume, having Flume <span class="strong"><strong>polling</strong></span> for data, or <span class="strong"><strong>embedding</strong></span> Flume or Flume
components into an application.</p><p>These mechanisms have different trade-offs&#8201;&#8212;&#8201;based on the semantics of the
operation.</p><p>Also, some sources can be <span class="strong"><strong>one shot</strong></span> or <span class="strong"><strong>continuous</strong></span> sources.</p><div class="section" title="5.1. Push Sources"><div class="titlepage"><div><div><h3 class="title"><a name="_push_sources"></a>5.1. Push Sources</h3></div></div></div><div class="variablelist"><dl><dt><span class="term">
<code class="literal">syslogTcp</code>, <code class="literal">syslogUdp</code> 
</span></dt><dd>
wire-compatibility with syslog, and syslog-ng
logging protocols.
</dd><dt><span class="term">
<code class="literal">scribe</code> 
</span></dt><dd>
wire-compatibility with the scribe log collection system.
</dd></dl></div></div><div class="section" title="5.2. Polling Sources"><div class="titlepage"><div><div><h3 class="title"><a name="_polling_sources"></a>5.2. Polling Sources</h3></div></div></div><div class="variablelist"><dl><dt><span class="term">
<code class="literal">tail</code>, <code class="literal">multitail</code> 
</span></dt><dd>
watches a file(s) for appends.
</dd><dt><span class="term">
<code class="literal">exec</code> 
</span></dt><dd>
This is good for extracting custom data by using existing programs.
</dd><dt><span class="term">
<code class="literal">poller</code> 
</span></dt><dd>
We can gather information from Flume nodes themselves.
</dd></dl></div></div><div class="section" title="5.3. Embedding Sources"><div class="titlepage"><div><div><h3 class="title"><a name="_embedding_sources"></a>5.3. Embedding Sources</h3></div></div></div><div class="warning" title="Warning" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Warning"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Warning]" src="images/warning.png"></td><th align="left">Warning</th></tr><tr><td align="left" valign="top"><p>these features are incomplete.</p></td></tr></table></div><p><code class="literal">log4j</code></p><p><code class="literal">simple client library</code></p><pre class="literallayout">// move this to gathering data from sources</pre></div><div class="section" title="5.4. Logging via log4j Directly"><div class="titlepage"><div><div><h3 class="title"><a name="_logging_via_log4j_directly"></a>5.4. Logging via log4j Directly</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_example_of_logging_hadoop_jobs">5.4.1. Example of Logging Hadoop Jobs</a></span></dt><dt><span class="section"><a href="#_logging_hadoop_daemons">5.4.2. Logging Hadoop Daemons</a></span></dt></dl></div><div class="section" title="5.4.1. Example of Logging Hadoop Jobs"><div class="titlepage"><div><div><h4 class="title"><a name="_example_of_logging_hadoop_jobs"></a>5.4.1. Example of Logging Hadoop Jobs</h4></div></div></div></div><div class="section" title="5.4.2. Logging Hadoop Daemons"><div class="titlepage"><div><div><h4 class="title"><a name="_logging_hadoop_daemons"></a>5.4.2. Logging Hadoop Daemons</h4></div></div></div></div></div></div><div class="section" title="6. Using Data Collected by Flume"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_using_data_collected_by_flume"></a>6. Using Data Collected by Flume</h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_the_data_model_of_a_flume_event">6.1. The Data Model of a Flume Event</a></span></dt><dt><span class="section"><a href="#_output_bucketing">6.2. Output Bucketing</a></span></dt><dt><span class="section"><a href="#_output_format">6.3. Output Format</a></span></dt><dt><span class="section"><a href="#_small_files_compared_to_high_latency">6.4. Small Files Compared to High Latency</a></span></dt></dl></div><p>The first goal of Flume is to collect data and reliably write it to HDFS.
Once data arrives, one wants the ability to control where and in what format
data is stored.  Flume provides basic output control mechanisms via the
properties configuration and in the dataflow language.  This gives the user
the ability to control the output format and output bucketing of incoming
data, and simplifies integration with other HDFS data consumers such as Hive
and HBase.</p><p>Here are some example use cases:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
When monitoring a web server, you want to bucket logs based on time,
  the page hit, and the browser being used.
</li><li class="listitem">
When tracking particular data nodes, you want to bucket logs based on
  time and the data node name.
</li><li class="listitem">
When tracking a feed of JIRA tickets from the Apache feed, you want
  to group based on the project identifier or a particular person.
</li><li class="listitem">
When collecting data from scribe sources, you want to use its bucket
  data based on its the event&#8217;s category information.
</li></ul></div><p>To support these kinds of features, Flume uses a simple data model,
provides a mechanism for bucketing events, and also provides basic
extraction operations for specifying custom bucketing discriminators.</p><div class="section" title="6.1. The Data Model of a Flume Event"><div class="titlepage"><div><div><h3 class="title"><a name="_the_data_model_of_a_flume_event"></a>6.1. The Data Model of a Flume Event</h3></div></div></div><p>A Flume event has these six main fields:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Unix timestamp
</li><li class="listitem">
Nanosecond timestamp
</li><li class="listitem">
Priority
</li><li class="listitem">
Source host
</li><li class="listitem">
Body
</li><li class="listitem">
Metadata table with an arbitrary number of attribute value pairs.
</li></ul></div><p>All events are guaranteed to have all of these elements.  However, the
body may have zero length, and the metadata table can be empty.</p><p>The Unix timestamp is measured in milliseconds and is Unix time stamp from the
source machine.  The nanosecond timestamp is machine specific nanosecond
counter also from the source machine.  It is safe to assume that the nanotime
from a machine is monotonically increasing&#8201;&#8212;&#8201;i.e. if event A has a larger
nanotime than event B from the same machine, event A was initially received
before event B.</p><p>Currently the priority of a message can have one of 6 values: TRACE, DEBUG,
INFO, WARN, ERROR, or FATAL.  These values are often provided by logging
systems such as syslog or log4j.</p><p>The source host is the name of the machine or the IP (whatever hostname call
returns).</p><p>The body is the raw log entry body.  The default is to truncate the body to a
maximum of 32KB per event.  This is a configurable value and can be changed by
modifying the <span class="emphasis"><em>flume.event.max.size.bytes</em></span> property.</p><p>Finally there is the metadata table which is a map from a string attribute
name to an arbitrary array of bytes.  This allows for custom bucketing
attributes and will be described in more depth in the Advanced Usage section
of this guide.</p></div><div class="section" title="6.2. Output Bucketing"><div class="titlepage"><div><div><h3 class="title"><a name="_output_bucketing"></a>6.2. Output Bucketing</h3></div></div></div><p>You can control the output of events to particular directories or files based
on the values of an event&#8217;s fields.  To enable this, you provide an escaping
mechanism that outputs data to a particular path.</p><p>For example, here is an output spec:</p><pre class="screen">collectorSink("hdfs://namenode/flume/webdata/%H00/", "%{host}-")</pre><p>The first argument is the directory where data is to be written.  The second
is a filename prefix where events are written.  Suppose you get an event from a
machine called server1 generated at time 18:58.  The events would get written
to HDFS with namenode namenode, in a directory called /flume/webdata/1800/,
with files named server1-xxx where xxx is some extra data for unique file
names.</p><p>What happened here?  Flume replaced the <span class="emphasis"><em>%H</em></span> with a string that represent the
hour of the timestamp found in the event&#8217;s data. Likewise, the <span class="emphasis"><em>%o</em></span> was
replace with the hostname field from the event.</p><p>What happens if the server1&#8217;s message had been delayed and the message wasn&#8217;t
sent downstream until 19:05?  Since the value of the timestamp on the event
was during the 18:00 hour, the event would be written into that directory.</p><div class="variablelist" title="Event data escape sequences"><p class="title"><b>Event data escape sequences</b></p><dl><dt><span class="term">
[horizontal] %{host} 
</span></dt><dd>
host
</dd><dt><span class="term">
%{nanos} 
</span></dt><dd>
nanos
</dd><dt><span class="term">
%{priority} 
</span></dt><dd>
priority string
</dd><dt><span class="term">
%{body} 
</span></dt><dd>
body
</dd><dt><span class="term">
%% 
</span></dt><dd>
a <span class="emphasis"><em>%</em></span> character.
</dd><dt><span class="term">
%t 
</span></dt><dd>
Unix time in millis
</dd></dl></div><p>Because bucketing by date is a frequently-requested feature, there are escape
sequences for finer control of date values that allow you to bucket data based
on date.</p><p>Here is another output spec:</p><pre class="screen">collectorSink("hdfs://namenode/flume/webdata/%Y-%m-%d/%H00/", "web-")</pre><p>This would create directories for each day, each with a subdirectory for each
hour with filenames prefixed "web-".</p><div class="horizontal"><a name="id417284"></a><p class="title"><b>Table 2. Fine grained escape sequences date and times</b></p><div class="horizontal-contents"><table summary="Fine grained escape sequences date and times" style="border: none;"><colgroup><col><col></colgroup><tbody valign="top"><tr><td style="" valign="top">
<p>
%a 
</p>
</td><td style="" valign="top">
<p>
locale&#8217;s short weekday name (Mon, Tue, &#8230;)
</p>
</td></tr><tr><td style="" valign="top">
<p>
%A 
</p>
</td><td style="" valign="top">
<p>
locale&#8217;s full weekday name (Monday, Tuesday, &#8230;)
</p>
</td></tr><tr><td style="" valign="top">
<p>
%b 
</p>
</td><td style="" valign="top">
<p>
locale&#8217;s short month name (Jan, Feb,&#8230;)
</p>
</td></tr><tr><td style="" valign="top">
<p>
%B 
</p>
</td><td style="" valign="top">
<p>
locale&#8217;s long month name (January, February,&#8230;)
</p>
</td></tr><tr><td style="" valign="top">
<p>
%c 
</p>
</td><td style="" valign="top">
<p>
locale&#8217;s date and time (Thu Mar  3 23:05:25 2005)
</p>
</td></tr><tr><td style="" valign="top">
<p>
%d 
</p>
</td><td style="" valign="top">
<p>
day of month (01)
</p>
</td></tr><tr><td style="" valign="top">
<p>
%D 
</p>
</td><td style="" valign="top">
<p>
date; same as %m/%d/%y
</p>
</td></tr><tr><td style="" valign="top">
<p>
%H 
</p>
</td><td style="" valign="top">
<p>
hour (00..23)
</p>
</td></tr><tr><td style="" valign="top">
<p>
%I 
</p>
</td><td style="" valign="top">
<p>
hour (01..12)
</p>
</td></tr><tr><td style="" valign="top">
<p>
%j 
</p>
</td><td style="" valign="top">
<p>
day of year (001..366)
</p>
</td></tr><tr><td style="" valign="top">
<p>
%k 
</p>
</td><td style="" valign="top">
<p>
hour ( 0..23)
</p>
</td></tr><tr><td style="" valign="top">
<p>
%l 
</p>
</td><td style="" valign="top">
<p>
hour ( 1..12)
</p>
</td></tr><tr><td style="" valign="top">
<p>
%m 
</p>
</td><td style="" valign="top">
<p>
month (01..12)
</p>
</td></tr><tr><td style="" valign="top">
<p>
%M 
</p>
</td><td style="" valign="top">
<p>
minute (00..59)
</p>
</td></tr><tr><td style="" valign="top">
<p>
%P 
</p>
</td><td style="" valign="top">
<p>
locale&#8217;s equivalent of am or pm
</p>
</td></tr><tr><td style="" valign="top">
<p>
%s 
</p>
</td><td style="" valign="top">
<p>
seconds since 1970-01-01 00:00:00 UTC
</p>
</td></tr><tr><td style="" valign="top">
<p>
%S 
</p>
</td><td style="" valign="top">
<p>
second (00..60)
</p>
</td></tr><tr><td style="" valign="top">
<p>
%y 
</p>
</td><td style="" valign="top">
<p>
last two digits of year (00..99)
</p>
</td></tr><tr><td style="" valign="top">
<p>
%Y 
</p>
</td><td style="" valign="top">
<p>
year (2010)
</p>
</td></tr><tr><td style="" valign="top">
<p>
%z 
</p>
</td><td style="" valign="top">
<p>
+hhmm numeric timezone (for example, -0400)
</p>
</td></tr></tbody></table></div></div><br class="horizontal-break"></div><div class="section" title="6.3. Output Format"><div class="titlepage"><div><div><h3 class="title"><a name="_output_format"></a>6.3. Output Format</h3></div></div></div><p>Now that you have control of where files go, this section describes how you can
control the output format of data.  Currently, this is set via the
<span class="emphasis"><em>flume.collector.output.format</em></span> property set in the flume-site.xml file.  The output
formats are:</p><div class="horizontal"><a name="id417657"></a><p class="title"><b>Table 3. Output formats</b></p><div class="horizontal-contents"><table summary="Output formats" style="border: none;"><colgroup><col><col></colgroup><tbody valign="top"><tr><td style="" valign="top">
<p>
syslog 
</p>
</td><td style="" valign="top">
<p>
a syslog like text output format
</p>
</td></tr><tr><td style="" valign="top">
<p>
log4j 
</p>
</td><td style="" valign="top">
<p>
a log4j pattern similar to that used by CDH output pattern.
</p>
</td></tr><tr><td style="" valign="top">
<p>
raw 
</p>
</td><td style="" valign="top">
<p>
Event body only.  This is most similar to copying a file but
does not preserve any uniqifying metadata like host/timestamp/nanos.
</p>
</td></tr><tr><td style="" valign="top">
<p>
avro 
</p>
</td><td style="" valign="top">
<p>
Avro Native file format.  Default currently is uncompressed.
</p>
</td></tr><tr><td style="" valign="top">
<p>
avrojson 
</p>
</td><td style="" valign="top">
<p>
JSON encoded date generated by avro
</p>
</td></tr><tr><td style="" valign="top">
<p>
avrodata 
</p>
</td><td style="" valign="top">
<p>
Binary encoded data written in the avro binary format.
</p>
</td></tr><tr><td style="" valign="top">
<p>
default 
</p>
</td><td style="" valign="top">
<p>
a debugging format.
</p>
</td></tr></tbody></table></div></div><br class="horizontal-break"><pre class="screen">&lt;property&gt;
  &lt;name&gt;flume.collector.output.format&lt;/name&gt;
  &lt;value&gt;avrojson&lt;/value&gt;
  &lt;description&gt;This is the output format for the data written to the
  collector.  There are several formats available:
    syslog - outputs events in a syslog-like format
    log4j - outputs events in a pattern similar to Hadoop's log4j pattern
    raw - Event body only.  This is most similar to copying a file but
      does not preserve any uniqifying metadata like host/timestamp/nanos.
    avro - Avro Native file format.  Default currently is uncompressed.
    avrojson - this outputs data as json encoded by avro
    avrodata - this outputs data as an avro binary encoded data
    debug - this is a format for debugging
  &lt;/description&gt;
&lt;/property&gt;</pre></div><div class="section" title="6.4. Small Files Compared to High Latency"><div class="titlepage"><div><div><h3 class="title"><a name="_small_files_compared_to_high_latency"></a>6.4. Small Files Compared to High Latency</h3></div></div></div><p>For all versions Hadoop&#8217;s file system that are earlier than 0.20.x,
HDFS has write-once read-many semantics. Thus, the only way to
reliably flush an HDFS file is to close the file.  Moreover, once a
file is closed, no new data can be appended to the file.  This
presents a tension between getting data written quickly to HDFS and
potentially having many small files (which is a potential scalability
bottleneck of HDFS).</p><p>On one side, to minimize the load and data stored throughput the system,
ideally one would flush data to HDFS as soon as it arrives. Flushing
frequently is in conflict with efficiently storing data to HDFS because this
could result in many small files, which eventually will stress an HDFS
namenode.  A compromise is to pick a reasonable trigger that has a collector
close "reasonably-sized" files (ideally larger than a single HDFS block, 64MB
by default).</p><p>When Flume is deployed at a scale where data collection volumes are small, it
may take a long time to reach the ideal minimum file size (a block size,
typically 64MB).  For example, if a single web server produces 10k of logs a
second (approx. 100 hit logs/s at 100B per log), it will take about 2 hours
(6400 seconds) before an ideal file size can reached.</p><p>In these situations, lean towards having more small files.  Small files
cause a few problems downstream.  These include potential scaling limitations
of Hadoop&#8217;s HDFS, and performance penalties when using MapReduce&#8217;s default
input processing mechanisms within Hadoop.</p><p>The following sections describe two mechanisms to mitigate these
potential problems:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Rolling up many small data files into larger batches
</li><li class="listitem">
Using a CombinedFileInputFormat
</li></ul></div><p>This particular problem becomes less of an issue when the scale of logging
goes up.  If a hundred machines were generating the same amount of logs, you
would reach reasonable files sizes every 64 seconds.</p><p>Future versions of Hadoop will mitigate this problem by providing a flush/sync
operation for currently open HDFS files (patch is already slated for
Hadoop HDFS 0.21.x).</p></div></div><div class="section" title="7. Compression for files written to HDFS."><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_compression_for_files_written_to_hdfs"></a>7. Compression for files written to HDFS.</h2></div></div></div><p>Flume supports basic compression for all log files that are written to
HDFS.  Compressed files are automatically suffixed with an extension
and follow the same naming format + directory structure as regular log
files.</p><p>If GZipCodec is selected, ".gz" is appended to the file name, if
BZip2Codec is selected, ".bz2" is appended.</p><pre class="screen">  &lt;property&gt;
    &lt;name&gt;flume.collector.dfs.compress.codec&lt;/name&gt;
    &lt;value&gt;None&lt;/value&gt;
    &lt;description&gt;Writes formatted data compressed in specified codec to
    dfs. Value is None, GZipCodec, DefaultCodec (deflate), BZip2Codec,
    or any other Codec Hadoop is aware of &lt;/description&gt;
  &lt;/property&gt;</pre></div><div class="section" title="8. Advanced Flume Usage"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_advanced_flume_usage"></a>8. Advanced Flume Usage</h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_the_flume_command_shell">8.1. The Flume Command Shell</a></span></dt><dd><dl><dt><span class="section"><a href="#_using_the_flume_command_shell">8.1.1. Using the Flume Command Shell</a></span></dt></dl></dd><dt><span class="section"><a href="#_flume_8217_s_dataflow_specification_language">8.2. Flume&#8217;s Dataflow Specification Language</a></span></dt><dd><dl><dt><span class="section"><a href="#_special_sinks_fan_out_fail_over_and_roll">8.2.1. Special Sinks: Fan out, Fail over, and Roll</a></span></dt><dt><span class="section"><a href="#_introducing_sink_decorators">8.2.2. Introducing Sink Decorators</a></span></dt><dt><span class="section"><a href="#_translations_of_high_level_sources_and_sinks">8.2.3. Translations of High-level Sources and Sinks</a></span></dt></dl></dd><dt><span class="section"><a href="#_custom_metadata_extraction">8.3. Custom Metadata Extraction</a></span></dt><dd><dl><dt><span class="section"><a href="#_extractors">8.3.1. Extractors</a></span></dt><dt><span class="section"><a href="#_meta_data_filtering_and_transformations">8.3.2. Meta Data Filtering and Transformations</a></span></dt><dt><span class="section"><a href="#_role_defaults">8.3.3. Role Defaults</a></span></dt><dt><span class="section"><a href="#_arbitrary_data_flows_and_custom_architectures">8.3.4. Arbitrary Data Flows and Custom Architectures</a></span></dt></dl></dd><dt><span class="section"><a href="#_extending_via_sink_source_decorator_plugins">8.4. Extending via Sink/Source/Decorator Plugins</a></span></dt><dd><dl><dt><span class="section"><a href="#_semantics_of_flume_extensions">8.4.1. Semantics of Flume Extensions</a></span></dt><dd><dl><dt><span class="section"><a href="#_simple_source_semantics">8.4.1.1. Simple source semantics.</a></span></dt><dt><span class="section"><a href="#_buffered_source_semantics">8.4.1.2. Buffered source semantics</a></span></dt><dt><span class="section"><a href="#_simple_sinks">8.4.1.3. Simple Sinks.</a></span></dt><dt><span class="section"><a href="#_buffered_sink_and_decorator_semantics">8.4.1.4. Buffered sink and decorator semantics</a></span></dt><dt><span class="section"><a href="#_retries_sleeps_and_unclean_exits">8.4.1.5. Retries, sleeps, and unclean exits.</a></span></dt></dl></dd></dl></dd><dt><span class="section"><a href="#_limiting_data_transfer_rate_between_source_sink_pairs">8.5. Limiting Data Transfer Rate between Source-Sink pairs</a></span></dt></dl></div><p>This section describes in further detail, how to automate the
control of Flume nodes via the FlumeShell, a deep dive into Flume&#8217;s dataflow
specification language, internals of the reliability mechanisms, how to do
metadata manipulations, and how to install source and sink plugins.</p><div class="section" title="8.1. The Flume Command Shell"><div class="titlepage"><div><div><h3 class="title"><a name="_the_flume_command_shell"></a>8.1. The Flume Command Shell</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_using_the_flume_command_shell">8.1.1. Using the Flume Command Shell</a></span></dt></dl></div><p>So far, you have been modifying the state of Flume using a simple (but
primitive) web interface to a Master server.</p><p>Flume also provides a shell, which allows the user to type commands into a
terminal and have them executed on a Flume deployment.</p><p>All of the commands available in the web form are available in the
Flume Shell.  The Flume Shell, however, actually has extra controls
for command submission control and state checking that aid
scriptability.</p><div class="section" title="8.1.1. Using the Flume Command Shell"><div class="titlepage"><div><div><h4 class="title"><a name="_using_the_flume_command_shell"></a>8.1.1. Using the Flume Command Shell</h4></div></div></div><p>You can start the FlumeShell by running <code class="literal">flume shell</code> in a terminal window.
The <code class="literal">connect</code> command can be used to establish a connection to any Master
server.</p><pre class="screen">hostname:~/flume$ flume shell
[flume (disconnected)] connect localhost:35873
Connecting to Flume master localhost:35873...
[flume localhost:35873]</pre><pre class="screen">hostname:~/flume$ flume shell -c localhost:35873
Connecting to Flume master localhost:35873...
[flume localhost:35873]</pre><div class="sidebar"><p class="title"><b></b></p><p>The port to use is the value of <code class="literal">flume.config.admin.port</code> and defaults to 35873.</p></div><p>The command line parameters for the Flume Shell are as follows:</p><pre class="screen">usage: FlumeShell [-c &lt;arg&gt;] [-e &lt;arg&gt;] [-q] [-s &lt;arg&gt;]
 -?         Command line usage help
 -c &lt;arg&gt;   Connect to master:port
 -e &lt;arg&gt;   Run a single command
 -q         Run in quiet mode - only print command results
 -s &lt;arg&gt;   Run a FlumeShell script</pre><p>The FlumeShell makes scripting Flume possible - either by using a single
invocation with <code class="literal">-e</code> or by running a script of commands with <code class="literal">-s</code>. It is also
possible to pipe <code class="literal">stdin</code> to the FlumeShell as in the following example:</p><pre class="screen">echo "connect localhost:35873\ngetconfigs\nquit" | flume shell -q</pre><p title="Flume Commands"><b>Flume Commands. </b>You can press Tab any time for some hints on available commands.  If you start
typing a command you can use TAB to complete command.</p><div class="variablelist"><dl><dt><span class="term">
<code class="literal">help</code> 
</span></dt><dd>
List the commands available in the shell.
</dd><dt><span class="term">
<code class="literal">connect <span class="emphasis"><em>master:port</em></span></code> 
</span></dt><dd>
connect to a master at machine <span class="emphasis"><em>master</em></span> on port
<span class="emphasis"><em>port</em></span>.
</dd><dt><span class="term">
<code class="literal">config</code> <span class="emphasis"><em>logicalnode</em></span> <span class="emphasis"><em>source</em></span> <span class="emphasis"><em>sink</em></span> 
</span></dt><dd>
configure a single logical node
<span class="emphasis"><em>logicalnode</em></span> with source <span class="emphasis"><em>source</em></span> and sink <span class="emphasis"><em>sink</em></span>.  <span class="emphasis"><em>source</em></span> and <span class="emphasis"><em>sink</em></span> will
likely need quotes to support some of the Flume configuration syntax.
</dd><dt><span class="term">
<code class="literal">getnodestatus</code> 
</span></dt><dd>
Output the status of the nodes the master knows
about. Nodes are in either HELLO, CONFIGURING, ACTIVE, IDLE, ERROR,
DECOMMISSIONED, or LOST states.  When a node shows up initially it is
HELLO state.  When a node is being configured, it is in CONFIGURING
state.  Once events are being pumped from source to sink, the node is
in ACTIVE state.  If a node has drained its source (and the source is
not "endless") it will enter IDLE state.  If a node encountered an
unrecoverable error or exited without flushing, it will be in ERROR
state.  A node is DECOMMISSIONED if it is removed on the master, and
LOST if it has not been seen by the master for a "long time".
</dd><dt><span class="term">
<code class="literal">getconfigs</code> 
</span></dt><dd>
This gets and dumps the configuration specifications of all
the logical nodes the master knows about.
</dd><dt><span class="term">
<code class="literal">getmappings [<span class="emphasis"><em>physical node</em></span>]</code> 
</span></dt><dd>
Display all logical nodes mapped to
<span class="emphasis"><em>physical node</em></span> or all mappings if <span class="emphasis"><em>physical node</em></span> is omitted.
</dd><dt><span class="term">
<code class="literal">exec</code> 
</span></dt><dd>
Synchronously execute a command on the master.  This command will
block until it is completed.
</dd><dt><span class="term">
<code class="literal">source <span class="emphasis"><em>file</em></span></code> 
</span></dt><dd>
Reads the specified file and attempts to execute all of the
specified commands.
</dd><dt><span class="term">
<code class="literal">submit</code> 
</span></dt><dd>
Asynchronously execute a command on the master.  This command will
return immediately and allows the submission of other commands.  The command
ID of the last command submitted is recorded.
</dd><dt><span class="term">
<code class="literal">wait <span class="emphasis"><em>ms</em></span> [<span class="emphasis"><em>cmdid</em></span>]</code> 
</span></dt><dd>
This commands blocks for up to <code class="literal">ms</code> milliseconds
until <code class="literal">cmdid</code> has entered the SUCCEEDED or FAILED state. If <code class="literal">ms</code> is 0 the
command may block forever.  If the command times out, the shell will
disconnect.  This is useful in conjunction with <code class="literal">submitted</code> commands.
</dd><dt><span class="term">
<code class="literal">waitForNodesActive <span class="emphasis"><em>ms</em></span> node1 [node2 [&#8230;]]</code> 
</span></dt><dd>
This command blocks for up to
<code class="literal">ms</code> milliseconds until the specified list of nodes have entered the ACTIVE or
CONFIGURING state.  If ms==0 then the command may block forever.
</dd><dt><span class="term">
<code class="literal">waitForNodesDone <span class="emphasis"><em>ms</em></span> node1 [node2 [&#8230;]]</code> 
</span></dt><dd>
This command blocks for up to
<code class="literal">ms</code> milliseconds until the specified list of nodes have entered the IDLE,
ERROR, or LOST state.
</dd><dt><span class="term">
<code class="literal">quit</code> 
</span></dt><dd>
Exit the shell.
</dd></dl></div><p title="Exec and Submit commands"><b>Exec and Submit commands. </b>Both the web form and the FlumeShell are interfaces to the same command
processing infrastructure inside Flume. This section introduces the
FlumeShell and show how you can use it to make administering Flume more
simple.</p><p>These commands are issued and run as if run from the master.  In the command
shell they have the form:</p><p><code class="literal">exec <span class="emphasis"><em>command</em></span> [<span class="emphasis"><em>arg1 [_arg2</em></span> [ &#8230; ] ] ]</code></p><p><code class="literal">submit <span class="emphasis"><em>command</em></span> [<span class="emphasis"><em>arg1 [_arg2</em></span> [ &#8230; ] ] ]</code></p><p>Complex arguments like those with spaces, or non alpha-numeric characters can
be expressed by using "double quotes"s and `single quotes&#8217;s.  If enclosed in
double quotes, the bodies of the strings are Java string unescaped.  If they
are enclosed in single quotes, arbitrary characters can be included except for
the ' character.</p><p><code class="literal">exec</code> commands block until they are completed.  <code class="literal">submit</code> commands are
asynchronously sent to the master in order to be executed.  <code class="literal">wait</code> are
essentially joins for recently <code class="literal">submit</code> ted commands.</p><div class="variablelist"><dl><dt><span class="term">
<code class="literal">noop</code> 
</span></dt><dd>
This command contacts the master and issues a noop (no
operation) command.
</dd><dt><span class="term">
<code class="literal">config <span class="emphasis"><em>logicalnode</em></span> <span class="emphasis"><em>source</em></span> <span class="emphasis"><em>sink</em></span></code> 
</span></dt><dd>
This command configures a node.
This is nearly identical to the <span class="emphasis"><em>config</em></span> command.
</dd><dt><span class="term">
<code class="literal">multiconfig <span class="emphasis"><em><span class="emphasis"><em>flumespec</em></span></em></span></code> 
</span></dt><dd>
This command configures a set of nodes on the
master using the aggregated format.
</dd><dt><span class="term">
<code class="literal">unconfig <span class="emphasis"><em>logicalnode</em></span></code> 
</span></dt><dd>
This command changes the configuration of a
particular node to have a <code class="literal">null</code> source and a <code class="literal">null</code> state.  +refresh
</dd><dt><span class="term">
<span class="emphasis"><em>logicalnode</em></span>+ 
</span></dt><dd>
This command refreshes the current configuration of a
logical node.  This forces the logicalnode to stop and then restart.  This
also causes a master re-evaluation that may change the failover lists.
</dd><dt><span class="term">
<code class="literal">refreshAll <span class="emphasis"><em>logicalnode</em></span></code> 
</span></dt><dd>
This atomically issues a refresh command to all
of the logical nodes.
</dd><dt><span class="term">
<code class="literal">save <span class="emphasis"><em><span class="emphasis"><em>filename</em></span></em></span></code> 
</span></dt><dd>
This saves the current configuration to the master&#8217;s
disk.
</dd><dt><span class="term">
<code class="literal">load <span class="emphasis"><em><span class="emphasis"><em>filename</em></span></em></span></code> 
</span></dt><dd>
This augments the current configuration with the
logical node specifications found in <code class="literal"><span class="emphasis"><em>filename</em></span></code>.
</dd><dt><span class="term">
<code class="literal">map <span class="emphasis"><em>physicalnode</em></span> <span class="emphasis"><em>logicalnode</em></span></code>
</span></dt><dd>
This creates a new mapping between logical
node <span class="emphasis"><em>logicalnode</em></span> and physical node <code class="literal"><span class="emphasis"><em>physicalnode</em></span></code>.  The node starts with a
<code class="literal">null</code> source and a <code class="literal">null</code> sink, and updates its configuration specified at the
master when it begins heartbeating.  Thus if a logical node configuration
already exists and is mapped, it will pick up the configuration for the logical
node.
</dd><dt><span class="term">
<code class="literal">spawn <span class="emphasis"><em>physicalnode</em></span> <span class="emphasis"><em>logicalnode</em></span></code>
</span></dt><dd>
The <code class="literal">spawn: command is a synonym for the
+map</code> command and has been deprecated.
</dd><dt><span class="term">
<code class="literal">decommission <span class="emphasis"><em>logicalnode</em></span></code> 
</span></dt><dd>
This removes a logical node from the logical
node configuration table, and unmaps it from any physical nodes it may be
installed on.
</dd><dt><span class="term">
<code class="literal">unmap <span class="emphasis"><em>physicalnode</em></span> <span class="emphasis"><em>logicalnode</em></span></code> 
</span></dt><dd>
This command breaks the assignment of
a <span class="emphasis"><em>logicalnode</em></span> from machine <span class="emphasis"><em>physicalnode</em></span>.  A logical node can be reassigned
to another physical node using the <code class="literal">map</code> command.
</dd><dt><span class="term">
<code class="literal">unmapAll</code> 
</span></dt><dd>
This command breaks the assignment of all logical node from
physical nodes.  A logical node can be reassigned to another physical node
using the <code class="literal">map</code> command.
</dd></dl></div></div></div><div class="section" title="8.2. Flume&#8217;s Dataflow Specification Language"><div class="titlepage"><div><div><h3 class="title"><a name="_flume_8217_s_dataflow_specification_language"></a>8.2. Flume&#8217;s Dataflow Specification Language</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_special_sinks_fan_out_fail_over_and_roll">8.2.1. Special Sinks: Fan out, Fail over, and Roll</a></span></dt><dt><span class="section"><a href="#_introducing_sink_decorators">8.2.2. Introducing Sink Decorators</a></span></dt><dt><span class="section"><a href="#_translations_of_high_level_sources_and_sinks">8.2.3. Translations of High-level Sources and Sinks</a></span></dt></dl></div><p>Using the Flume node roles (collector, agent) is the simplest method to get up
and running with Flume.  Under the covers, these sources and sinks
(collectorSink, collectorSource, and agentSource) are actually composed of
primitive sinks that have been augmented with <span class="strong"><strong>translated components</strong></span> with
role defaults, <span class="strong"><strong>special sinks</strong></span> and <span class="strong"><strong>sink decorators</strong></span>. These components make
configuration more flexible, but also make configuration more complicated.
The combination of special sinks and decorators expose a lot of details of the
underlying mechanisms but are a powerful and expressive way to encode rich
behavior.</p><p>Flume enables users to enter their own composition of sinks, sources, and
decorators by using a domain specific data flow language.  The following
sections will describe this in more details.</p><pre class="screen">nodeName      ::=  NodeId
simpleSource  ::=  SourceId args?
simpleSink    ::=  SinkId args?
decoratorSink ::=  DecoId args?

source ::= simpleSource

sink ::=   simpleSink            // single sink
     |     [ sink (, sink)* ]    // fanout sink
     |     { decoratorSink =&gt; sink }  // decorator sink
     |     &lt; sink ? sink &gt;           // failover / choice sink
     |     roll(...) { sink }           // roll sink
     |     let SinkId := sink in sink  // let expression

logicalNode ::= NodeId : source | sink ;

spec   ::=  (logicalNode)*</pre><div class="section" title="8.2.1. Special Sinks: Fan out, Fail over, and Roll"><div class="titlepage"><div><div><h4 class="title"><a name="_special_sinks_fan_out_fail_over_and_roll"></a>8.2.1. Special Sinks: Fan out, Fail over, and Roll</h4></div></div></div><p>Three special sinks are FanOutSinks, FailoverSinks, and RollSinks.</p><p>Fanout sinks send any incoming events to all of the sinks specified to be its
children.  These can be used for data replication or for processing data off
of the main reliable data flow path.  Fanout is similar to the Unix <code class="literal">tee</code>
command or logically acts like an AND operator where the event is sent to each
subsink.</p><p>The syntax for a FanoutSink is :</p><pre class="screen">[ console, collectorSink ]</pre><p>FailoverSinks are used to handle failures when appending new events.
FailoverSinks can be used to specify alternate collectors to contact in the
event the primary collector fails, or a local disk sink to store data until
the primary collector recovers.  Failover is similar to exception handling and
logically acts like an OR operator. If a failover is successful, one of the
subsinks has received the event.</p><p>The syntax for a FailoverSink is :</p><pre class="screen">&lt; logicalSink("collector1") ? logicalSink("collector2") &gt;</pre><p>So, you could configure node "agent1" to have a failover to collector2 if
collector1 fails (for example, if the connection to <code class="literal">collector1</code> goes down or
if <code class="literal">collector1</code>'s HDFS becomes full):</p><pre class="screen">agent1 : source | &lt; logicalSink("collector1") ? logicalSink("collector2")
&gt; ;</pre><p>Roll sink opens and closes a new instance of its subsink every
<span class="emphasis"><em>millis</em></span> milliseconds.  A roll is an atomic transition that closes
the current instance of the sub-sink, and then opens a new instance of
as an escape sequence to differentiate data in different roll periods.
This can be used to make every roll produce a new file with a unique
name.</p><p>The syntax for a roll sink is:</p><pre class="screen">roll(millis) sink</pre><p>These can be composed to have even richer behavior.  For example, this sink
outputs to the console and has a failover collector node.</p><pre class="screen">[ console, &lt; logicalSink("collector1") ? logicalSink("collector2") &gt; ]</pre><p>This one rolls the collector every 1000 milliseconds writing to a different
HDFS file after each roll.</p><pre class="screen">roll(1000) [ console, escapedCustomDfs("hdfs://namenode/flume/file-%{rolltag}") ]</pre></div><div class="section" title="8.2.2. Introducing Sink Decorators"><div class="titlepage"><div><div><h4 class="title"><a name="_introducing_sink_decorators"></a>8.2.2. Introducing Sink Decorators</h4></div></div></div><p>Fan out and failover affect where messages go in the system but do not modify
the messages themselves.  To augment or filter events as they pass through the
dataflow, you can use <span class="strong"><strong>sink decorators</strong></span>.</p><p>Sink decorators can add properties to the sink and can modify the data streams
that pass through them.  For example, you can use them to increase reliability
via write ahead logging, increase network throughput via batching/compression,
sampling, benchmarking, and even lightweight analytics.</p><p>The following simple sampling example uses an intervalSampler which is
configured to send every 10th element from source "source" to the sink "sink":</p><pre class="screen">flumenode: source | { intervalSampler(10) =&gt; sink };</pre><p>Here&#8217;s an example that batches every 100 events together.</p><pre class="screen">flumenode: source | { batch(100) =&gt; sink };</pre><p>Like fanout and failover, decorators are also composable. Here is an example
that creates batches of 100 events and then compresses them before moving
them off to the sink:</p><pre class="screen">flumenode: source | { batch(100) =&gt;  { gzip =&gt; sink } };</pre></div><div class="section" title="8.2.3. Translations of High-level Sources and Sinks"><div class="titlepage"><div><div><h4 class="title"><a name="_translations_of_high_level_sources_and_sinks"></a>8.2.3. Translations of High-level Sources and Sinks</h4></div></div></div><p>Internally, Flume translates sinks into compositions of other simpler
decorators, failovers, rollers to add properties.  The proper compositions
create pipelines that provide different levels of reliability in Flume.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>This section describes how agents work, but this version of
Flume does not expose the translations the same way the auto*Chains
are exposed.  A future version of Flume will expose these details.
The exact translations are still under development.</p></td></tr></table></div><p>Suppose you have nodes using the different agent sinks:</p><pre class="screen">node1 : tail("foo") | agentE2ESink("bar");
node2 : tail("foo") | agentDFOSink("bar");
node3 : tail("foo") | agentBESink("bar");</pre><p>In the translation phases, agentE2ESink is actually converted into these Flume
sinks:</p><pre class="screen">node1 : tail("foo") | { ackedWriteAhead =&gt; { lazyOpen =&gt; { stubbornAppend
=&gt; logicalSink("bar") } } } ;
node2 : tail("foo") | let primary := { lazyOpen -&gt; {stubbornAppend =&gt; logicalSink("bar") } } in &lt; primary ? { diskFailover =&gt; { insistentOpen =&gt; primary} } &gt;;
node3 : tail("foo") | { lazyOpen =&gt; { stubbornAppend =&gt;  logicalSink("bar")  } };</pre><p><code class="literal">ackedWriteAhead</code> is actually a complicated decorator that internally
uses rolls and some other special decorators.  This decorator
interface allows us to manually specify wire batching and compression
options.  For example, you could compress every 100 messages using
gzip compression:</p><pre class="screen">node1 : tail("foo") | { ackedWriteAhead =&gt; { batch(100) =&gt; { gzip =&gt; {
lazyOpen =&gt; { stubbornAppend =&gt; logicalSink("bar") } } } } };</pre><p><code class="literal">collectorSink("xxx","yyy",15000)</code> is also a bit complicated with some custom
decorators to handle acks.  Under the covers however, it depends on a roller
with a escapedCustomDfsSink inside of it.</p><pre class="screen">roll(15000) { collectorMagic =&gt;  escapedCustomDfs("xxx", "yyy-%rolltag") }</pre><p>Another place translations happen is with logical nodes.  Lets start of with a
few nodes:</p><pre class="screen">node1 : tail("foo") | { .... =&gt; logicalSink("node2") };
node2 : logicalSource | collectorSink("...");</pre><p>The translation mechanisms converts logicalSources and logicalSinks into lower
level physical rpcSources and rpcSinks.  Lets assume that node1 is on machine
host1 and node2 is on machine host2.  After the translation you end up with the
following configurations:</p><pre class="screen">node1 : tail("foo") | { .... =&gt; rpcSink("host2",12345) };
node2 : rpcSource(12345) | collectorSink("..."");</pre><p>Suppose that you swap the mapping so that node2 is now on host1 and node1 is
on host2.</p><pre class="screen"># flume shell commands
exec unmapAll
exec map host1 node2
exec map host2 node1</pre><p>The original configuration will now be translated to:</p><pre class="screen">node1 : tail("foo") | { .... =&gt; rpcSink("host1",12345) };
node2 : rpcSource(12345) | collectorSink("..."");</pre></div></div><div class="section" title="8.3. Custom Metadata Extraction"><div class="titlepage"><div><div><h3 class="title"><a name="_custom_metadata_extraction"></a>8.3. Custom Metadata Extraction</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_extractors">8.3.1. Extractors</a></span></dt><dt><span class="section"><a href="#_meta_data_filtering_and_transformations">8.3.2. Meta Data Filtering and Transformations</a></span></dt><dt><span class="section"><a href="#_role_defaults">8.3.3. Role Defaults</a></span></dt><dt><span class="section"><a href="#_arbitrary_data_flows_and_custom_architectures">8.3.4. Arbitrary Data Flows and Custom Architectures</a></span></dt></dl></div><p>While Flume can take in raw data, you can add structure to data based on the
nodes they flowed through, and filter out portions of data to minimize the
amount of raw data needed if only a portion is needed.</p><p>The simplest is the <code class="literal">value</code> decorator.  This takes two arguments: an attribute
name, and a value to add to the event&#8217;s metadata.  This can be used to
annotate constant information about the source, or arbitrary data to a
particular node.  It could also be used to naively track the provenance of
data through Flume.</p><div class="section" title="8.3.1. Extractors"><div class="titlepage"><div><div><h4 class="title"><a name="_extractors"></a>8.3.1. Extractors</h4></div></div></div><p>Extraction operations are also available to extract values from logs that
have known structure.</p><p>One example is the <code class="literal">regex</code> decorator.  This takes three arguments: a regular
expression, an index, and an attribute name which allows the user to use a
extract a particular regex group out of the body of the event and write it as
the value of the specified attribute.</p><p>Similarly the <code class="literal">split</code> decorator also takes three arguments: a regular
expression, an index, and an attribute name.  This splits the body
based on the regular expression, extracts the text group after the
instance of the separator, and writes the value to the specified
attribute.  One can think of this as a much simplified version of the
<code class="literal">awk</code> Unix utility.</p></div><div class="section" title="8.3.2. Meta Data Filtering and Transformations"><div class="titlepage"><div><div><h4 class="title"><a name="_meta_data_filtering_and_transformations"></a>8.3.2. Meta Data Filtering and Transformations</h4></div></div></div><p>Flume enforces an invariant that prevents the modification of an attribute
that has already been written upstream.  This simplifies debugging of
dataflows, and improves the visibility of data when debugging.</p><p>If many stages are used, however, frequently a lot of extra metadata ends up
being moved around.  To deal with this, a few extra operations are available.</p><p>A <code class="literal">select</code> operation is available.  This operation is like SQL select, which
provides a relational calculus&#8217;s set projection operation that modifies an
event so that the specified metadata fields are forwarded.</p><p>A <code class="literal">mask</code> operation is also available that forwards all metadata attributes
except for the attributes specified.</p><p>A <code class="literal">format</code> decorator is also available that uses the escape mechanisms
to rewrite the body of an event to a user customizable message.  This is
useful for outputting summary data to low volume sources.  Example: writing
summary information out to an IRC channel periodically.</p></div><div class="section" title="8.3.3. Role Defaults"><div class="titlepage"><div><div><h4 class="title"><a name="_role_defaults"></a>8.3.3. Role Defaults</h4></div></div></div><p>To simplify things for users, you can assign a particular role to a logical
node&#8201;&#8212;&#8201;think of these as an "automatic" specification that have
default settings.  Two roles currently provided are the agent role and the
collector role.  Since there are many nodes with the same role, each of
these are called a tier.  So, for example, the agent tier consists of all the nodes
that are in the agent role.  Nodes that have the collector role are in the
collector tier.</p><p>Agents and collectors roles have defaults specified in the <code class="literal">conf/flume-site.xml</code>
file.  Look at the <code class="literal">conf/flume-conf.xml</code> file for properties prefixed with
<code class="literal">flume.agent.*</code> and <code class="literal">flume.collector.*</code> for descriptions of the configuration
options.</p><p>Each node maintains it own node state and has its own configuration. If the
master does not have a data flow configuration for the logical node, the
logical node will remain in IDLE state.  If a configuration is present, the
logical node will attempt to instantiate the data flow and have it work
concurrently with other data flows.</p><p>This means that each machine essentially only lives in one tier.  In a more
complicated setup, it is possible to have a machine that contains many logical
nodes, and because each of these nodes can take on different roles, the
machine lives in multiple tiers.</p></div><div class="section" title="8.3.4. Arbitrary Data Flows and Custom Architectures"><div class="titlepage"><div><div><h4 class="title"><a name="_arbitrary_data_flows_and_custom_architectures"></a>8.3.4. Arbitrary Data Flows and Custom Architectures</h4></div></div></div><p>With tsinks and tsource, data can be sent through multiple nodes.  If ack
injection and ack checking decorators are properly inserted, you can achieve
reliability.</p></div></div><div class="section" title="8.4. Extending via Sink/Source/Decorator Plugins"><div class="titlepage"><div><div><h3 class="title"><a name="_extending_via_sink_source_decorator_plugins"></a>8.4. Extending via Sink/Source/Decorator Plugins</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_semantics_of_flume_extensions">8.4.1. Semantics of Flume Extensions</a></span></dt><dd><dl><dt><span class="section"><a href="#_simple_source_semantics">8.4.1.1. Simple source semantics.</a></span></dt><dt><span class="section"><a href="#_buffered_source_semantics">8.4.1.2. Buffered source semantics</a></span></dt><dt><span class="section"><a href="#_simple_sinks">8.4.1.3. Simple Sinks.</a></span></dt><dt><span class="section"><a href="#_buffered_sink_and_decorator_semantics">8.4.1.4. Buffered sink and decorator semantics</a></span></dt><dt><span class="section"><a href="#_retries_sleeps_and_unclean_exits">8.4.1.5. Retries, sleeps, and unclean exits.</a></span></dt></dl></dd></dl></div><p>An experimental plugin mechanism is provided that allows you to add new custom
sources, sinks, and decorators to the system.</p><div class="orderedlist" title="Two steps are required to use this feature."><p class="title"><b>Two steps are required to use this feature.</b></p><ol class="orderedlist" type="1"><li class="listitem">
First, add the jar with the new plugin classes to flume&#8217;s classpath.
  If the plugin requires DLL&#8217;s/so&#8217;s make sure these are in the
  LD_LIBRARY_PATH (unix .so) or PATH (windows .dll)
</li><li class="listitem">
Second, in <code class="literal">flume-site.xml</code>, add the class names of the new sources,
  sinks, and/or decorators to the <code class="literal">flume.plugin.classes</code> property.
  Multiple classes can be specified by comma separating the list. Java
  reflection is used to find some special static methods that add new
  components to the system and data flow language&#8217;s library.
</li></ol></div><p>An example component has been "pluginified"&#8201;&#8212;&#8201;the "HelloWorld"
source, sink, and decorator. This plugin does something very simple;
the source generates the text "hello world!" every three seconds, the
sink writes events to a "helloworld.txt" text file, and the decorator
prepends "hello world!" to any event it encounters.</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
cd into the <code class="literal">plugins/helloworld</code> directory and type <code class="literal">ant</code>, a <code class="literal">helloworld_plugin.jar</code> file will be generated
</li><li class="listitem"><p class="simpara">
Add the following to flume-site.xml (create it if it doesn&#8217;t already
  exist)
  "helloworld.HelloWorldSink,helloworld.HelloWorldSource,helloworld.HelloWorldDecorator"
  to the <code class="literal">flume.plugin.classes</code> property in <code class="literal">flume-site.xml</code>.
</p><div class="important" title="Important" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Important"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Important]" src="images/important.png"></td><th align="left">Important</th></tr><tr><td align="left" valign="top"><p>if you use the provided <code class="literal">flume-site.xml.template</code> file to
create your <code class="literal">flume-site.xml</code> be sure to comment out or remove any
example properties contained in the sample template.</p></td></tr></table></div><p title="Example flume-site.xml contents"><b>Example flume-site.xml contents. </b>
</p><pre class="programlisting">&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;flume.plugin.classes&lt;/name&gt;
    &lt;value&gt;helloworld.HelloWorldSink,helloworld.HelloWorldSource,helloworld.HelloWorldDecorator&lt;/value&gt;
    &lt;description&gt;Comma separated list of plugins&lt;/description&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</pre><p title="Example flume-site.xml contents">
</p></li><li class="listitem"><p class="simpara">
Start the Flume master and at least one logical node in separate terminals
</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
in each terminal cd into the top-level flume directory, should be just above <code class="literal">plugins</code>
</li><li class="listitem"><p class="simpara">
Add <code class="literal">helloworld_plugin.jar</code> to the FLUME_CLASSPATH in <span class="strong"><strong>both</strong></span> terminals
</p><pre class="screen">export FLUME_CLASSPATH=`pwd`/plugins/helloworld/helloworld_plugin.jar</pre></li><li class="listitem">
in terminal 1 run <code class="literal">bin/flume master</code>
</li><li class="listitem">
in terminal 2 run <code class="literal">bin/flume node -n hello1</code>
</li></ol></div></li><li class="listitem"><p class="simpara">
At this point the master and hello1 nodes should be started and will have loaded the plugin
</p><p title="You should see log output similar to the following in both master and hello1:"><b>You should see log output similar to the following in both master and hello1: </b>
</p><pre class="screen">10/07/29 17:35:28 INFO conf.SourceFactoryImpl: Found source builder helloWorldSource in helloworld.HelloWorldSource
10/07/29 17:35:28 INFO conf.SinkFactoryImpl: Found sink builder helloWorldSink in helloworld.HelloWorldSink
10/07/29 17:35:28 INFO conf.SinkFactoryImpl: Found sink decorator helloWorldDecorator in helloworld.HelloWorldDecorator</pre><p title="You should see log output similar to the following in both master and hello1:">
</p><div class="tip" title="Tip" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Tip"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Tip]" src="images/tip.png"></td><th align="left">Tip</th></tr><tr><td align="left" valign="top"><p>Another way to verify that your plugin is loaded is to check if it is displayed on this page <a class="ulink" href="http://localhost:35871/masterext.jsp" target="_top">http://localhost:35871/masterext.jsp</a></p></td></tr></table></div></li><li class="listitem"><p class="simpara">
Configure hello1
</p><div class="tip" title="Tip" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Tip"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Tip]" src="images/tip.png"></td><th align="left">Tip</th></tr><tr><td align="left" valign="top"><p>The easiest way to do this is open the configuration page of the master in a browser, typically this link <a class="ulink" href="http://localhost:35871/flumeconfig.jsp" target="_top">http://localhost:35871/flumeconfig.jsp</a></p></td></tr></table></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
load the helloworld source/sink into our <code class="literal">hello1</code> node (the bottom text box, then submit button if you are using the master&#8217;s web interface"
</p><pre class="screen">hello1: helloWorldSource() | helloWorldSink();</pre></li><li class="listitem"><p class="simpara">
you could also try the hello world decorator
</p><pre class="screen">hello1: helloWorldSource() | { helloWorldDecorator() =&gt; helloWorldSink() };</pre><p class="simpara">In either case <code class="literal">hello1</code> will output a <code class="literal">helloworld.txt</code> file into it&#8217;s current working directory. Every 3 seconds a new "hello world!" line will be output to the file.</p></li></ol></div></li></ol></div><div class="section" title="8.4.1. Semantics of Flume Extensions"><div class="titlepage"><div><div><h4 class="title"><a name="_semantics_of_flume_extensions"></a>8.4.1. Semantics of Flume Extensions</h4></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_simple_source_semantics">8.4.1.1. Simple source semantics.</a></span></dt><dt><span class="section"><a href="#_buffered_source_semantics">8.4.1.2. Buffered source semantics</a></span></dt><dt><span class="section"><a href="#_simple_sinks">8.4.1.3. Simple Sinks.</a></span></dt><dt><span class="section"><a href="#_buffered_sink_and_decorator_semantics">8.4.1.4. Buffered sink and decorator semantics</a></span></dt><dt><span class="section"><a href="#_retries_sleeps_and_unclean_exits">8.4.1.5. Retries, sleeps, and unclean exits.</a></span></dt></dl></div><p>Flume uses threading to support reconfiguration and multiple logical
nodes.  Sources, sinks, and decorator are extensions that can use
queues for buffering and can use clock sleeps for periodic rolls and
retries.  Because we use these blocking operations and because the
internal concurrency control mechanisms we use can cause deadlocks or
hangs, there are several rules that need to be followed and enforced
by test cases in order to be a well-behaved source, sink, or
decorator.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>This section is a draft and not exhaustive or completely formal.
More restrictions may be added to this in the future.</p></td></tr></table></div><p title="Semantics of sources"><b>Semantics  of sources. </b>Sources must implement 4 methods</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
void open() throws IOException
</li><li class="listitem">
Event next() throws IOException
</li><li class="listitem">
void close() throws IOException
</li><li class="listitem">
ReportEvent getReport()
</li></ul></div><p>Along with these signatures, each of these methods can also throw
RuntimeExceptions.  These exceptions indicate a failure condition and
by default will make a logical node shutdown in ERROR state.  These
error messages are user visible and it is important that they have
actionable descriptions about the failure conditions without having to
dump stack.  Thus, NullPointerExceptions are not acceptable&#8201;&#8212;&#8201;they
are not descriptive or helpful without their stack traces.</p><p>Some examples of valid runtime exceptions include invalid arguments
that prevent a source from opening and invalid state infractions
(attempting to next on a closed source).</p><div class="section" title="8.4.1.1. Simple source semantics."><div class="titlepage"><div><div><h5 class="title"><a name="_simple_source_semantics"></a>8.4.1.1. Simple source semantics.</h5></div></div></div><div class="informalfigure"><div class="mediaobject"><img src="sourceStatesSimple.png" alt="sourceStatesSimple.png"></div></div><p>Simple sources are assumed to be sources whose open and close
operation happen quickly and do not block for long periods of time.
The longest pauses tolerated here on the order of 10s (default time
for a failing DNS lookup).</p><p>The constructor for the sources should be callable without grabbing
resources that can block or that require IO such as network connectors
or file handles.  If there are errors due to configuration settings
that can be caught in the constructor, an IllegalArgumentException
should be thrown.</p><p><code class="literal">open()</code> is a call that grabs resources for the source so that the
<code class="literal">next()</code> call can be made.  The <code class="literal">open</code> call of a simple source should
attempt to fail fast. It can throw a IOException or a RuntimeException
such as IllegalStateException.  Open should only be called on a CLOSED
sink&#8201;&#8212;&#8201;if a sink is opened twice, the second call should throw an
IOException or IllegalStateException.</p><p><code class="literal">next()</code> is a call that gets an event from an open source.  If a
source is not open, this should throw an IllegalStateException.  This
call can and will often block. If <code class="literal">next()</code> is blocking, a call to
<code class="literal">close()</code> should unblock the <code class="literal">next()</code> by having it exit cleanly
returning null.  Many resources such as TCP-sockets (and ideally rpc
frameworks) default to throwing an exception on blocked network reads
(like a <code class="literal">next()</code> call) when <code class="literal">close()</code>'d.</p><p><code class="literal">close()</code> is a call that releases resources that the open call of a
source grabs.  The <code class="literal">close()</code> call itself blocks until all resources
are released.  This allows a subsequent <code class="literal">open()</code> in the same thread to
not fail due to resource contention (ex, closing a server socket on
port 12345 should not return until port 12345 is ready to be bound
again).</p><p><code class="literal">getReport()</code> returns a ReportEvent.  These values should be available
regardless if the node is open or closed and this call should not
block the other source by other calls (due to potential lock inversion
issues).  The values retrieved are ideally atomically grabbed but this
is not required as long as no errors are caused by racy execution.</p><p>If a source is opened or closed multiple times, it is up to the source to
determine if values are reset or persisted between open/close cycles.</p></div><div class="section" title="8.4.1.2. Buffered source semantics"><div class="titlepage"><div><div><h5 class="title"><a name="_buffered_source_semantics"></a>8.4.1.2. Buffered source semantics</h5></div></div></div><div class="informalfigure"><div class="mediaobject"><img src="sourceStatesBuffered.png" alt="sourceStatesBuffered.png"></div></div><p>Some sources have in memory queues or buffers stored persistently on
disk.  The guiding principle here is that on <code class="literal">close()</code>, buffered
sources should prevent new data from entering and attempt to flush the
buffered data.  Also any subordinate threads should be released before
close returns.  If no progress is being made on <code class="literal">close()</code>, for a given
period of time (30s is the default currently) the controlling thread
will for an Thread.interrupt() call.  The source should be able to
handle InterruptedExceptions and percolate interrupted status up the
returning call stack.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>In v0.9.1 and v0.9.2, interruptions when caught should be
handled by re-flagging the Thread&#8217;s interrupted flag (a call to
<code class="literal">Thread.currentThread().interrupt()</code>) and then throwing an
IOException.  The API for extensions will likely change in the future
to throw either an IOException or an InterruptedException.</p></td></tr></table></div><p><code class="literal">open()</code> no change.</p><p><code class="literal">close()</code> This call is usually made from a separate thread than the
open() or next() calls.  Since this call should block until resources
are freed, it should attempt to flush its buffers before returning.
For example, so if a network source has some buffered data, the
network connection should be closed to prevent new data from entering,
and then the buffered data should be flushed.  The source should be
able to handle InterruptedExceptions and percolate interrupted status
up the returning call stack and indicate an error state.</p><p>A <code class="literal">next()</code> call while in CLOSING state should continue pulling values
out of the buffer until it is empty.  This is especially true if the
next() call is happening in the main driver thread or a subordinate
driver thread.  One mechanism to get this effect is to add a special
DONE event to the queue/buffer that indicates a clean exit.</p><p><code class="literal">getReport()</code> ideally includes metric information like the size of the
queues in the source, and the number of elements in the queue.</p><p title="Semantics of sinks and decorators"><b>Semantics of sinks and decorators. </b>Sinks and decorators must implement 4 methods</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
void open() throws IOException
</li><li class="listitem">
void append(Event e) throws IOException
</li><li class="listitem">
void close() throws IOException
</li><li class="listitem">
ReportEvent getReport()
</li></ul></div><p>Along with these signatures, each of these methods can also throw
RuntimeExceptions.  Run-time exceptions indicate a failure condition
and by default will make a logical node shutdown in ERROR state.
These error messages are user visible and it is important that they
have helpful descriptions about the failure conditions without having
to dump stack.  Thus, NullPointerExceptions are not acceptable&#8201;&#8212;&#8201;they
are not descriptive or helpful without their stack traces.</p></div><div class="section" title="8.4.1.3. Simple Sinks."><div class="titlepage"><div><div><h5 class="title"><a name="_simple_sinks"></a>8.4.1.3. Simple Sinks.</h5></div></div></div><div class="informalfigure"><div class="mediaobject"><img src="sinkStatesSimple.png" alt="sinkStatesSimple.png"></div></div><p>Simple sinks are assumed to have open, close, and append operations
that happen quickly and do not block for long periods of time.  The longest
pauses tolerated here on the order of 10s (default time for a failing
DNS lookup).</p><p>The constructor for the sinks and decorators should be callable
without grabbing resources that can block or that require IO such as
network connectors or file handles.  If there are errors due to
configuration settings that can be caught in the constructor, an
IllegalArgumentException should be thrown.</p><p><code class="literal">open()</code> is a call that grabs resources for the sink or decorator so
that the <code class="literal">append(Event)</code> call can be made.  If there are errors due to
configuration settings not detectable without IO in the constructor,
<code class="literal">open()</code> should attempt to fail fast and throw a IOException or
RuntimeException such as IllegalStateException or
IllegalArgumentException.  Open should only be called on a CLOSED sink&#8201;&#8212;&#8201;if a sink is opened twice, the second call should throw an
IOException or IllegalStateException.</p><p><code class="literal">append()</code> is a call that delivers a event.  If a sink is not open,
this should throw an IllegalStateException.</p><p>If a normal decorator fails to open or to append because of an
internal failure or a subsink fails to open, the decorator should
release its resources attempt to close the subsink and then throw an
exception.  There are some sink/decos that specifically manipulate
these semantics&#8201;&#8212;&#8201;this needs to be done with care.</p><p><code class="literal">close()</code> is a call that releases resources that the open call of a
source grabs.  If <code class="literal">open()</code> or <code class="literal">next()</code> is blocking, a call to this
should unblock the call and have them exit.  <code class="literal">close()</code> should be
called called on an open sink, but we allow a closed sink to have
<code class="literal">close()</code> called on it without throwing an exception (generally LOG
warning however).</p><p><code class="literal">getReport()</code> returns a ReportEvent.  These values should be available
regardless if the node is open or closed and this call should not
cause get blocked by other calls (due to potential lock inversion
issues).  The values retrieved are ideally atomically grabbed but this
is not required as long as no errors are caused by racy execution.  If
a sink is opened or closed multiple times, it is up to the sink to
determine if values are reset or persisted between open/close cycles.</p></div><div class="section" title="8.4.1.4. Buffered sink and decorator semantics"><div class="titlepage"><div><div><h5 class="title"><a name="_buffered_sink_and_decorator_semantics"></a>8.4.1.4. Buffered sink and decorator semantics</h5></div></div></div><div class="informalfigure"><div class="mediaobject"><img src="sinkStatesBuffered.png" alt="sinkStatesBuffered.png"></div></div><p>Some sinks have queues or buffers stored in memory or persistently on
disk.  The guiding principle here is that buffered sinks should
attempt to flush its buffers when prompted to <code class="literal">close()</code>.  This needs
to be balanced with the requirement that sinks and decorated sinks
should attempt to close in a relatively quick fashion.</p><p><code class="literal">open()</code> Since this sink isn&#8217;t open this generally means there is no
buffered data.  However, an <code class="literal">open()</code> on a sink or decorator with
persistent data should attempt to recover data and enqueue it in the
<code class="literal">open()</code> call. Examples of these include a DFO or WAL log recovering
dfo/wla logs, when a network subsink is down.</p><p>An <code class="literal">append()</code> call may buffer data before sending it (such as a
batching decorator).  A <code class="literal">close()</code> call, should attempt to append
buffered data to its (sub)sink before executing the close. Also, any
subordinate threads should be stopped before shutdown.</p><p>If no progress is being made on <code class="literal">close()</code>, for a given period of time
(30s is the default currently) it will be interrupted and should
handle abruptly exiting because of this interruption.</p></div><div class="section" title="8.4.1.5. Retries, sleeps, and unclean exits."><div class="titlepage"><div><div><h5 class="title"><a name="_retries_sleeps_and_unclean_exits"></a>8.4.1.5. Retries, sleeps, and unclean exits.</h5></div></div></div><div class="informalfigure"><div class="mediaobject"><img src="sinkStatesOpening.png" alt="sinkStatesOpening.png"></div></div><p>Retry on open semantics</p><div class="informalfigure"><div class="mediaobject"><img src="sinkStatesAppending.png" alt="sinkStatesAppending.png"></div></div><p>Retry on Append semantics</p><p>Some decorators introduce retries and sleeps. An author who uses these
needs to ensure that these operations behave well on open, append, and
close.  When combined with buffering sinks, flushing a buffer on close
may not be possible! (ex: wal trying to send data to a dead network
connection).  This means these sinks/decos need a way to exit abruptly
and report an error.  There are two operations that make these cases
 need to be handled: unbounded retries and unbounded/long
sleeps/await/blocking.</p><p>Some decorators have potentially unbounded retry semantics.  For
example, InsistentOpen, InsistentAppend, and FailoverSinks have the
potential to retry <code class="literal">open()</code> and <code class="literal">append()</code> calls an unbounded number
of times.  These decorators can also be wrapped with in others&#8201;&#8212;&#8201;this
means we need to be able to percolate the hard exit and bypass the
retry logic.</p><p>To do this we require that any sink/deco that has retry logic must
check for hard exit before retrying.  These sinks must to propagate
the hard exit interruption to its upstream decorators (in case they
have retry logic!).</p><p>Some sinks have the potential to backoff or sleep for long or
potentially unbounded amounts of time.  Code using sleeps or
synchronization operations such as waiting on latches
(<code class="literal">CountDownLatch.await()</code>) or thread sleeps (<code class="literal">Thread.sleep()</code>) must
properly handle interruptions.  Since these yielding operations are
usually only used in retry operations (which meant there was a
failure), the sink/deco needs to propagate the interruption and fail
with error.</p><p>There are some ramifications of these semantics.  Care must be taken
with locking open, close, and append operations.  If there are any
sleeps or blocking <code class="literal">open()</code> operations (ex: InsistentOpen,
FailoverSinks), ideally a close call will cause it to shutdown, it and
the open call should get unblocked.  <code class="literal">append()</code></p><p>The sink signaled to be closed but blocked on <code class="literal">append()</code> or <code class="literal">open()</code>
should exit in a reasonable amount of time&#8201;&#8212;&#8201;ideally within a few
heartbeats (5s is the default, so ideally &lt;30s).  If the sink exits
and its buffers are empty, it should do a normal successful return.
If there were unflushed events, it should return error by throwing an
exception.  If there are subordinate threads, these should be
terminated before close returns.</p></div></div></div><div class="section" title="8.5. Limiting Data Transfer Rate between Source-Sink pairs"><div class="titlepage"><div><div><h3 class="title"><a name="_limiting_data_transfer_rate_between_source_sink_pairs"></a>8.5. Limiting Data Transfer Rate between Source-Sink pairs</h3></div></div></div><p>Flume has the ability to limit the rate of transfer of data between source-sink pairs.
This is useful to divide the network bandwidth non-uniformly among different
source-sink pairs based on the kind of logs they are transferring.
For example, you can ship some types of logs at a faster rate than others, or you can transfer logs at different rates at different times of the day.</p><p>Another example of when it is beneficial to limit the rate of data transfer is when the network (or a collector) recovers after a failure.
In this case, the agents might have a lot of data backed up to ship; if there is no limit on the transfer rate,
the agents can exhaust all of the resources of the collector and possibly cause it to crash.</p><p>In Flume, you can use a special sink-decorator called the <code class="literal">choke</code> decorator to limit the rate of transfer of data between source-sink pairs.
Each <code class="literal">choke</code> decorator (called a <code class="literal">choke</code>) has to be assigned to a <code class="literal">choke-id</code>.
Here is an example of using a <code class="literal">choke</code> between a source and sink of a node; the <code class="literal">choke-id</code> of this <code class="literal">choke</code> is "Cid".</p><pre class="screen">node: source | { choke("Cid") =&gt; sink };</pre><p>The <code class="literal">choke-ids</code> are specific to a physical node. Before using a <code class="literal">choke</code> on <code class="literal">node</code>,
you must register the <code class="literal">choke-id</code> on the physical node containing the <code class="literal">node</code>.
You can register a <code class="literal">choke-id</code> on a physical node using the <code class="literal">setChokeLimit</code> command.
When registering a <code class="literal">choke-id</code>, you must also assign a rate limit (in KB/sec) to it.
Here is an example of registering a <code class="literal">choke-id</code> "Cid" on a physical node <code class="literal">host</code> and assigning a limit 1000 KB/sec.</p><pre class="screen">exec setChokeLimit host Cid 1000</pre><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>You can also use setChokeLimit at the run-time to change the limit assigned to a <code class="literal">choke-id</code>.</p></td></tr></table></div><p>The limit on the <code class="literal">choke-id</code> specifies an upper bound on the rate at which the <code class="literal">chokes</code> using that <code class="literal">choke-id</code> can collectively transfer data.
In the preceding example, there is only one source-sink pair on the physical node <code class="literal">host</code> that uses a <code class="literal">choke</code> with <code class="literal">choke-id</code> "Cid". Consequently, the rate of data transfer between that source-sink pair is limited to 1000 KB/sec.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>For the purpose of rate limiting, only the size of the event body is taken into account.</p></td></tr></table></div><p>The <code class="literal">choke</code> decorator works as follows: when <code class="literal">append()</code> is called on the sink to which the <code class="literal">choke</code>
is attached, the <code class="literal">append()</code> call works normally if the amount of data transferred
(during a small duration of time) is within the limit assigned to the <code class="literal">choke-id</code> corresponding to the <code class="literal">choke</code>.
If the limit has been exceeded, then <code class="literal">append()</code> is blocked for a small duration of time.</p><p>Suppose there are multiple source-sink pairs using <code class="literal">chokes</code> between them, and they are using
the same <code class="literal">choke-id</code>.  Suppose further that both <code class="literal">node1</code> and <code class="literal">node2</code> are logical nodes on the same physical node <code class="literal">host</code>,
and a <code class="literal">choke-id</code> "Cid" with limit 1000KB/sec is registered on <code class="literal">host</code>.</p><pre class="screen">node1: source1 | { choke("Cid") =&gt; sink1 };
node2: source2 | { choke("Cid") =&gt; sink2 };</pre><p>In this example, because both <code class="literal">source1-sink1</code> and <code class="literal">source2-sink2</code> pairs are using <code class="literal">chokes</code> with the same <code class="literal">choke-id</code> "Cid", the total data going across these source-sink pairs collectively is limited to 1000KB/sec.
Flume does not control how this limit will be divided between the source-sink pairs,
but it does guarantee that neither source-sink pair will starve.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>If multiple source-sink pairs on the same physical node use chokes that have the same choke-id,
then there is no guarantee how the rate limit will be divided between these source-sink pairs.</p></td></tr></table></div></div></div><div class="section" title="9. Flume and HDFS Security Integration"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_flume_and_hdfs_security_integration"></a>9. Flume and HDFS Security Integration</h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_basics">9.1. Basics</a></span></dt><dt><span class="section"><a href="#_setting_up_flume_users_on_kerberos">9.2. Setting up Flume users on Kerberos</a></span></dt><dd><dl><dt><span class="section"><a href="#_administering_kerberos_principals">9.2.1. Administering Kerberos principals</a></span></dt></dl></dd></dl></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>This section is only required if you are using a Kerberized HDFS
cluster.  If you are running CDH3b2 or a Hadoop version 0.21.x or
earlier, you can safely skip this section.</p></td></tr></table></div><p>Flume&#8217;s datapath needs to be able to interact with "secured" Hadoop
and HDFS.  The Hadoop and HDFS designers have chosen to use the
Kerberos V5 system and protocols to authenticate communications
between clients and services.  Hadoop clients include users, MR jobs
on behalf of users, and services include HDFS, MapReduce.</p><p>In this section we will describe how setup up a Flume node to be a
client as user <span class="emphasis"><em>flume</em></span> to a kerberized HDFS service.  This section
will <span class="strong"><strong>not</strong></span> talk about securing the communications between Flume nodes
and Flume masters, or the communications between Flume nodes in a
Flume flow.  The current implementation does not support writing
individual isolated flows as different users.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>This has only been tested with the security enhanced betas of
CDH (CDH3b3+), and the MIT Kerberos 5 implementation.</p></td></tr></table></div><div class="section" title="9.1. Basics"><div class="titlepage"><div><div><h3 class="title"><a name="_basics"></a>9.1. Basics</h3></div></div></div><p>Flume will act as a particular Kerberos principal (user) and needs
credentials.  The Kerberos credentials are needed in order to interact
with the kerberized service.</p><p>There are two ways you can get credentials. The first is used by
interactive users because it requires an interactive logon.  The
second is generally used by services (like a Flume daemon) and uses a
specially protected key table file called a <span class="emphasis"><em>keytab</em></span>.</p><p>Interactively using the <code class="literal">kinit</code> program to contact the Kerberos KDC
(key distribution center) is one way is to prove your identity. This
approach requires a user to enter a password.  To do this you need a
two part principal setup in the KDC, which is generally of the form
<code class="literal">user@REALM.COM</code>.  Logging in via <code class="literal">kinit</code> will grant a ticket granting
ticket (TGT) which can be used to authenticate with other services.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>this user needs to have an account on the namenode machine as
well&#8201;&#8212;&#8201;Hadoop uses this user and group information from that machine
when authorizing access.</p></td></tr></table></div><p>Authenticating a user or a service can alternately be done using a
specially protected <span class="emphasis"><em>keytab</em></span> file.  This file contains a ticket
generating ticket (TGT) which is used to mutually authenticate the
client and the service via the Kerberos KDC.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>The keytab approach is similar to an "password-less" ssh
connections.  In this case instead of an id_rsa private key file, the
service has a keytab entry with its private key.</p></td></tr></table></div><p>Because a Flume node daemon is usually started unattended (via service
script), it needs to login using the keytab approach.  When using a
keytab, the Hadoop services requires a three part principal.  This has
the form <code class="literal">user/host.com@REALM.COM</code>.  We recommend using <code class="literal">flume</code> as the
user and the hostname of the machine as the service.  Assuming that
Kerberos and kerberized Hadoop has been properly setup, you just need
to a few parameters to the Flume node&#8217;s property file
(flume-site.xml).</p><pre class="screen">&lt;property&gt;
&lt;name&gt;flume.kerberos.user&lt;/name&gt;
&lt;value&gt;flume/host1.com@REALM.COM &lt;/value&gt;
&lt;description&gt;&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
&lt;name&gt;flume.kerberos.keytab&lt;/name&gt;
&lt;value&gt;/etc/flume/conf/keytab.krb5 &lt;/value&gt;
&lt;description&gt;&lt;/description&gt;
&lt;/property&gt;</pre><p>In this case, <code class="literal">flume</code> is the user, <code class="literal">host1.com</code> is the service, and
<code class="literal">REALM.COM</code> is the Kerberos realm.  The <code class="literal">/etc/keytab.krb5</code> file contains
the keys necessary for <code class="literal">flume/host1.com@REALM.COM</code> to authenticate
with other services.</p><p>Flume and Hadoop provides a simple keyword (_HOST) that gets expanded
to be the host name of the machine the service is running on.  This
allows you to have one flume-site.xml file with the same
flume.kerberos.user property on all of your machines.</p><pre class="screen">&lt;property&gt;
&lt;name&gt;flume.kerberos.user&lt;/name&gt;
&lt;value&gt;flume/_HOST@REALM.COM &lt;/value&gt;
&lt;description&gt;&lt;/description&gt;
&lt;/property&gt;</pre><p>You can test to see if your Flume node is properly setup by running
the following command.</p><pre class="screen">flume node_nowatch -1 -n dump -c 'dump: console |  collectorSink("hdfs://kerb-nn/user/flume/%Y%m%D-%H/","testkerb");'</pre><p>This should write data entered at the console to a kerberized HDFS
with a namenode named kerb-nn, into a <code class="literal">/user/flume/YYmmDD-HH/</code>
directory.</p><p>If this fails, you many need to check to see if Flume&#8217;s Hadoop
settings (in core-site.xml and hdfs-site.xml) are using Hadoop&#8217;s
settings correctly.</p></div><div class="section" title="9.2. Setting up Flume users on Kerberos"><div class="titlepage"><div><div><h3 class="title"><a name="_setting_up_flume_users_on_kerberos"></a>9.2. Setting up Flume users on Kerberos</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_administering_kerberos_principals">9.2.1. Administering Kerberos principals</a></span></dt></dl></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>These instructions are for MIT Kerb5.</p></td></tr></table></div><p>There are several requirements to have a "properly setup" Kerberos
HDFS + Flume.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Need to have a prinicipal for the Flume user on each machine.
</li><li class="listitem">
Need to have a keytab that has keys for each principal on each machine.
</li></ul></div><p>Much of this setup can be done by using the <code class="literal">kadmin</code> program, and
verified using the <code class="literal">kinit</code>, <code class="literal">kdestroy</code>, and <code class="literal">klist</code> programs.</p><div class="section" title="9.2.1. Administering Kerberos principals"><div class="titlepage"><div><div><h4 class="title"><a name="_administering_kerberos_principals"></a>9.2.1. Administering Kerberos principals</h4></div></div></div><p>First you need to have permissions to use the <code class="literal">kadmin</code> program and the
ability to add to principals to the KDCs.</p><pre class="screen">$ kadmin -p &lt;adminuser&gt; -w &lt;password&gt;</pre><p>If you entered this correctly, it will drop you do the kadmin prompt</p><pre class="screen">kadmin:</pre><p>Here you can add a Flume principal to the KDC</p><pre class="screen">kadmin: addprinc flume
WARNING: no policy specified for flume@REALM.COM; defaulting to no policy
Enter password for principal "flume@REALM.COM":
Re-enter password for principal "flume@REALM.COM":
Principal "flume@REALM.COM" created.
kadmin:</pre><p>You also need to add principals with hosts for each Flume node that
will directly write to HDFS.  Since you will be exporting the key to a
keytab file, you can use the -randkey option to generate a random key.</p><pre class="screen">kadmin: addprinc -randkey flume/host.com
WARNING: no policy specified for flume/host.com@REALM.COM; defaulting to no policy
Principal "flume/host.com@REALM.COM" created.
kadmin:</pre><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Hadoop&#8217;s Kerberos implementation requires a three part principal
name&#8201;&#8212;&#8201;user/host@REALM.COM.  As a user you usually only need the user
name, <a class="ulink" href="mailto:user@REALM.COM" target="_top">user@REALM.COM</a>.</p></td></tr></table></div><p>You can verify that the user has been added by using the <code class="literal">kinit</code>
program, and entering the password you selected.  Next you can verify
that you have your Ticket Granting Ticket (TGT) loaded.</p><pre class="screen">$ kinit flume/host.com
Password for flume/host.com@REALM.COM:
$ klist
Ticket cache: FILE:/tmp/krb5cc_1016
Default principal: flume/host.com@REALM

Valid starting     Expires            Service principal
09/02/10 18:59:38  09/03/10 18:59:38  krbtgt/REALM.COM@REALM.COM


Kerberos 4 ticket cache: /tmp/tkt1016
klist: You have no tickets cached
$</pre><p>You can ignore the Kerberos 4 info.  To "logout" you can use the
<code class="literal">kdestroy</code> command, and then verify that credentials are gone by
running <code class="literal">klist</code>.</p><pre class="screen">$ kdestroy
$ klist
klist: No credentials cache found (ticket cache FILE:/tmp/krb5cc_1016)


Kerberos 4 ticket cache: /tmp/tkt1016
klist: You have no tickets cached
$</pre><p>Next to enable automatic logins, we can create a keytab file so that
does not require manually entering a password.</p><div class="warning" title="Warning" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Warning"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Warning]" src="images/warning.png"></td><th align="left">Warning</th></tr><tr><td align="left" valign="top"><p>This keytab file contains secret credentials that should be
protected so that only the proper user can read the file.  After
created, it should be in 0400 mode (-r--------) and owned by the user
running the Flume process.</p></td></tr></table></div><p>Then you can generate a keytab file (int this example called
<code class="literal">flume.keytab</code>) and add a user <code class="literal">flume/host.com</code> to it.</p><pre class="screen">kadmin: ktadd -k flume.keytab flume/host.com</pre><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>This will invalidate the ability for flume/host.com to manually
login of the account.  You could however have a Flume user does not
use a keytab and that could log in.</p></td></tr></table></div><div class="warning" title="Warning" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Warning"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Warning]" src="images/warning.png"></td><th align="left">Warning</th></tr><tr><td align="left" valign="top"><p><code class="literal">ktadd</code> can add keytab entries for mulitple principals into a
single file and allow for a single keytab file with many keys.  This
however weakens the security stance and may make revoking credentials
from misbehaving machines difficult.  Please consult with your
security administrator when assessing this risk.</p></td></tr></table></div><p>You can verify the names and the version (KVNO) of the keys by running
the following command.</p><pre class="screen">$ klist -Kk flume.keytab
Keytab name: FILE:flume.keytab
KVNO Principal
---- --------------------------------------------------------------------------
   5 flume/host.com@REALM.COM (0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa)
   5 flume/host.com@REALM.COM (0xbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb)
   5 flume/host.com@REALM.COM (0xcccccccccccccccc)
   5 flume/host.com@REALM.COM (0xdddddddddddddddd)</pre><p>You should see a few entries and your corresponding keys in hex after
your principal names.</p><p>Finally, you can use <code class="literal">kinit</code> with the <code class="literal">flume@REALM.COM</code> principal to
interactively do a Kerberos login and use the Hadoop commands to browse HDFS.</p><pre class="screen">$ kinit flume
Password for flume@REALM.COM:  &lt;-- enter password
$ hadoop dfs -ls /user/flume/</pre></div></div></div><div class="section" title="10. Appendix"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_appendix"></a>10. Appendix</h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_flume_source_catalog">10.1. Flume Source Catalog</a></span></dt><dt><span class="section"><a href="#_flume_sinks_catalog">10.2. Flume Sinks Catalog</a></span></dt><dt><span class="section"><a href="#_flume_sink_decorator_catalog">10.3. Flume Sink Decorator Catalog</a></span></dt><dt><span class="section"><a href="#_flume_environment_variables">10.4. Flume Environment Variables</a></span></dt><dt><span class="section"><a href="#_flume_site_xml_configuration_settings">10.5. flume-site.xml configuration settings</a></span></dt><dt><span class="section"><a href="#_troubleshooting">10.6. Troubleshooting</a></span></dt><dd><dl><dt><span class="section"><a href="#_what_are_the_default_ports">10.6.1. What are the default ports?</a></span></dt><dt><span class="section"><a href="#_what_versions_of_hadoop_hdfs_can_i_use_how_do_i_change_this">10.6.2. What versions of Hadoop HDFS can I use?  How do I change this?</a></span></dt><dt><span class="section"><a href="#_why_doesn_8217_t_a_flume_node_appear_on_flume_master">10.6.3. Why doesn&#8217;t a Flume node appear on Flume Master?</a></span></dt><dt><span class="section"><a href="#_why_is_the_state_of_a_flume_node_changing_rapidly">10.6.4. Why is the state of a Flume node changing rapidly?</a></span></dt><dt><span class="section"><a href="#_where_are_the_logs_for_troubleshooting_flume_itself">10.6.5. Where are the logs for troubleshooting Flume itself?</a></span></dt><dt><span class="section"><a href="#_what_can_i_do_if_i_get_node_failure_due_to_out_of_file_handles">10.6.6. What can I do if I get node failure due to out of file handles?</a></span></dt><dt><span class="section"><a href="#_failures_due_when_using_disk_failover_or_write_ahead_log">10.6.7. Failures due when using Disk Failover or Write Ahead Log</a></span></dt><dt><span class="section"><a href="#_can_i_write_data_to_amazon_s3">10.6.8. Can I write data to Amazon S3?</a></span></dt></dl></dd></dl></div><div class="section" title="10.1. Flume Source Catalog"><div class="titlepage"><div><div><h3 class="title"><a name="_flume_source_catalog"></a>10.1. Flume Source Catalog</h3></div></div></div><p title="Flume&#8217;s Tiered Event Sources"><b>Flume&#8217;s Tiered Event Sources. </b>These sources and sinks are actually translated from their original
specification into compositions of more specific lower level configurations.
They generally have reasonable default arguments assigned by the properties
xml file or by the Master.  The defaults can be overridden by the users.</p><div class="horizontal"><table style="border: none;"><colgroup><col><col></colgroup><tbody valign="top"><tr><td style="" valign="top">
<p>
<code class="literal">collectorSource[(<span class="emphasis"><em>port</em></span>)]</code>
</p>
</td><td style="" valign="top">
<p>
Collector source. Listens for data from
agentSinks forwarding to port <code class="literal"><span class="emphasis"><em>port</em></span></code>.  If port is not specified, the
node default collector TCP port, 35863.  This source registers itself
at the Master so that its failover chains can automatically be determined.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">autoCollectorSource</code> 
</p>
</td><td style="" valign="top">
<p>
Auto collector source. Creates a logical collector
that, when assigned to a physical node, will be included in the list of
collectors in a failover chain. This is the collector counterpart to
auto*Chain() sinks. See the section Automatic Failover Chains for additional
information.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">logicalSource</code> 
</p>
</td><td style="" valign="top">
<p>
Logical Source. This source has a port assigned to it by
the Master and listens for rpcSink formatted data.
</p>
</td></tr></tbody></table></div><p title="Flume&#8217;s Basic Sources"><b>Flume&#8217;s Basic Sources. </b>These sources are untranslated and generally need all of their arguments.</p><div class="horizontal"><table style="border: none;"><colgroup><col><col></colgroup><tbody valign="top"><tr><td style="" valign="top">
<p>
<code class="literal">null</code> 
</p>
</td><td style="" valign="top">
<p>
Null source. Opens, closes, and returns null (last record) on next().
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">console</code> 
</p>
</td><td style="" valign="top">
<p>
Stdin console source.  This is for inputting events as an
interactive user and provides features such as edit history and
keyboard edit shortcuts.  A flume node must be started with the <code class="literal">flume
node_nowatch</code>&#8201;&#8212;&#8201;the watchdog does not allow console input.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">stdin</code> 
</p>
</td><td style="" valign="top">
<p>
Stdin source.  This is for piping data into a flume node&#8217;s
standard input data source.  A flume node must be started with the
<code class="literal">flume node_nowatch</code>&#8201;&#8212;&#8201;the watchdog does not allow console input.
WARNING: although this can be used as an interactive console, it will
hang a flume node until a newline is entered.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">rpcSource(<span class="emphasis"><em>port</em></span>)</code> 
</p>
</td><td style="" valign="top">
<p>
A remote procedure call (RPC) server that is
configured to listen on TCP port <code class="literal"><span class="emphasis"><em>port</em></span></code>. Supports both Apache-Thrift and
Apache-Avro RPC framework.  The type of RPC framework is specified by <code class="literal">event.rpc.type</code>
property (THRIFT or AVRO), the default is THRIFT.  Note that same RPC framework is used for rpcSink.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">text("<span class="emphasis"><em>filename</em></span>")</code> 
</p>
</td><td style="" valign="top">
<p>
One-time text file source.  One event per <code class="literal">\n</code>
delimited line.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">tail("<span class="emphasis"><em>filename</em></span>"[, <span class="emphasis"><em>startFromEnd</em></span>=false])</code> 
</p>
</td><td style="" valign="top">
<p>
Similar to Unix&#8217;s tail
utility. One line is one event. Generates events for the entire file
then stays open for more data, and follows filename.  (e.g. if tailing
file "foo" and then "foo" is moved to "bar" and a new file appears
named "foo", it will finish reading the new "bar" file and then start
from the beginning of "foo"). If the <code class="literal">startFromEnd</code> parameter is
false, tail will re-read from the beginning of the file.  If it is
true, it will only start reading from the current end of file.  If the
last line of a file does not end with a newline character (<span class="emphasis"><em>\n</em></span>), the
<code class="literal">tail</code> source will only send an event with this last line when the
<code class="literal">tail</code> is closed.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">multitail("<span class="emphasis"><em>filename</em></span>"[, <span class="emphasis"><em>file2</em></span> [,<span class="emphasis"><em>file3</em></span> &#8230; ] ])</code> 
</p>
</td><td style="" valign="top">
<p>
Like <code class="literal">tail</code> but can
follow multiple files concurrently.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">tailDir("<span class="emphasis"><em>dirname</em></span>"[, fileregex=".*"])</code> 
</p>
</td><td style="" valign="top">
<p>
Tails all files in a
directory <code class="literal">dirname</code> that match the specified <code class="literal">fileregex</code>.  Be careful
and make sure because the regex argument requires java style escaping
of <span class="emphasis"><em>\</em></span> and <span class="emphasis"><em>\"</em></span>.  For example <span class="emphasis"><em>\w+</em></span> would have to be written as
"\\w+".  If a new file appears, it is added to the list of files to
tail.  If pointed at a new directory, it will attempt to read all
files that match!
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">seqfile("<span class="emphasis"><em>filename</em></span>")</code> 
</p>
</td><td style="" valign="top">
<p>
Read from a Hadoop sequence file formatted file,
with <code class="literal">com.cloudera.flume.handlers.hdfs.WriteableEventKey</code> and
<code class="literal">com.cloudera.flume.handlers.hdfs.WriteableEvent</code> values. Conveniently, this
source can read files generated by the seqfile sink.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">syslogUdp(<span class="emphasis"><em>port</em></span>)</code> 
</p>
</td><td style="" valign="top">
<p>
Syslog over UDP <code class="literal"><span class="emphasis"><em>port</em></span></code>.  This is syslog compatible.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">syslogTcp(<span class="emphasis"><em>port</em></span>)</code> 
</p>
</td><td style="" valign="top">
<p>
Syslog over TCP <code class="literal"><span class="emphasis"><em>port</em></span></code>. This is syslog-ng compatible.
This is a server that can listen and receive on many concurrent connections.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">syslogTcp1(<span class="emphasis"><em>port</em></span>)</code> 
</p>
</td><td style="" valign="top">
<p>
Syslog over TCP <code class="literal"><span class="emphasis"><em>port</em></span></code>. This is syslog-ng
compatible.  This is only available for a single connection and then shuts
down afterwards.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">execPeriodic("<span class="emphasis"><em>cmdline</em></span>", <span class="emphasis"><em>ms</em></span>)</code> 
</p>
</td><td style="" valign="top">
<p>
Execute an arbitrary program
specified by <code class="literal"><span class="emphasis"><em>cmdline</em></span></code>.  The entire output of the execution becomes
the body of generated messages.  <code class="literal"><span class="emphasis"><em>ms</em></span></code> specifies the number of
milliseconds to wait before the next execution (and next
event). Ideally the program is short lived.  This does not process
shell pipes or redirection operations&#8201;&#8212;&#8201;for these write a script and
use the script as the <code class="literal"><span class="emphasis"><em>cmdline</em></span></code> argument.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">execStream("<span class="emphasis"><em>cmdline</em></span>")</code> 
</p>
</td><td style="" valign="top">
<p>
Execute an arbitrary program specified by
<code class="literal"><span class="emphasis"><em>cmdline</em></span></code>.  Each line outputted will become a new event.  Ideally
the program is long lived.  This does not process shell pipes or
redirection operations&#8201;&#8212;&#8201;for these write a script and use the script
as the <code class="literal"><span class="emphasis"><em>cmdline</em></span></code> argument.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">exec("<span class="emphasis"><em>cmdline</em></span>"[, <span class="emphasis"><em>aggregate</em></span>=false[, <span class="emphasis"><em>restart</em></span>=false[,<span class="emphasis"><em>period</em></span>=0]]])</code> 
</p>
</td><td style="" valign="top">
<p>
Execute an arbitrary program specified by <code class="literal"><span class="emphasis"><em>cmdline</em></span></code>.  If the <code class="literal"><span class="emphasis"><em>aggregate</em></span></code>
argument is <code class="literal">true</code> entire program output is considered an event; otherwise,
each line is considered a new event.  If the <code class="literal"><span class="emphasis"><em>restart</em></span></code> argument is <code class="literal">true</code>,
then the program is restarted after it exits after waiting for <code class="literal"><span class="emphasis"><em>period</em></span></code>
milliseconds. <code class="literal">execStream("foo")</code> is equivalent to <code class="literal">exec("foo", false, false,
0)</code>. <code class="literal">execPeriodic("foo", 1000)</code> is equivalent to <code class="literal">exec("foo", true, true,
1000)</code>
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">synth(<span class="emphasis"><em>msgCount</em></span>,<span class="emphasis"><em>msgSize</em></span>)</code> 
</p>
</td><td style="" valign="top">
<p>
A source that synthetically generates
<code class="literal"><span class="emphasis"><em>msgCount</em></span></code> random messages of size <code class="literal"><span class="emphasis"><em>msgSize</em></span></code>.  This will generate non
printable characters.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">synthrndsize(<span class="emphasis"><em>msgCount</em></span>,<span class="emphasis"><em>minSize</em></span>,<span class="emphasis"><em>maxSize</em></span>)</code> 
</p>
</td><td style="" valign="top">
<p>
A source that synthetically
generates <code class="literal"><span class="emphasis"><em>msgCount</em></span></code> random messages of size between randomly <span class="emphasis"><em>minSize</em></span> and <span class="emphasis"><em>maxSize</em></span>.
This will generate non printable characters.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">nonlsynth(<span class="emphasis"><em>msgCount</em></span>,<span class="emphasis"><em>msgSize</em></span>)</code>
</p>
</td><td style="" valign="top">
<p>
A source that synthetically generates
<code class="literal"><span class="emphasis"><em>msgCount</em></span></code> random messages of size <code class="literal"><span class="emphasis"><em>msgSize</em></span></code>.  This converts all <code class="literal">'\n'</code>
chars into <code class="literal">' '</code> chars.  This will generate non-printable characters but since
all randomly generated <span class="emphasis"><em>\n</em></span> are converted, sources dependent on <span class="emphasis"><em>\n</em></span> as a
record separator can get uniformly sized data.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">asciisynth(<span class="emphasis"><em>msgCount</em></span>,<span class="emphasis"><em>msgSize</em></span>)</code>
</p>
</td><td style="" valign="top">
<p>
A source that synthetically generates
<code class="literal"><span class="emphasis"><em>msgCount</em></span></code> random messages of size <code class="literal"><span class="emphasis"><em>msgSize</em></span></code>.  This converts all <code class="literal">'\n'</code>
chars into <code class="literal">' '</code> chars, and all non ASCII characters into printable ASCII
characters.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">twitter("<span class="emphasis"><em>username</em></span>","<span class="emphasis"><em>pw</em></span>"[,"<span class="emphasis"><em>url</em></span>"])</code> 
</p>
</td><td style="" valign="top">
<p>
(Unsupported) A source that
collects data from a twitter "spritzer" stream.  <code class="literal"><span class="emphasis"><em>username</em></span></code> is a twitter
username, <code class="literal"><span class="emphasis"><em>pw</em></span></code> is the password for the user, and <code class="literal"><span class="emphasis"><em>url</em></span></code> is the url for the
feed.  If not specified, <code class="literal">http://stream.twitter.com/1/statuses/sample.json</code> is
used by default the <code class="literal"><span class="emphasis"><em>url</em></span></code>. See <a class="ulink" href="http://apiwiki.twitter.com/Streaming-API" target="_top">http://apiwiki.twitter.com/Streaming-API</a>-
Documentation for more details.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">irc("<span class="emphasis"><em>server</em></span>",<span class="emphasis"><em>port</em></span>, "<span class="emphasis"><em>nick</em></span>","<span class="emphasis"><em>chan</em></span>")</code> 
</p>
</td><td style="" valign="top">
<p>
(Unsupported) An IRC channel
source.  Each line sent to the channel is a new event.  It attempts to connect
to <code class="literal"><span class="emphasis"><em>server</em></span></code> on TCP port <code class="literal"><span class="emphasis"><em>port</em></span></code> (standard is 6667).  When it connects it
attempts to take the nickname <code class="literal"><span class="emphasis"><em>nick</em></span></code>, and enter channel <code class="literal"><span class="emphasis"><em>chan</em></span></code> (like
<code class="literal">#hadoop</code>   ).
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">scribe[(+<span class="emphasis"><em>port</em></span></code>)] 
</p>
</td><td style="" valign="top">
<p>
A scribe source.  This provides a network socket that
is compatible with data generated by Facebook&#8217;s Scribe collection system.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">report[(periodMillis)]</code> 
</p>
</td><td style="" valign="top">
<p>
This source polls the local physical node for its
report every <span class="emphasis"><em>periodMillis</em></span> milliseconds and turns it into a new event.  The
attribute names seen from the node report page are present, and the values are
uninterpreted arrays of bytes.
</p>
</td></tr></tbody></table></div></div><div class="section" title="10.2. Flume Sinks Catalog"><div class="titlepage"><div><div><h3 class="title"><a name="_flume_sinks_catalog"></a>10.2. Flume Sinks Catalog</h3></div></div></div><div class="horizontal"><a name="id422467"></a><p class="title"><b>Table 4. Flume&#8217;s Collector Tier Event Sinks</b></p><div class="horizontal-contents"><table summary="Flume&#8217;s Collector Tier Event Sinks" style="border: none;"><colgroup><col><col></colgroup><tbody valign="top"><tr><td style="" valign="top">
<p>
<code class="literal">collectorSink("<span class="emphasis"><em>fsdir</em></span>","<span class="emphasis"><em>fsfileprefix</em></span>", <span class="emphasis"><em>rollmillis</em></span>)</code> 
</p>
</td><td style="" valign="top">
<p>
Collector sink.
<code class="literal"><span class="emphasis"><em>fsdir</em></span></code> is a fs directory URI such as <code class="literal">hdfs://namenode/path</code> or <code class="literal">file:///
path</code>.  <code class="literal"><span class="emphasis"><em>fsfileprefix</em></span></code> is a file name prefix for outputted files. Both of
these can use escape sequences documented to bucket data as documented in the
<span class="strong"><strong>Output Bucketing</strong></span> section. <code class="literal"><span class="emphasis"><em>rollmillis</em></span></code> is the number of milliseconds
between when a HDFS file should be rolled (opened and closed).  The format for
data outputted by collectors is specified by the
<code class="literal">flume.collector.output.format</code> property.
</p>
</td></tr></tbody></table></div></div><br class="horizontal-break"><div class="horizontal"><a name="id422568"></a><p class="title"><b>Table 5. Flume&#8217;s Agent Tier Event Sinks</b></p><div class="horizontal-contents"><table summary="Flume&#8217;s Agent Tier Event Sinks" style="border: none;"><colgroup><col><col></colgroup><tbody valign="top"><tr><td style="" valign="top">
<p>
<code class="literal">agentSink[("<span class="emphasis"><em>machine</em></span>"[, <span class="emphasis"><em>port</em></span>])]</code>  
</p>
</td><td style="" valign="top">
<p>
Defaults to <code class="literal">agentE2ESink</code>
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">agentE2ESink[("<span class="emphasis"><em>machine</em></span>"[, <span class="emphasis"><em>port</em></span>])]</code> 
</p>
</td><td style="" valign="top">
<p>
Agent sink with write ahead log and
end-to-end ack.  Optional arguments specify a <code class="literal"><span class="emphasis"><em>machine</em></span></code>, and the TCP
<code class="literal"><span class="emphasis"><em>port</em></span></code> pointing to a <code class="literal">collectorSource</code>.  If none is specified, the values
specified by the <code class="literal">flume.collector.event.host</code> and the <code class="literal">flume.collector.port</code>
properties will be used.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">agentDFOSink[("<span class="emphasis"><em>machine</em></span>"[, <span class="emphasis"><em>port</em></span>])]</code> 
</p>
</td><td style="" valign="top">
<p>
DiskFailover Agent sink that stores
to local disk on detected failure. This sink periodically checks with the
<code class="literal"><span class="emphasis"><em>machine:port</em></span></code> and resends events if becomes alive again. Optional arguments
specify a <code class="literal"><span class="emphasis"><em>machine</em></span></code>, and the TCP <code class="literal"><span class="emphasis"><em>port</em></span></code> pointing to a <code class="literal">collectorSource</code>.
If none is specified, the values specified by the <code class="literal">flume.collector.event.host</code>
and the <code class="literal">flume.collector.port</code> properties will be used.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">agentBESink[("<span class="emphasis"><em>machine</em></span>"[, <span class="emphasis"><em>port</em></span>])]</code> 
</p>
</td><td style="" valign="top">
<p>
BestEffort Agent sink.  This drops
messages on failures and continues sending.  Optional arguments specify a
<code class="literal"><span class="emphasis"><em>collector</em></span></code>, and the TCP <code class="literal"><span class="emphasis"><em>PORT</em></span></code> pointing to a <code class="literal">collectorSource</code>.  If none
is specified, the values specified by the <code class="literal">flume.collector.event.host</code> and the
<code class="literal">flume.collector.port</code> properties will be used.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">agentE2EChain("<span class="emphasis"><em>m1</em></span>[:_p1_]"[, "<span class="emphasis"><em>m2</em></span>[:_p2_]"[,&#8230;]])</code> 
</p>
</td><td style="" valign="top">
<p>
Agent sink
with write-ahead log and end-to-end ack and collector failover
chains. <code class="literal"><span class="emphasis"><em>m1:p1</em></span></code> specifies a machine and optional port of the primary
default collector.  If all failovers are exhausted due to failures,
and since data is already durable locally, it will back off attempts
to send down stream.  Optional arguments specify a list of failover
machine:port pairs in a ranked order.  If a primary collector is not
responding, the backups are used.  The primary collectors are checked
periodically to see if they have come back up.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">agentDFOChain("<span class="emphasis"><em>m1</em></span>[:_p1_]"[, "<span class="emphasis"><em>m2</em></span>[:_p2_]"[,&#8230;]])</code> 
</p>
</td><td style="" valign="top">
<p>
DiskFailover
Agent sink that first attempts to fail over to other
collectors. <code class="literal"><span class="emphasis"><em>m1:p1</em></span></code> specifies a machine and optional port of the
primary default collector.  If all failovers are exhausted due to
failures, it will store to local disk.  Optional arguments specify a
list of failover machine:port pairs in a ranked order.  If a primary
collector is not responding, the backups are used.  The primary
collectors are checked periodically to see if they have come back up.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">agentBEChain("<span class="emphasis"><em>m1</em></span>[:_p1_]"[, "<span class="emphasis"><em>m2</em></span>[:_p2_]"[,&#8230;]])</code> 
</p>
</td><td style="" valign="top">
<p>
BestEffort
Agent sink with collector failover chains.  <code class="literal"><span class="emphasis"><em>m1:p1</em></span></code> specifies a
machine and optional port of the primary default collector.  If all
failovers are exhausted due to failures, this drops messages. Optional
arguments specify a <code class="literal"><span class="emphasis"><em>collector</em></span></code>, and the TCP <code class="literal"><span class="emphasis"><em>port</em></span></code> of the
collector.  If none is specified, the values specified by the
<code class="literal">flume.collector.event.host</code> and the <code class="literal">flume.collector.port</code> properties
will be used.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">autoE2EChain</code> 
</p>
</td><td style="" valign="top">
<p>
This sink is an <code class="literal">agentE2EChain</code> that has failover nodes
populated automatically by the master.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">autoDFOChain</code> 
</p>
</td><td style="" valign="top">
<p>
This sink is an <code class="literal">agentDFOChain</code> that has failover nodes
populated automatically by the master.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">autoBEChain</code> 
</p>
</td><td style="" valign="top">
<p>
This sink is an <code class="literal">agentBEChain</code> that has failover nodes
populated automatically by the master.
</p>
</td></tr></tbody></table></div></div><br class="horizontal-break"><div class="variablelist" title="Flume&#8217;s Logical Sinks"><p class="title"><b>Flume&#8217;s Logical Sinks</b></p><dl><dt><span class="term">
<code class="literal">logicalSink("<span class="emphasis"><em>logicalnode</em></span>")</code> 
</span></dt><dd>
This sink creates an rpcSink that is
assigned a host and IP based on the name of a logical node.  This information
is maintained and automatically selected by the master.
</dd></dl></div><div class="horizontal"><a name="id423066"></a><p class="title"><b>Table 6. Flume&#8217;s Basic Sinks</b></p><div class="horizontal-contents"><table summary="Flume&#8217;s Basic Sinks" style="border: none;"><colgroup><col><col></colgroup><tbody valign="top"><tr><td style="" valign="top">
<p>
<code class="literal">null</code> 
</p>
</td><td style="" valign="top">
<p>
Null sink. Events are dropped
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">console[("<span class="emphasis"><em>formatter</em></span>")]</code> 
</p>
</td><td style="" valign="top">
<p>
Console sink.  Display events to process&#8217;s
stdout using the optionally specified output formatter.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">text("<span class="emphasis"><em>txtfile</em></span>"[,"<span class="emphasis"><em>formatter</em></span>"])</code> 
</p>
</td><td style="" valign="top">
<p>
Textfile sink.  Write to text file
txtfile, using an optionally specified <code class="literal"><span class="emphasis"><em>formatter</em></span></code>.  If a file already
exists, this sink will attempt to overwrite it.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">seqfile("filename")</code> 
</p>
</td><td style="" valign="top">
<p>
Seqfile sink.  Write to a Hadoop sequence file
formatted file, with <code class="literal">com.cloudera.flume.handlers.hdfs.WriteableEventKey</code> keys
and <code class="literal">com.cloudera.flume.handlers.hdfs.WriteableEvent</code> values.  If a file
already exists, this sink will attempt to overwrite it.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">dfs("<span class="emphasis"><em>hdfspath</em></span>")</code> 
</p>
</td><td style="" valign="top">
<p>
Hadoop dfs seqfile sink.  Write to a dfs path
in Flume- specific Hadoop seqfile record format. The <code class="literal"><span class="emphasis"><em>hdfspath</em></span></code> can
use escape sequences to bucket data as documented in the <span class="strong"><strong>Output
Bucketing</strong></span> section.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">customdfs("<span class="emphasis"><em>hdfspath</em></span>"[, "<span class="emphasis"><em>format</em></span>"])</code> 
</p>
</td><td style="" valign="top">
<p>
Hadoop dfs formatted file
sink.  The <span class="emphasis"><em>hdfspath</em></span> string is <span class="strong"><strong>not</strong></span> escaped. The output format of
writes to a dfs path in using specified output <span class="emphasis"><em>format</em></span>.
</p>
</td></tr><tr><td style="" valign="top">
<p>
+escapedCustomDfs("<span class="emphasis"><em>hdfspath</em></span>", "<span class="emphasis"><em>file</em></span>", "<span class="emphasis"><em>format</em></span>") 
</p>
</td><td style="" valign="top">
<p>
Hadoop dfs formatted
file sink.  The <span class="emphasis"><em>hdfspath</em></span> string is escaped and events will get written to
particular directories and filenames based on this string. The output format
of writes to a dfs path in using specified output <span class="emphasis"><em>format</em></span>. The <code class="literal"><span class="emphasis"><em>hdfspath</em></span></code>
can use escape sequences documented to bucket data as documented in the
<span class="strong"><strong>Output Bucketing</strong></span> section.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">rpcSink("<span class="emphasis"><em>host</em></span>"[, <span class="emphasis"><em>port</em></span>])</code> 
</p>
</td><td style="" valign="top">
<p>
A remote procedure call (RPC) sink that is
configured to send to machine <code class="literal"><span class="emphasis"><em>host</em></span></code> on TCP port <code class="literal"><span class="emphasis"><em>port</em></span></code>. Default port is 35861
and can be overridden by setting the <code class="literal">flume.collector.event.port</code> property.
Supports both Apache-Thrift and Apache-Avro RPC framework.
The type of RPC framework is specified by <code class="literal">event.rpc.type</code> property (THRIFT or AVRO),
the default is THRIFT.  Note that same RPC framework is used for rpcSource.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">syslogTcp("<span class="emphasis"><em>host</em></span>"[,<span class="emphasis"><em>port</em></span>])</code> 
</p>
</td><td style="" valign="top">
<p>
Syslog TCP sink.  Write to host "host" on
port "port" in syslog over TCP format (syslog-ng compatible). Default port is
TCP 514.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">irc("<span class="emphasis"><em>host</em></span>",<span class="emphasis"><em>port</em></span>, "<span class="emphasis"><em>nick</em></span>", "<span class="emphasis"><em>chan</em></span>")</code> 
</p>
</td><td style="" valign="top">
<p>
(Unsupported) An IRC channel
sink.  Each event is sent to the channel as a line.  It attempts to connect to
<code class="literal"><span class="emphasis"><em>server</em></span></code> on TCP port <code class="literal"><span class="emphasis"><em>port</em></span></code>.  When it connects it attempts to take the
nickname <code class="literal"><span class="emphasis"><em>nick</em></span></code>, and enter channel <code class="literal"><span class="emphasis"><em>chan</em></span></code> (like <code class="literal">#hadoop</code>).
</p>
</td></tr></tbody></table></div></div><br class="horizontal-break"></div><div class="section" title="10.3. Flume Sink Decorator Catalog"><div class="titlepage"><div><div><h3 class="title"><a name="_flume_sink_decorator_catalog"></a>10.3. Flume Sink Decorator Catalog</h3></div></div></div><div class="horizontal"><a name="id423502"></a><p class="title"><b>Table 7. Flume&#8217;s Sink Decorators</b></p><div class="horizontal-contents"><table summary="Flume&#8217;s Sink Decorators" style="border: none;"><colgroup><col><col></colgroup><tbody valign="top"><tr><td style="" valign="top">
<p>
<code class="literal">nullDeco</code> 
</p>
</td><td style="" valign="top">
<p>
This is a decorator that just passes data through to its child sink.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">writeAhead(&#8230;)</code> 
</p>
</td><td style="" valign="top">
<p>
Write-ahead decorator.  Provides durability by writing
events to disk before sending them.  This can be used as a buffering mechanism&#8201;&#8212;&#8201;receive and send are decoupled in different threads.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">ackedWriteAhead[(<span class="emphasis"><em>maxmillis</em></span>)]</code> 
</p>
</td><td style="" valign="top">
<p>
Write-ahead decorator that adds
acknowledgement tags and checksums to events.  Provides durability by writing
events to disk before sending them.  This can be used as a buffering mechanism&#8201;&#8212;&#8201;receive and send are decoupled in different threads.  This generates and
tracks groups of events, and also notifies other components to check for
acknowledgements.  These checks for retries are done where there is an
exponential backoff that can top out at <span class="emphasis"><em>maxmillis</em></span> milliseconds.  The default
value for <span class="emphasis"><em>maxmillis</em></span> is <code class="literal">flume.agent.logdir.maxage</code> property.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">diskFailover[(<span class="emphasis"><em>maxmillis</em></span>)]</code> 
</p>
</td><td style="" valign="top">
<p>
Disk failover decorator.  Events that enter
this decorator are sent to its sub sink.  In the event of a down stream error,
data is written to disk, and periodically these disk-buffered events are
retried.  These checks for retries are done where there is an exponential
backoff that can top out at <span class="emphasis"><em>maxmillis</em></span> milliseconds.  The default value for
<span class="emphasis"><em>maxmillis</em></span> is <code class="literal">flume.agent.logdir.maxage</code> property.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">ackInjector</code> 
</p>
</td><td style="" valign="top">
<p>
This decorator injects an extra ack group start message on
open, tags appended events with an ack tag, and injects an extra ack group end
message.  These tags contain a checksum, for all the bodies of the events that
pass through the <code class="literal">ackInjector</code>.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">ackChecker</code> 
</p>
</td><td style="" valign="top">
<p>
This decorator tracks ack group start, end, and checksum
values inserted by <code class="literal">ackInjector</code>.  If a group has arrived and its checksum is
correct, it sends notifications to other components.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">lazyOpen</code> 
</p>
</td><td style="" valign="top">
<p>
This decorator tracks open/closed state of the sub sink but does
not actually open the sink until an append is called.  Thus if a this
decorator is opened and closed without any appends, the sub sink is never
opened.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">insistentOpen[(<span class="emphasis"><em>max</em></span>[<span class="emphasis"><em>init</em></span>[,<span class="emphasis"><em>cumulativeMax</em></span>]],)]</code> 
</p>
</td><td style="" valign="top">
<p>
An insistent open
attempts to open its subsink multiple times until it succeeds with the
specified backoff properties.  This is useful for starting a network client up
when the network server may not yet be up.  When an attempt to open the
subsink fails, this exponentially backs off and then retries the open.  <span class="emphasis"><em>max</em></span>
is the max number of millis per backoff (default is Integer.MAX_VALUE).
<span class="emphasis"><em>init</em></span> is the initial number of millis to back off on the first encountered
failure (default 1000). <span class="emphasis"><em>cumulativeMax</em></span> is the maximum backoff allowed from a
single failure before an exception is forwarded (default is
Integer.MAX_VALUE).  Note that this synchronously blocks the open call until
the open succeeds or fails after <span class="emphasis"><em>cumulativeMax</em></span> millis.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">stubbornAppend</code> 
</p>
</td><td style="" valign="top">
<p>
A stubborn append normally passes through append
operations to its subsink.  It catches the first exception that a subsink&#8217;s
append method triggers, and then closes, opens, and reappends to the subsink.
If this second attempt fails, it throws an exception. This is useful in
conjunction with network sinks where connections can be broken.  The open/
close retry attempt is often sufficient to re-establish the connection.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">value("<span class="emphasis"><em>attr</em></span>","<span class="emphasis"><em>value</em></span>")</code> 
</p>
</td><td style="" valign="top">
<p>
The value decorator adds a new metadata
attribute <span class="emphasis"><em>attr</em></span> with the value <span class="emphasis"><em>value</em></span>.  Agents can mark their data with
specific tags for later demultiplexing.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">mask("<span class="emphasis"><em>attr1</em></span>"[,"<span class="emphasis"><em>attr2</em></span>", &#8230;])</code> 
</p>
</td><td style="" valign="top">
<p>
The mask decorator outputs inputted
events that are modified so that all metadata <span class="strong"><strong>except</strong></span> the attributes
specified pass through.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">select("<span class="emphasis"><em>attr1</em></span>"[,"<span class="emphasis"><em>attr2</em></span>", &#8230;])</code> 
</p>
</td><td style="" valign="top">
<p>
The select decorator outputs inputted
events that are modified so that <span class="strong"><strong>only</strong></span> the metadata attributes specified pass
through.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">format("<span class="emphasis"><em>pattern</em></span>")</code> 
</p>
</td><td style="" valign="top">
<p>
The format decorator outputs inputted events that are
modified so that their bodies are replaced with an escaped version of the
<code class="literal"><span class="emphasis"><em>pattern</em></span></code> argument.  Since checksums rely on body data, this should only be
used on unreliable flows or reporting flows. Inappropriate use may result in
message loss.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">regex("<span class="emphasis"><em>regex</em></span>",idx,"<span class="emphasis"><em>attr</em></span>")</code> 
</p>
</td><td style="" valign="top">
<p>
The regex decorator applies the regular
expression <span class="emphasis"><em>regex</em></span>, extracts the <span class="emphasis"><em>idx</em></span> th capture group, and writes this value
to the <span class="emphasis"><em>attr</em></span> attribute.  The regex must use java-style escaping.  Thus a
regexs that want to use the <code class="literal">\d</code> macro need to be specified as "\\d".
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">split("<span class="emphasis"><em>regex</em></span>",idx,"<span class="emphasis"><em>attr</em></span>")</code> 
</p>
</td><td style="" valign="top">
<p>
The split decorator uses the
regular expression <span class="emphasis"><em>regex</em></span> to split the body into tokens (not
including the splitter value). The <span class="emphasis"><em>idx</em></span> is then written as the value
to the <span class="emphasis"><em>attr</em></span> attribute.  The regex must use java-style escaping.
Thus, a regex that wants to use the <code class="literal">\d</code> macro must be specified as
"\\d".
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">batch(<span class="emphasis"><em>n</em></span>,<span class="emphasis"><em>maxlatency</em></span>)</code> 
</p>
</td><td style="" valign="top">
<p>
buffers <span class="emphasis"><em>n</em></span> events and then sends one
aggregate event.  If <span class="emphasis"><em>maxlatency</em></span> millis have passed, all current
buffered events are sent out as an aggregate event.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">unbatch</code> 
</p>
</td><td style="" valign="top">
<p>
Unbatch takes an aggregate event generated by batch, splits it,
and then forwards its original events.  If an event is not an aggregate it is
just forwarded.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">gzip</code> 
</p>
</td><td style="" valign="top">
<p>
gzips a serialized event.  This is useful when used in conjunction
with aggregate events.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">gunzip</code> 
</p>
</td><td style="" valign="top">
<p>
gunzip&#8217;s a gzip&#8217;ed event.  If the event is not a gzip event, it is
just forwarded.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">intervalSampler(<span class="emphasis"><em>n</em></span>)</code> 
</p>
</td><td style="" valign="top">
<p>
Interval sampler. Every <code class="literal"><span class="emphasis"><em>n</em></span></code> th event gets
forwarded.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">probSampler(<span class="emphasis"><em>p</em></span>)</code> 
</p>
</td><td style="" valign="top">
<p>
Probability sampler. Every event has a probability <span class="emphasis"><em>p</em></span>
(where 0.0 &#8804; <span class="emphasis"><em>p</em></span> &#8804; 1.0) chance of being forwarded.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">reservoirSampler(<span class="emphasis"><em>k</em></span>)</code> 
</p>
</td><td style="" valign="top">
<p>
Reservoir sampler.  When flushed, at most <span class="emphasis"><em>k</em></span>
events are forwarded.  If more than <span class="emphasis"><em>k</em></span> elements have entered this decorator,
exactly <span class="emphasis"><em>k</em></span> events are forwarded.  All events that pass through have the same
probability of being selected.  NOTE: This will reorder the events being sent.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">delay(<span class="emphasis"><em>ms</em></span>)</code> 
</p>
</td><td style="" valign="top">
<p>
adds a <span class="emphasis"><em>ms</em></span> millisecond delay before forwarding events down
the pipeline.  This blocks and prevents other events from entering the
pipeline.  This is useful for workload simulation in conjunction with
<code class="literal">asciisynth</code> sources.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">choke[(<span class="emphasis"><em>choke-id</em></span>)]</code> 
</p>
</td><td style="" valign="top">
<p>
Limits the transfer rate of data going into the sink.
The <code class="literal">choke-id</code> should have been registered on the physical node where this decorator is being created using the <code class="literal">setChokeLimit</code> command.
Refer to <span class="strong"><strong>Limiting Data Transfer Rate between Source-Sink pairs</strong></span> section for more details.
</p>
</td></tr></tbody></table></div></div><br class="horizontal-break"></div><div class="section" title="10.4. Flume Environment Variables"><div class="titlepage"><div><div><h3 class="title"><a name="_flume_environment_variables"></a>10.4. Flume Environment Variables</h3></div></div></div><p>This section describes several environment variables that affect how
Flume operates.  The <code class="literal">flume</code> script in <code class="literal">./bin</code> uses these system
environment variables.  Many of these variables are set by the
<code class="literal">flume-daemon.sh</code> script used when flume is run as a daemon.</p><div class="horizontal"><table style="border: none;"><colgroup><col><col></colgroup><tbody valign="top"><tr><td style="" valign="top">
<p>
<code class="literal">FLUME_PID_DIR</code> 
</p>
</td><td style="" valign="top">
<p>
The directory where a flume node or flume
master will drop pid files corresponding to the daemon process.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">FLUME_CLASSPATH</code> 
</p>
</td><td style="" valign="top">
<p>
The custom java classpath environment variable
additions you want flume to run with.  These values are prepended to
the normal Flume generated CLASSPATH.  WARNING: The <code class="literal">flume</code> script
overwrites the standard CLASSPATH when it is executed.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">FLUME_LOG_DIR</code> 
</p>
</td><td style="" valign="top">
<p>
The directory where debugging logs
generated by the flume node or flume master are written.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">FLUME_LOGFILE</code> 
</p>
</td><td style="" valign="top">
<p>
This sets the suffix for the logfiles generated by
flume node or flume master.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">FLUME_ROOT_LOGGER</code> 
</p>
</td><td style="" valign="top">
<p>
The log4j logging setting for the executing
command.  By default it is "INFO,console".
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">ZOOKEEPER_ROOT_LOGGER</code> 
</p>
</td><td style="" valign="top">
<p>
The log4j logging setting for a master&#8217;s
embedded zookeeper server&#8217;s logs.  By default it is "ERROR,console".
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">WATCHDOOG_ROOT_LOGGER</code> 
</p>
</td><td style="" valign="top">
<p>
The log4j logging setting for the logging
the watchdog that wraps flume nodes and flume masters generates.  By
default it is "INFO,console".
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">FLUME_CONF_DIR</code> 
</p>
</td><td style="" valign="top">
<p>
The directory where the <code class="literal">flume-site.xml</code> and
<code class="literal">flume-conf.xml</code> files flume node and flume master will use reside.
This defaults to <code class="literal">./conf</code> if a <code class="literal">./conf/flume-conf.xml</code> file is found,
or to <code class="literal">/etc/flume/conf/</code> if it <code class="literal">flume-conf.xml</code> is found there.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">HADOOP_HOME</code> 
</p>
</td><td style="" valign="top">
<p>
The directory where Hadoop jars are expected to be
found.  If not specified it will use jars found in <code class="literal">/usr/lib/hadoop</code>
or <code class="literal">./lib/</code>.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">FLUME_DEVMODE</code> 
</p>
</td><td style="" valign="top">
<p>
If this value is set to "true" the <code class="literal">./libbuild</code>
jars which include ant jars required to compile JSP servlets will be
included in the CLASSPATH.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">FLUME_VERBOSE</code> 
</p>
</td><td style="" valign="top">
<p>
If it this is toggled, the flume script will print
out the command line being executed.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">FLUME_VERBOSE_JAVA</code> 
</p>
</td><td style="" valign="top">
<p>
If this is toggled along with FLUME_VERBOSE,
the "-verbose" flag will be passed to the JVM running flume.
</p>
</td></tr></tbody></table></div></div><div class="section" title="10.5. flume-site.xml configuration settings"><div class="titlepage"><div><div><h3 class="title"><a name="_flume_site_xml_configuration_settings"></a>10.5. flume-site.xml configuration settings</h3></div></div></div></div><div class="section" title="10.6. Troubleshooting"><div class="titlepage"><div><div><h3 class="title"><a name="_troubleshooting"></a>10.6. Troubleshooting</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_what_are_the_default_ports">10.6.1. What are the default ports?</a></span></dt><dt><span class="section"><a href="#_what_versions_of_hadoop_hdfs_can_i_use_how_do_i_change_this">10.6.2. What versions of Hadoop HDFS can I use?  How do I change this?</a></span></dt><dt><span class="section"><a href="#_why_doesn_8217_t_a_flume_node_appear_on_flume_master">10.6.3. Why doesn&#8217;t a Flume node appear on Flume Master?</a></span></dt><dt><span class="section"><a href="#_why_is_the_state_of_a_flume_node_changing_rapidly">10.6.4. Why is the state of a Flume node changing rapidly?</a></span></dt><dt><span class="section"><a href="#_where_are_the_logs_for_troubleshooting_flume_itself">10.6.5. Where are the logs for troubleshooting Flume itself?</a></span></dt><dt><span class="section"><a href="#_what_can_i_do_if_i_get_node_failure_due_to_out_of_file_handles">10.6.6. What can I do if I get node failure due to out of file handles?</a></span></dt><dt><span class="section"><a href="#_failures_due_when_using_disk_failover_or_write_ahead_log">10.6.7. Failures due when using Disk Failover or Write Ahead Log</a></span></dt><dt><span class="section"><a href="#_can_i_write_data_to_amazon_s3">10.6.8. Can I write data to Amazon S3?</a></span></dt></dl></div><div class="section" title="10.6.1. What are the default ports?"><div class="titlepage"><div><div><h4 class="title"><a name="_what_are_the_default_ports"></a>10.6.1. What are the default ports?</h4></div></div></div><p>TCP ports are used in all situations.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; " width="40%"><colgroup><col><col><col></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>node collector port</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p><code class="literal">flume.collector.port</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>35853+</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>node status web server</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p><code class="literal">flume.node.http.port</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>35862+</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>master status web server</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p><code class="literal">flume.master.http.port</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>35871</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>master heartbeat port</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p><code class="literal">flume.master.heartbeat.port</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>35872</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>master admin/shell port</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p><code class="literal">flume.master.admin.port</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>35873</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>master gossip port</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p><code class="literal">flume.master.gossip.port</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>35890</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>master &#8594; zk port</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p><code class="literal">flume.master.zk.client.port</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>3181</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>zk &#8594; zk quorum port</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p><code class="literal">flume.master.zk.server.quorum.port</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>3182</p></td></tr><tr><td style="border-right: 0.5pt solid ; " align="left" valign="top"><p>zk &#8594; zk election port</p></td><td style="border-right: 0.5pt solid ; " align="left" valign="top"><p><code class="literal">flume.master.zk.server.election.port</code></p></td><td style="" align="left" valign="top"><p>3183</p></td></tr></tbody></table></div></div><div class="section" title="10.6.2. What versions of Hadoop HDFS can I use? How do I change this?"><div class="titlepage"><div><div><h4 class="title"><a name="_what_versions_of_hadoop_hdfs_can_i_use_how_do_i_change_this"></a>10.6.2. What versions of Hadoop HDFS can I use?  How do I change this?</h4></div></div></div><p>Currently, there are constraints writing to HDFS.  A Flume node
can only write to one version of Hadoop.  Although Hadoop&#8217;s HDFS API has been
fairly stable, HDFS clients are only guaranteed to be wire compatible with the
same major version of HDFS.  Cloudera&#8217;s testing used Hadoop HDFS 0.20.x and HDFS
0.18.x.  They are API compatible so all that is necessary to switch versions
is to swap out the Hadoop jar and restart the node that will write to the
other Hadoop version.</p><p>You still need to configure this instance of Hadoop so that it talks to the
correct HDFS nameNode.  You configure the Hadoop client settings (such as
pointers to the name node) the same way as with Hadoop dataNodes or worker
nodes&#8201;&#8212;&#8201;modify and use <code class="literal">conf/core-site.xml</code>.</p><p>By default, Flume checks for a Hadoop jar in <code class="literal">/usr/lib/hadoop</code>.  If it is not
present, it defaults to a jar found in its own lib directory, <code class="literal">/usr/lib/flume/
lib</code>.</p></div><div class="section" title="10.6.3. Why doesn&#8217;t a Flume node appear on Flume Master?"><div class="titlepage"><div><div><h4 class="title"><a name="_why_doesn_8217_t_a_flume_node_appear_on_flume_master"></a>10.6.3. Why doesn&#8217;t a Flume node appear on Flume Master?</h4></div></div></div><p>If a node does not show up on the Master, you should first check to see if the
node is running.</p><pre class="screen"># jps | grep FlumeNode</pre><p>You can also check logs found in <code class="literal">/usr/lib/flume/logs/flume-flume-node*.log</code>.</p><p>Common errors are error messages or warnings about being unable to contact
the masters.  This could be due to host misconfiguration, port
misconfiguration (35872 is the default heartbeat port), or fire walling
problems.</p><p>Another possible error is to have a permissions problems with the local
machine&#8217;s write-ahead log directory.  On an out-of-the-box setup, this is in
the <code class="literal">/tmp/flume/agent</code> directory).  If a Flume Node is ever run as a user
other than <code class="literal">flume</code>, (especially if it was run as <code class="literal">root</code>), the directory needs
to be either deleted or its contents must have its permissions modified to
allow Flume to use it.</p></div><div class="section" title="10.6.4. Why is the state of a Flume node changing rapidly?"><div class="titlepage"><div><div><h4 class="title"><a name="_why_is_the_state_of_a_flume_node_changing_rapidly"></a>10.6.4. Why is the state of a Flume node changing rapidly?</h4></div></div></div><p>Nodes by default start and choose their <code class="literal">hostname</code> as their physical node
name.  Physical nodes names are supposed to be unique.  Unexpected results may
occur if multiple physical nodes are assigned the same name.</p></div><div class="section" title="10.6.5. Where are the logs for troubleshooting Flume itself?"><div class="titlepage"><div><div><h4 class="title"><a name="_where_are_the_logs_for_troubleshooting_flume_itself"></a>10.6.5. Where are the logs for troubleshooting Flume itself?</h4></div></div></div><p>On Ubuntu-based installations, logs are written to <code class="literal">/usr/lib/logs/</code>.</p><div class="variablelist"><dl><dt><span class="term">
Master logs   
</span></dt><dd>
<code class="literal">/usr/lib/logs/flume-flume-master-<span class="emphasis"><em>host</em></span>.log</code>
</dd><dt><span class="term">
Master stdout 
</span></dt><dd>
<code class="literal">/usr/lib/logs/flume-flume-master-<span class="emphasis"><em>host</em></span>.out.*</code>
</dd><dt><span class="term">
Node logs 
</span></dt><dd>
<code class="literal">/usr/lib/ logs/flume-flume-node-<span class="emphasis"><em>host</em></span>.log</code>
</dd><dt><span class="term">
Node stdout 
</span></dt><dd>
<code class="literal">/usr/lib/logs/flume-flume- node-<span class="emphasis"><em>host</em></span>.out.*</code>
</dd></dl></div></div><div class="section" title="10.6.6. What can I do if I get node failure due to out of file handles?"><div class="titlepage"><div><div><h4 class="title"><a name="_what_can_i_do_if_i_get_node_failure_due_to_out_of_file_handles"></a>10.6.6. What can I do if I get node failure due to out of file handles?</h4></div></div></div><p>There are two limits in Linux&#8201;&#8212;&#8201;the max number of allowable open files
(328838), and max number of allowable open files for a user (default 1024).
Sockets are file handles so this limits the number of open TCP connections
available.</p><p>On Ubuntu, need to add a line to <code class="literal">/etc/security/limits.conf</code></p><pre class="screen">&lt;user&gt; hard nofile 10000</pre><p>The user should also add the following line to a <code class="literal">~/.bash_profile</code> to raise
the limit to the hard limit.</p><pre class="screen">ulimit -n 10000</pre></div><div class="section" title="10.6.7. Failures due when using Disk Failover or Write Ahead Log"><div class="titlepage"><div><div><h4 class="title"><a name="_failures_due_when_using_disk_failover_or_write_ahead_log"></a>10.6.7. Failures due when using Disk Failover or Write Ahead Log</h4></div></div></div><p>Flume currently relies on the file system for logging mechanisms. You must
make sure that the user running Flume has permissions to write to the
specified logging directory.</p><p>Currently the default is to write to /tmp/flume.  In a production system you
should write to a directory that does not automatically get deleted on reboot.</p></div><div class="section" title="10.6.8. Can I write data to Amazon S3?"><div class="titlepage"><div><div><h4 class="title"><a name="_can_i_write_data_to_amazon_s3"></a>10.6.8. Can I write data to Amazon S3?</h4></div></div></div><p>Yes.  In the collector sink or dfs sinks you can use the <code class="literal">s3n://</code> or <code class="literal">s3://</code>
prefixes to write to S3 buckets.</p><p>First you must add some jars to your CLASSPATH in order to support s3 writing.
Here is a set that Cloudera has tested (other versions will likely work as well):</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
commons-codec-1.3.jar
</li><li class="listitem">
commons-httpclient-3.0.1.jar
</li><li class="listitem">
jets3t-0.6.1.jar
</li></ul></div><p><code class="literal">s3n</code> uses the native s3 file system and has some limitations on individual
file sizes.  Files written with this method are compatible with other programs
like <code class="literal">s3cmd</code>.</p><p><code class="literal">s3</code> writes using an overlay file system must go through Hadoop for file
system interaction.</p></div></div></div><div class="glossary" title="Glossary"><div class="titlepage"><div><div><h2 class="title"><a name="_glossary"></a>Glossary</h2></div></div></div><div class="variablelist"><dl><dt><span class="term">
Agent
</span></dt><dd>
A Flume node located at the start of a flow that captures data from
external sources, ready for feeding downstream.
</dd><dt><span class="term">
Collector
</span></dt><dd>
A Flume node located at the end of a flow that delivers data to its eventual destination.
</dd><dt><span class="term">
Flow
</span></dt><dd>
A set of nodes wired together in sequence which together process data
from a single source into its eventual destination.
</dd><dt><span class="term">
Master
</span></dt><dd>
A service that controls the configuration of all nodes, and to
which all nodes report.
</dd><dt><span class="term">
Sink
</span></dt><dd>
The place where a node sends its data after all processing is done.
</dd><dt><span class="term">
Source
</span></dt><dd>
The place where a node gets its data stream.
</dd></dl></div></div><div class="section" title="11. Versions"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_versions"></a>11. Versions</h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_history">11.1. history</a></span></dt></dl></div><div class="section" title="11.1. history"><div class="titlepage"><div><div><h3 class="title"><a name="_history"></a>11.1. history</h3></div></div></div><div class="variablelist"><dl><dt><span class="term">
v0.9.2 11/15/10
</span></dt><dd>
Support for all hadoop supported compression codecs.
Avro RPC support.  Flexible throughput thottling.  Useful error
messsages in shell. Upgrades to Thrift RPC support.  Performance and
robustness improvements in tail and exec.
</dd><dt><span class="term">
v0.9.1u1 10/12/10
</span></dt><dd>
Added write support for Kerberized HDFS. Flume
cookbook.
</dd><dt><span class="term">
v0.9.1 8/9/10
</span></dt><dd>
Improved error messages and visibility of property
configuration values.  First external contributions.  Fixed
reconfiguration hangs.  Improved implementing plugins documentation.
Updated scribe and syslog support.  Compression on output files.
</dd><dt><span class="term">
v0.9 6/29/10 
</span></dt><dd>
metrics and reporting framework, logical nodes+logical
names abstraction, wal/dfo isolation by flow, transformation-based
high level sinks.  Open source and initial push to github.
</dd><dt><span class="term">
v0.3 3/31/10 
</span></dt><dd>
ZK based master/multi-master, automatic failovers for
data and control planes.  flume shell. deb/rpm packaging.
</dd><dt><span class="term">
v0.2 1/21/09 
</span></dt><dd>
Different reliability modes: WAL 2.0, DFO, Best
effort. Output file escaping/bucketing.  Proliferation of many sink
and decorators.
</dd><dt><span class="term">
v0.1 11/23/09 
</span></dt><dd>
First installation deployment, users tests.
</dd><dt><span class="term">
v0.0 9/21/09 
</span></dt><dd>
First cut with current architecture (centralized
master, configuration language, web interface.)  First version of WAL.
Simple visualizations, samplers, thrift based rpc.
</dd><dt><span class="term">
pre history 7/21/09 
</span></dt><dd><p class="simpara">
First commit.  design, experimental
implementations.  Initial implementation had individual agent and
collector programs, watchdog
</p><pre class="literallayout">     ______
    / ___//_  ______  ____
   / /_/ / / / /    \/ __/
  / __/ / /_/ / / / / __/
 / / /_/\____/_/_/_/\__/
/_/ Distributed Log Collection.</pre></dd></dl></div></div></div></div><script type="text/javascript">
     var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
     document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
  </script><script type="text/javascript">
     try{
        var pageTracker = _gat._getTracker("UA-2275969-4");
        pageTracker._setDomainName(".cloudera.com");
        pageTracker._trackPageview();
     } catch(err) {}
  </script><div class="footer-text"><span align="center"><a href="index.html"><img src="images/home.png" alt="Documentation Home"></a></span><br>
  This document was built from Flume source available at
  <a href="http://github.com/cloudera/flume">http://github.com/cloudera/flume</a>.
  </div></body></html>
