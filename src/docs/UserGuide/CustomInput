
== Integrating Flume with your Data Sources

.WARNING 
******************** 
This section is incomplete.
********************

Flume's source interface is designed to be simple yet power and enable logging 
of all kinds of data -- from unstructured blobs of byte, semi-structured blobs 
with structured metadata, to completed structured data.

////
Issues: language neutrality, reliability, push vs pull, one shot vs 
continuous. 
////

In this section we describe some of the basic mechanisms that can be used to 
pull in data.  Generally, this approach has three flavors. *Pushing* data to 
Flume, having Flume *polling* for data, or *embedding* Flume or Flume 
components into an application.

These mechanisms have different trade-offs -- based on the semantics of the 
operation.

Also, some sources can be *one shot* or *continuous* sources.


=== Push Sources

+syslogTcp+, +syslogUdp+ :: wire-compatibility with syslog, and syslog-ng 
logging protocols.

+scribe+ :: wire-compatibility with the scribe log collection system.

=== Polling Sources

+tail+, +multitail+ :: watches a file(s) for appends.

+exec+ :: This is good for extracting custom data by using existing programs.

+poller+ :: We can gather information from Flume nodes themselves.

=== Embedding Sources

WARNING: these features are incomplete.

+log4j+

+simple client library+


 // move this to gathering data from sources

=== Logging via log4j Directly

//// 
WARNING: These instructions are out of date and currently untested.

Modify hadoop-daemon.sh so that it includes Flume.

Places where log4j mentioned:

---- 
bin/hadoop-daemon.sh    -- defaults to INFO,DRFA 
conf/hadoop-env.sh      -- can be set but can cause perms issues 
bin/hadoop              -- defaults to INFO,console 
conf/log4j.properties   -- loggers are defined here, but the default root logger is ignored

src/java/..../TaskRunner.java -- INFO,TLA 
---- 
////

==== Example of Logging Hadoop Jobs 
//// 
For jobs initiated by the user, the easiest mechanism to enable Flume logging is to modify conf/hadoop-env.sh to include:

export HADOOP_ROOT_LOGGER=INFO,flume,console 
////

==== Logging Hadoop Daemons 
//// 
To log events generated by Hadoop's daemons (tasktracker, jobtracker, datanode, secondarynamenode, namenode), modify bin/hadoop-daemon.sh so that HADOOP_ROOT_LOGGER is set to  

export HADOOP_ROOT_LOGGER=INFO,flume,DRFA

TODO (jon) this doesn't seem to working right now --  need to figure out why. 
////
